{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "- RNN은 인간의 생각의 persistence를 고려해 정보가 지속될 수 있도록 루프가 있는 네트워크이다.\n",
    "\n",
    "\n",
    "- BUT Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.\n",
    "\n",
    "\n",
    "- 이론적으로는 RNN이 Long-Term Dependency를 다룰 수 있지만, 실전에서는 잘 되지 않아 LSTM을 사용한다. 그리고, RNN의 Long-Term Dependency의 이유는 다음 논문에서 자세하게 다루었다.  [\"Learning Long-Term Dependencies with Gradient Descent is Difficult\",  Bengio, et al. (1994)](http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf)\n",
    "\n",
    "\n",
    "- LSTM은 speech recognition, language modeling, translation, image captioning 등의 다양한 문제에서 성공을 했으며, 다음 블로그 자료를 통해 자세하게 알 수 있다.  [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "<img  src=\"./image/RNN.PNG\" width=\"65%\">\n",
    "\n",
    "\n",
    "## LSTM Networks\n",
    "\n",
    "<img  src=\"./image/LSTM_1.PNG\" width=\"65%\">\n",
    "\n",
    "<img  src=\"./image/LSTM_2.PNG\" width=\"55%\">\n",
    "\n",
    "- LSTM은 대놓고 Long-term을 위해 만들어졌고, 긴 시간동안 정보를 기억하는 것은 애를 쓰는 것이 아니라, LSTMs의 기본행동이다. \n",
    "    - LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "\n",
    "\n",
    "- RNN과 비교한 LSTM의 구조는 single neural network layer가 4개로 늘어 서로 interacting in a very special way 하는 것을 볼 수 있다.\n",
    "    - Each line carries an entire vector, from the output of one node to the inputs of others. \n",
    "    - The pink circles represent pointwise operations, like vector addition.\n",
    "    - Yellow boxes are learned neural network layers.\n",
    "    - Lines merging denote concatenation,\n",
    "    - Line forking denote its content being copied and the copies going to different locations.\n",
    "\n",
    "\n",
    "### The Core Idea Behind LSTMs\n",
    "\n",
    "- LSTM의 열쇠는 수평적으로 지나가는 Cell State이다.\n",
    "    - 컨베이어벨트와 같으며, some minor linear interactions로 쭉 지나간다.    \n",
    "    - 그리고 cell state에 정보를 추가하거나 제거하기 위해 Gates를 이용한다.\n",
    "        - The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\n",
    "\n",
    "<img  src=\"./image/LSTM_3.PNG\" width=\"45%\">\n",
    "\n",
    "- Gates는 정보를 선택하도록 만든 것으로, sigmoid와 pointwise multiplication 연산으로 이루어져, Sigmoid의 output이 0일 때는 \"아무것도 지나가지 않도록\", 1일 때는 \"모두 통과하도록\" 한다.\n",
    "\n",
    "- LSTM은 Cell State를 protect하고 control하기 위한 3개의 게이트가 있다.\n",
    "\n",
    "\n",
    "### Step-by-Step LSTM Walk Through\n",
    "    \n",
    "1) Forget Gate \n",
    "<img  src=\"./image/LSTM_4.PNG\" width=\"45%\">\n",
    "- $ f_{t} = \\sigma (W_{f} \\cdot \\left[ h_{t-1}, x_{t} \\right] + b_{f}) $\n",
    "- decide what information we’re going to throw away from the cell state.\n",
    "    - sigmoid layer인 \"forget gate layer\"에 의해 어떤 정보를 버릴지 정해진다.\n",
    "    \n",
    "<br>\n",
    "\n",
    "2) Input Gate\n",
    "<img  src=\"./image/LSTM_5.PNG\" width=\"45%\">    \n",
    "- $ i_{t} = \\sigma (W_{i} \\cdot \\left[ h_{t-1}, x_{t} \\right] + b_{i}) $\n",
    "- $ \\tilde{C_{t}} = tanh (W_{C} \\cdot \\left[ h_{t-1}, x_{t} \\right] + b_{C}) $\n",
    "- decide what new information we’re going to store in the cell state.\n",
    "    -  A Sigmoid layer“input gate layer” decides which values we’ll update.\n",
    "    -  A Tanh layer creates a vector of new candidate values, $ \\tilde{C_{t}}$,  that could be added to the state.\n",
    "    -  Combine these two to create an update to the state.\n",
    "\n",
    "<br>\n",
    "\n",
    "3) Update\n",
    "<img  src=\"./image/LSTM_6.PNG\" width=\"45%\">    \n",
    "- $ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C_{t}} $\n",
    "- Update the old cell state, $ C_{ t − 1 } $ , into the new cell state $ C_{t} $ \n",
    "- This is the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "<br>\n",
    "\n",
    "4) Output Gate\n",
    "<img  src=\"./image/LSTM_6.PNG\" width=\"45%\">    \n",
    "- $ o_{t} = \\sigma (W_{o} \\cdot \\left[ h_{t-1}, x_{t} \\right] + b_{o}) $\n",
    "- $ h_{t} = o_{t} * tanh(C_{t}) $\n",
    "- Decide what we’re going to output    \n",
    "    - This output will be based on our cell state, but will be a filtered version.    \n",
    "- A sigmoid layer which decides what parts of the cell state we're going to output.\n",
    "- Put the cell state through \"tanh\"(-1 ~ 1의 값) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
    "\n",
    "\n",
    "## GRU - Gated Recurren Unit\n",
    "\n",
    "<img  src=\"./image/GRU.PNG\" width=\"45%\">\n",
    "\n",
    "- $ z_{t} =  \\sigma (W_{z} \\cdot \\left[ h_{t-1}, x_{t} \\right]) $\n",
    "- $ r_{t} =  \\sigma (W_{r} \\cdot \\left[ h_{t-1}, x_{t} \\right]) $\n",
    "- $ \\tilde{h_{t}} =  tanh (W \\cdot \\left[ r_{t} * h_{t-1}, x_{t} \\right]) $\n",
    "- $ h_{t} = (1-z_{t}) * h_{t-1} + z_{t} * \\tilde{h_{t}} $\n",
    "\n",
    "- It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state, and makes some other changes.\n",
    "- simpler than standard LSTM models, and has been growing increasingly popular.\n",
    "\n",
    "### Variants on Long Short Term Memory \n",
    "\n",
    "- Peephole Connection LSTMs\n",
    "- Coupling Forget and input gates\n",
    "\n",
    "#### CF)\n",
    "- LSTMs were a big step in what we can accomplish with RNNs. It's natural to wonder: is there another big step? A common opinion among researchers is: \"Yes! There is a next step and it's attention!\" The idea is to let every step of an RNN pick information to look at from some larger collection of information.\n",
    "\n",
    "- Grid LSTMs\n",
    "- Work using RNNs in generative models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: \n",
    "\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "<br> Christopher Olah의 블로그\n",
    "\n",
    "- LSTM paper: http://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "- GRU paper: https://arxiv.org/pdf/1406.1078v3.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
