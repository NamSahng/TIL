{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1. Motivation and Basics\n",
    "<hr>\n",
    "### Q. Thumbtack Question\n",
    "\n",
    "Billionaire tried for five times <br>\n",
    "The nail’s up case is three out of five trials\n",
    "\n",
    "압정을 5번 던져서 nail up 3번, down이 2번 나왔다. \n",
    "확률이 각각 3/5, 2/5 일까?\n",
    "\n",
    "\n",
    "## -   Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Flips는 independent and identically distributed 이다. (i.i.d) <br>\n",
    "(Independent events & Identically distributed according to Binomial Distribution) <br>\n",
    "\n",
    "+ $ P(D|\\theta) = \\theta^{a_{H}}(1 - \\theta)^{a_{T}}  $ 로 표현 가능  <br>\n",
    "\n",
    "$ P(D|\\theta) = \\theta^{a_{H}}(1 - \\theta)^{a_{T}}  $<br>\n",
    "여기서 Hypothesis 는 Thumbtack 은 $\\theta$ 라는 확률 분포(Binomial)를 따른 다고 한 것. <br>\n",
    "\n",
    "** To make Hypothesis Strong!  가정을 강하게 해보자  ** \n",
    "\n",
    "1. Find out better Distribution of observation 더 잘 표현하는 분포있으면 바꾸자 <br>\n",
    "2. (Binomial을 따르 는 것을 인정하고) Find out the best candidate of $\\theta$ <br>\n",
    "$ \\rightarrow \\ \\    Maximum \\ Likelihood \\ Estimation $ (확률의 추론) <br>\n",
    "Choose $\\theta$ that maximizes the probability of observed data <br>\n",
    "(관측된 데이터에 대하여 확률을 최대화하는 $\\theta$를 찾자)<br>\n",
    "$ \\rightarrow  \\ \\  \\hat{\\theta} = argmax_{\\theta}P(D|\\theta) $ <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### MLE Calculation\n",
    "\n",
    "$ \\quad \\hat{\\theta} = argmax_{\\theta}P(D|\\theta) = argmax_{\\theta} \\theta^{a_{H}}(1 - \\theta)^{a_{T}}   $\n",
    "\n",
    "단조증가함수인 log를 씌어 계산해도 argmax 의 값은 같으니까\n",
    "\n",
    "\n",
    "$ \\quad \\hat{\\theta} = argmax_{\\theta}ln(P(D|\\theta)) = argmax_{\\theta}ln(\\theta^{a_{H}}(1 - \\theta)^{a_{T}}) = \n",
    "argmax_{\\theta}(a_{H}ln\\theta + a_{T}ln(1-\\theta) )  $\n",
    "\n",
    "Then, this is a maximization problem, so you use a derivative that is set\n",
    "to zero <br>\n",
    "(최대값을 찾는 문제이므로, 극점을 활용하자.)\n",
    "\n",
    "$ {d \\over{d\\theta}}{(a_{H}ln\\theta + a_{T}ln(1-\\theta)) = 0} $ \n",
    "\n",
    "$ {a_{H} \\over{\\theta}}-{a_{T}\\over{1-\\theta}} = 0 $\n",
    "\n",
    "$ {\\theta} = {a_{H} \\over a_{H}+a_{T}} $\n",
    "\n",
    "$ {\\theta} $ 는 $ {a_{H} \\over a_{H}+a_{T}} $ 일 때, MLE관점에서 Best Candidate 이다.\n",
    "\n",
    "$ \\hat{\\theta} = {a_{H} \\over a_{H}+a_{T}} $\n",
    "\n",
    "MLE는 가장 간단한 parameter estimation method이지만, observation에 따라 그 값이 너무 민감하게 변한다는 단점을 가지고 있다. 다시 동전 던지기를 예로 들어보자. 동전 던지기는 확률 과정이기 때문에 극단적인 경우로 n번을 던져서 앞면이 n번이 나올 수가 있다. 이 경우 MLE는 이 동전은 앞면만 나오는 동전이라고 판단해버린다. [2]<http://sanghyukchun.github.io/58/>\n",
    "\n",
    "***\n",
    "### Q. While you were calculating, I was flipping more times.  \n",
    "\n",
    "It turns out that we have 30 heads and 20 tails.\n",
    " Does this change anything?\n",
    "\n",
    "### - Simple Error Bound\n",
    "\n",
    "A. 우리는 파라미터를 추정한 것이기 때문에, Error Bound 가 존재하며 그 것이 줄어든 것이다.\n",
    "\n",
    "We have a simple upper bound on the probability provided by Hoeffding’s inequality <br>\n",
    "$ P(|\\hat{\\theta} - {\\theta}^{*}| \\ge \\epsilon) \\le 2e^{-2N\\epsilon^{2}} $ <br>\n",
    "$ \\rightarrow \\ $   N: Trial 이므로 N이 커질수록 Error Bound는 줄어든다.\n",
    " \n",
    "- PAC Learning (Probably Approximate Correct): <br>\n",
    "확률(Probability)이 저 오차 범위(Approximate) 내에서는 Correct한 Learning의 결과물이다. <br>\n",
    "PAC Learning에 대한 추가 설명:  <http://sanghyukchun.github.io/66/>\n",
    "\n",
    "### Q. WAIT !  Bayes says \"Don't you think it is 50:50?\"\n",
    "\n",
    "- Incorporating Prior Knowledge: Bayes’theorem\n",
    "\n",
    "${P(\\theta|D) = {P (D | \\theta ) P(\\theta) \\over P(D) } }$\n",
    "\n",
    "&nbsp;&nbsp;$P(\\theta)$  Prior knowledge  , is the part of the prior knowledge\n",
    "<br>\n",
    "&nbsp;&nbsp;$P(D|\\theta)$:  Likelihood &nbsp;&nbsp;&nbsp;  ${P(D)}$ : Normalizing Constant\n",
    " &nbsp;&nbsp;&nbsp; ${P(\\theta|D)}$:  Posterior\n",
    " <br>\n",
    " &nbsp;&nbsp;${P(\\theta|D)}$ is the conclusion influenced by the data and the prior knowledge\n",
    " \n",
    "${P(\\theta|D)} \\  \\propto \\ P(D|\\theta) P(\\theta) $\n",
    "<br>&nbsp;&nbsp; 여기서, $P(D|\\theta)$는 Binomial Distribution을 따른다고 가정해  $ P(D|\\theta) = \\theta^{a_{H}}(1 - \\theta)^{a_{T}}$ 이다.\n",
    "<br>&nbsp;&nbsp; $P(\\theta)$ 는?\n",
    "<br>&nbsp;&nbsp;\n",
    "$\\to$  Beta Distribution을 사용한다.\n",
    "\n",
    "&nbsp;&nbsp; $P(\\theta) = {{\\theta ^ {\\alpha -1} ( 1 - \\theta)^{\\beta -1}} \\over {B(\\alpha,\\beta)} }  $ &nbsp;&nbsp;, &nbsp;&nbsp; ${B(\\alpha,\\beta)} = {{\\Gamma(\\alpha)}{\\Gamma(\\beta)}\\over {\\Gamma(\\alpha + \\beta)} } $  &nbsp;&nbsp;, &nbsp;&nbsp; ${\\Gamma(\\alpha) = (\\alpha - 1)!}$\n",
    "\n",
    "마찬가지로 분모는 $\\theta$ 에 의존하지 않으니까\n",
    "\n",
    "&nbsp;&nbsp; ${P(\\theta|D)}  \\propto  P(D|\\theta) P(\\theta) \\propto {\\color{Red}{\\theta^{a_{H}}(1 - \\theta)^{a_{T}}}}   {\\color{Blue}{\\theta ^ {\\alpha -1} ( 1 - \\theta)^{\\beta -1}} } = \\theta^{a_{H} + \\alpha -1} (1-\\theta)^{a_{T}+\\beta -1}  $\n",
    "\n",
    "## -   Maximum a Posteriori Estimation (MAP)\n",
    "\n",
    "- Previously in MLE\n",
    "<br>\n",
    "$\\  \\hat{\\theta} = argmax_{\\theta}P(D|\\theta) $\n",
    "<br>\n",
    "$ P(D|\\theta) = \\theta^{a_{H}}(1 - \\theta)^{a_{T}}  $ \n",
    "<br>\n",
    "$ \\hat{\\theta} = {a_{H} \\over a_{H}+a_{T}} $\n",
    "\n",
    "\n",
    "- Now in MAP\n",
    "<br>\n",
    "$\\  \\hat{\\theta} = argmax_{\\theta}{P(\\theta|D)} $\n",
    "<br>\n",
    "${P(\\theta|D)}  \\propto \\theta^{a_{H} + \\alpha -1} (1-\\theta)^{a_{T}+\\beta -1}  $\n",
    "<br>\n",
    "$ \\hat{\\theta} = {{a_{H} + \\alpha -1} \\over {a_{H} + \\alpha + a_{T} + \\beta -2 }} $\n",
    "\n",
    "둘다 극점을 활용했지만 관점이 다르다.\n",
    "\n",
    "### Q. 사전정보에 너무 치우쳐 진 것 아닌지?   \n",
    "\n",
    "No! $a_{H}$와 $a_{T}$가 커지면 $\\alpha, \\beta$의 영향은 줄어들고, $a_{H}$, $a_{T}$에 의해 Term이 dominant해진다.\n",
    "\n",
    "### Q. Who decides $\\alpha, \\beta$ ?\n",
    "\n",
    "maybe grad students?\n",
    "\n",
    "<br><br>\n",
    "### Reference:\n",
    "\n",
    "문일철 교수님, 인공지능 및 기계학습 개론Ⅰ, https://www.edwith.org/machinelearning1_17\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
