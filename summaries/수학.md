# 수학

- **선형대수**
    - **선형독립**: 두 벡터가 선형 독립이라는 것은 두 벡터의 선형결합으로 0벡터를 표현하는 것이 0 벡터 밖에 없는 경우를 말한다. 이는 벡터들을 열벡터로 놓은 행렬 A로 놓고 행렬 A의 영공간이 0벡터밖에 없는 경우에도 해당 벡터들 간에 선형 독립이라 한다.
    - **기저**: 벡터 공간을 이루는 최소 선형 독립 벡터 집합이다.
    - **차원**: 벡터 공간의 기저의 수를 차원이라 한다.
    - **랭크**: 행렬 A의 랭크는 열벡터 공간의 기저의 수, 행벡터 공간의 기저의 수, 가우스 소거법 이 후 피벗의 수이다.
    - **고유값 & 고유벡터**
        
        **고유값**: 특성방정식의 해 lambda
        
        **고유벡터**: 특성방정식의 v
        
        → 어떤 행렬을 사상이 어떤 방향으로 얼마나 크게 변화하는 선형변환인지 확인할 수 있는 값과 벡터
        
    - **고유값 분해**: $A = Q\Lambda Q^{-1}$
        
        Eigen Decomposition을 통해 구하는 값과 벡터로, 특성방정식(Ax = lambda x)을 통해 구할 수 있으며, 이는 정방행렬이 어떤 방향으로 얼마나 크게 변화하는 선형변환인지 확인할 수 있는 크기와 벡터를 말한다. 
        
        $A = Q\Lambda Q^{-1}$ (Q: 열벡터가 고유벡터, 람다: 고유값)
        
    - **SVD:** $A = U\Sigma V^{T}$
        
        SVD란 고유값분해와 달리 정방행렬이 아니더라도 모든 행렬에 대해서도 수행할 수 있다는 점이 다르다. 또한, 고유값과 달리 항상 양의 실수 값을 가진다. SVD는 차원축소, matrix completion, matrix factorization, 의사역행렬 등에 사용될 수 있다.
        
        - **SVD 구하기**
            
            $V^T$: $A^{T}A$의 고유벡터로 구성됨 (v1, v2)
            
            $U$: $A^{T}A$의 고유값의 제곱근인 $\sigma$,  A, v(위에서 구한) 것, 그리고 $A$의 left null space의 벡터를 그 크기로 나눈 것으로 구성된다.
            
            ![Untitled](%E1%84%89%E1%85%AE%E1%84%92%E1%85%A1%E1%86%A8%201a112793afc54d24865ea0eab27b5396/Untitled.png)
            
            또는 $U$는 $AA^{T}$의 고유벡터로 구성된다.
            
        - **Matrix Approximation**
            
            SVD를 통해 $A = \Sigma_{i=1}^{k}\sigma_{i}u_{i}v_{i}^{T}$로 표현할 수 있다. (u는 열벡터 v는 행벡터) 프로비니우스 노름으로 계산한 rank k에서의 최소 행렬 근사는 SVD를 통해 분해한 행렬의 벡터들을 이용한 것이다.
            
        - **Left pseudo inverse를 구할 때 사용 가능**
            
            $A^{+} = V \Sigma^{-1}U^{T}$
            
        - **SVD vs EVD**
            
            SVD 모든 Matrix에 대해 존재 / EVD 정방행렬에 대해 고유벡터 기저가 있는 경우에만 존재
            
            $A = U\Sigma V^{T}$에서 U는 AAT의 고유벡터, V는 ATA의 고유벡터로 구성된다
            
            Symmetric, Positive Definite한 행렬 A에 대하여 SVD와 EVD는 같다.
            
            Singular value는 항상 양의 실수이지만, Eigen vlaue는 복소수나 음수 일 수 있다.
            
    - **Symmetric, Positive Definite Matrix**
        
        x는 0벡터가 아닌 모든 벡터에 대해, $x^{T}Ax > 0$이고, 대칭인 행렬 A
        
        항상 숄레스키 분해가 가능하며 이는 두 대각행렬로 구성된다.
        
    - **pseudo-inverse**
        
        $A \in \mathbb{R}^{m \times n}$
        
        - Left pseudo Inverse: $A^{+} = (A^{T}A)^{-1}A^{T}$
            
            참고로 $AA^{+}$는 행렬 A의 열벡터로의 projection matrix이다.
            
        - Right pseudo Inverse: $A^{+} = A^{T}(AA^{T})^{-1}$
    - **Norm**
        
        벡터의 길이, 힘, 에너지의 크기를 측정
        
        - 프로비니어스 노름: 벡터의 l2 norm과 유사하며, 대각성분(trace)의 제곱합의 루트이며, 이를 SVD의 특이값의 제곱합의 루트와 같다.
    - **유사 행렬 (Similar Matrix) & 상등 행렬(Equivalence Matrix)**
        
        $\tilde{A} = Y^{-1}AS$ 을 만족하는 가역행렬 S와 T가 있다면 두 행렬 $A, \tilde{A}$ 는 상등(Equivalence)한 관계이다. 이 때, $A, \tilde{A}$가 nxn 행렬인 경우 유사(similar)한 관계에 있다.
        
        두 유사한 행렬의 determinant, trace, eigenvalue는 모두 같다.
        

- **확률 통계**
    - **Information Theory**
        - **Self Information**: 정보의 량을 말하며, $\log{{1}\over{P(X=x)}}$로 표현된다.
        - **Entropy**: 확률분포가 가지는 정보량을 수치로 표현한 것이다. 이는 손실이 없는 가변길이인코딩 시 최적으로 인코딩하는 길이를 의미하기도 한다. $\Sigma P(X)\log {{1}\over{p(X)}}$
            
            연속확률변수에서는 미분엔트로피라고도 하며, $\int p(x)\log({{1}\over{p(x)}})dx$
            
            연속확률변수의 구간 [a,b]에서의 최대 엔트로피 분포는 균등분포이고, $(-\infty, \infty)$에서의 분산 $\sigma^{2}$와 기댓값 0에서의 최대 엔트로피는 정규분포이다. 
            
        - **Joint Entropy:**
            
            $H[X,Y] = \Sigma_{x}\Sigma_{y} P(X,Y) {{1}\over{P(Y,X)}}$
            
            X, Y에 대한 정보량
            
        - **Conditinal Entorpy**
            
            $H[Y|X] = \Sigma_{x}\Sigma_{y} P(X,Y) {{1}\over{P(Y|X)}}$
            
            X를 알고 있을 때 Y에 대한 정보량
            
        - **Information Gain:**
            
            $I[X;Y] \equiv H[Y] - H[Y|X]$
            
            Y를 알 때의 정보량에서, X가 주어진 상태에서 Y를 알 때의 정보량의 차이.
            
            차이가 클수록 X는 Y에 대해 많은 정보를 포함하고 있다는 것
            
            I=0 일 때, 독립과 동치이며, 상관계수와의 달리 범위가 다르다. 의사결정 나무 모델중 IG가 큰 feature를 노드로 설정해 분기
            
        - **KL Divergence (Relative Entropy)**
            
            $KL[p||q] = \int p(x)\log{{p(x)}\over{q(x)}}$
            
            분포 p와 q가 얼마나 다른지 측정하기 위해 사용. 가변길이 인코딩으로 해석하면, q 분포에 최적으로 인코딩한 시스템에 p 분포에 적용했을 때의 평균인코딩 길이가 교차엔트로피이고, p에 맞게 인코딩한 길이와의 차이가 KLD라고 할 수 있다.
            
            기존 p가 우리가 목표로하는 분포이고 q를 파라미터화 하여, p에 맞추려고 한다고 해석하면, KLD를 줄이는 방안은 likelihood를 최대화 하는 방안이다. 
            
        - **Cross Entropy (교차엔트로피)**
            
            $H_{P}[Q] = H[P] + KL[P||Q]$
            
            교차엔트로피는 자기 엔트로피와 KLD 의 합이다. Cross Entropy를 minize 한다는 것은 KL Divergence를 줄인다는 것과 같다.
            
            가변길이 인코딩으로 해석하면, q 분포에 최적으로 인코딩한 시스템에 p 분포에 적용했을 때의 평균인코딩 길이이다.
            
    - **확률 & 가능도(우도)**
        
        **확률**: 어떤 시행의 특정 결과가 나올 가능성. 시행 전 경우의 수의 가능성은 정해짐
        
        - 빈도주의: 확률을 무한번 실험한 결과, 객관적으로 발생하는 현상의 빈도수로 봄
        - 베이지안: 사전 확률을 미리 염두해두고 사건의 발생에 따라 베이즈 정리로 사후 확률을 구해 다시 사전 확률을 변경. 믿음 전파.
        
        **likelihood**: 어떤 모델에서 해당 데이터가 나올 확률
        
    - **확률 변수**
        
        확률변수란 사건에 실수값을 부여하는 함수
        
        - **관계**
            1. **독립성**
                
                 X에 대한 정보는 Y에 대해 아무런 정보를 주지 못하는 것
                
                카이제곱 독립성 검정, 피셔 정확도 검정(모든 셀이 5 이상을 충족하지 않을 때) → 두 방안 모두 연속 확률변수에 대해서는 적용하기 어려움.
                
                연속형인 경우 공분산이나 상관계수를 볼 수 있지만 검정은 아님.
                
            2. **종속성, 연관성**
                
                두 변수는 서로 유의미한 정보를 제공함
                
            3. **상관성**
                
                피어슨 상관성 = 표본 공분산 / 각각 표본 표준편차
                
                상관성 계수에 대해 신뢰구간이 0을 포함하는지 확인 또는 상관계수는 0인지에 대해 t-검정 수행. 선형성 가정이 있어 산점도 확인 필요.
                
            4. **인과성**
                
                상관이 있는 관계에서 인과관계가 있는 것
                
        - 회귀에서 독립 변수들 간의 관계
            
            ![Untitled](%E1%84%89%E1%85%AE%E1%84%92%E1%85%A1%E1%86%A8%201a112793afc54d24865ea0eab27b5396/Untitled%201.png)
            
            - Chain relation(연쇄관계): x1과 y의 연관성이 사라짐
            - Suprious corrleation(허위상관성): x2가 x1과 Y에 영향을 미치지만, x1은 Y에 영향을 미치지 않는 경우 , ex. 범죄, 교회, 인구수
            - Multiple Causes(다중 원인): x1, x2는 서로 독립된 원인. 다중 회귀분석은 다중원인으로 가정하고 수행
            - Interaction(상호 작용): 설명변수와 조절변수가 임계점/맥락/비선형조건부/잠재 등의 효과등이 있다. 이는 상호작용분석이나 유전알고리즘등을 이용할 수 있다.
    - **베이즈 정리**
        
        $P(Y|X) = {{P(X|Y) P(Y)}\over{P(X)}}$
        
    - **확률 product rule**
        
        $P(X|Y) = P(X,Y)/P(Y)$
        
    - **확률 sum rule**
        
        $P(X) = \Sigma_{Y} P(X,Y)$
        
    - **중심극한 정리**
        
        IID하게 추출한 n개의 표본들의 평균에 대한 분포는 n이 적당히 크면, 모집단의 분포와 상관없이 정규분포에 가까워 진다.
        
        이를 이용해 표본의 통계량을 이용해 모집단의 모수를 추정할 수 있는 수학적 근거를 마련할 수 있게함.
        
    - **Parametric & Non parametric**
        
        모수적 방법: 모집단이 있는 상황을 가정해 진행하는 방법
        
        비모수적 방법은 모집단을 가정하지 않고, 주어진 자료만 활용해 판단하는 방법
        
        - 모수적 방법: 빈도분석, T-Test, Anova, 상관관계분석, 회귀분석, 요인분석, 군집분석
        - 비모수적 방법: 카이제곱 검정, 피셔검정, 부호검정 등
    - **큰 수의 법칙**
        
        I.I.D 에서 추출한 확률 변수들의 평균 (표본 평균은) 표본 집단의 크기가 커지면 모평균에 가까워진다.
        
    - **가설검정**
        - **P-Value**
            
            **귀무가설(영가설, null hypothesis)에서 주장한바가 옳을 확률, 귀무가설이 참이라는 가정아래 얻은 통계량이 귀무가설을 얼마나 지지하는지를 나타낸 확률.** 검정통계량이 얼마나 커야 기각할 수 있는지 알기 어렵기 때문에 사용한다. 보통 0.05보다 작으면 귀무가설 기각 0.1 이상 귀무가설 참
            
        - **검정통계량**
            
            귀무가설이 참이라는 가정 아래 얻은 통계량
            
        - **t-검정**
            
            표본평균으로 단일 모집단의 평균 검정
            
            두 모집단의 평균의 차이를 검정 (등분산이 아닐 경우, Welch T-test)
            
            회귀 계수 검정 (전체를 한번에 할 경우, F-검정)
            
            모델 성능 간의 차이 검정
            
    - **MLE**
        
        parametric density estimation (모수적 밀도 추정 방법)으로 파라미터 세타로 구성된 어떤 확률밀도함수 p(x|세타)에서 관측된 표본 데이터 집합을 x = (x1, ... xn)이라 할 때 표본들로 파라미터를 추정하는 방법이다.
        
        데이터를 통해, 데이터를 생성과정을 설명하는 모수를 찾는다는 점(inversion of probability)에서 MAP와 같지만, MAP와 달리 사전확률을 사용하지 않는다.
        
        ex. 100개 항아리 검흰 구슬이 있는 데, 10개 추출해서 검6 흰 4개가 나옴. MLE로 100개에 검흰이 몇 개있냐
        
        이항분포를 따른다고 가정하고 각 시행이 독립이라 가정하면 → log( 10C4 * p^6 * (1-p)^4 ) → 미분 → 6/p + 4/(1-p) = 0 → p = 를 구해 100에 곱함
        
        MAP는 가능도에 사전분포확률을 곱해 최대가 되는 사후확률을 구한다.
        
        [https://www.youtube.com/watch?v=sOtkPm_1GYw](https://www.youtube.com/watch?v=sOtkPm_1GYw)
        
    - **피셔 정보량**
        
        log likelihood의 1차 도함수(score function)의 분산. MLE의 불확실성을 측정하기 위해, 피셔정보량을 확인할 수 있으며 이를 통해 회귀 계수의 표준오차를 통해 계수에 대한 검정을 수행할 수 있다. 
        
        왈드검정 - t-test / 우도비 검정 - F-test
        
    - **최소제곱법**
        
        위키: 잔차제곱합을 최소화하여 overconstrained 시스템의 해법에 근사치를 갖는 회귀분석의 표준 접근법
        
        기하: Xa = y 로 놓았을 때, X의 열벡터공간(feature)으로 y를 표현할 수 없으니, y를 가장 잘 표현하는 벡터를 X의 열벡터 공간에 projection을 해 표현. X의 SVD를 이용해 의사역행렬을 구해 푸는 것도 이와 같음
        
        수리통계: 최소제곱 추정은 가우스 마르코프 이론으로 설명 가능하며, 최소제곱추정은 선형 최소 분산 비편향 추정량이다. 비편향 추정을 확인하는 방법은 결정이론의 손실의 기댓값인 Risk function을 통해 확인한다.
        
        구한 추정치는 신뢰구간이 0을 포함하는지, 각 추정치가 0인지에 대한 T-검정, 모든 추정치 중 적어도 하나는 0이 아니다에 대한 F검정(ANOVA)를 통해 추정치의 불확실성을 확인한다.
        
        모델 평가: MAE, RMSE, R^2: 결정계수를 통해 모델의 설명력의 지표를 0~1로 보여줌. 1-(잔차 제곱합)/(종속변수의 변동)
        
- **Vector Calculus**
    - **gradient**
        
        f(x)의 x1, x2, ... xn에 대한 편미분을 행벡터로 표현한 것. 행벡터로 표현하는 이유는 벡터함수의 gradient를 행렬로 표현 가능하며, chain rule 적용 시 차원을 고려하지 않아도 됨.
        
    - **Jacobian**
        
        벡터함수의 1차 편미분 행렬
        
    - **Hessian**
        
        함수의 2차 편도함수의 모음이다. f: $\mathbb{R}^{n} \to \mathbb{R}$ 에서는 nxn 행렬이며, f: $\mathbb{R}^{n} \to \mathbb{R}^{m}$에서는 mxnxn 텐서의 형태이다.
        
        ![Untitled](%E1%84%89%E1%85%AE%E1%84%92%E1%85%A1%E1%86%A8%201a112793afc54d24865ea0eab27b5396/Untitled%202.png)
        
    - **자동 미분**
        
        Backpropagatioin은 수치해석의 자동미분의 특별한 경우이다. 자동미분은 chain rule을 이용해 gradient를 수치적으로 계산하는 방법이다. 기초 산술 연산과 초등함수의 미분을 미리 계산해 복잡한 함수의 gradient를 chain Rule을 이용해 구할 수 있다.
        
- **Continuous Optimization**
    - **Stationary point**
        
        f’(x) = 0인 지점
        
        - 극소: f’’(x) < 0, convex
        - 변곡: f’’(x) = 0 or undefined
    - **Gradient Descent**
        
        Gradient Descent란 $\theta_{t+1} = \theta_{t} - \nabla(f(\bold{x})) \times lr$ 로 gradient가 줄어드는 방향으로 lr 만큼 움직여 함수 f(x)의 최솟값을 찾는 방법이며, 제약조건이 없는 최적화 문제에 가장 간단한 방법이다.
        
        - **문제점**
            1. 1차 미분 알고리즘이기 때문에, 목적함수 표면 곡률 정보를 이용하지 않는다. → momentum 방법: 이전 파라미터의 변화량을 고려해 이동평균과 같이 작동
            2. 미분가능하지 않은 함수를 다루는 것 → subgradient method, proximal gradient descent
        - **SGD**
            
            모든 샘플에 대해 gradient를 구해 파라미터를 업데이트 하는 방안이 시간이 오래걸리므로, gradeint의 근사치를 이용한 방법. 
            
            Mini batch를 사용하는 것이 일반적이며, mini batch나 SGD는 true gradient의 불편추정량이기 때문에 가능한 것이다.
            
    - **Newton’s Method, Newton Raphson Method**
        
        뉴튼법은 함수의 zero-finding 문제에 사용. 제약조건이 없는 최적화 문제에서 f(x)의 최솟값을 찾는다는 것의 필요조건은, f(x)의 미분이 0인 지점을 찾는 것이 필요조건이기 때문에, 뉴튼법을 활용할 수 있다. GD방법과 달리 1/f’’(x1)로 정해져 있어 보다 빨리 수렴하지만, 헤시안을 계산해야하기 때문에 헤시안을 추정하는 방법이 보다 많이 사용됨.
        
    - **Constrained Optimization & Convex Optimization & KKT Condition**
        
        f(x)의 최소값을 구하고, 제약조건이 있는 문제는 primal problem(원시 문제)라 하며, 이 때 제약조건을 고려하지 않은 최적해를 primal optimal (point)라 한다. 이를 풀기 위해, 라그랑지안 승수를 도입해 Duality problem으로 바꿔 문제를 푸는 방법이 가장 일반적이다.
        
        라그랑지안 승수를 도입해 만든 식 q(뮤, 람다)은 primal optimal에서의 뮤와 람다(부등식 제약조건과 등식제약조건의 계수)에 대한 함수로, q(뮤, 람다)가 최대가 되는 지점을 dual optimal 이라 한다.
        
        듀얼 최적값이 프리말 최적값과 같은 경우 strong duality이며, 듀얼최적값이 더 작은 경우를 weak duality라 한다.
        
        제약조건이 있는 문제 중 목적함수 f(x)와, 부등식 제약조건 g(x)가 convex function이고, 등식제약조건 h(x)는 affine 함수이고 convex set인 문제를 convex optimization이라 하며, 이 때는 strong duality를 갖는다.
        
        제약조건이 없는 최적화 문제에서 f’(x) = 0 이 최소값을 갖기 위한 필요조건인 것과 유사하게 Convex 문제에서 KKT Condition을 만족하는 x*, 뮤, 람다를 찾는다면, 이 때의 x*는 최적값이기 위한 필요충분조건이 된다. 이 때, 라그랑지안 승수를 도입한 함수는 0 인 조건은 stationary condition이라 한다.
        
        KKT 조건이 제약조건 최적화 문제의 필요조건이기 위한 조건은 Regularity condition을 만족해야한다.