{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  C. Text Classification & Sentence Representation\n",
    "\n",
    "Supervised Learning이 실제로 Text Categorization에 어떻게 쓰이는지 보자. <br>\n",
    "NN을 쓰면 Text Classification할 때 자연스럽게 Sentence Representation 생각을 해야하기 때문에 좋은 예제이다.\n",
    "\n",
    "- Examples\n",
    "    - Sentiment analysis\n",
    "    - Text categorization\n",
    "    - Intent classification\n",
    "\n",
    "\n",
    "### How to represent sentence & token?\n",
    "\n",
    "text의 경우에는 이미지나 Signal과 같이 이미 Regular하게 표현할 수 있는 방법들들을 가진 것들과는 조금 표현하는 방법이 다르다.\n",
    "문장은 각 unit, token들이 굉장히 arbitrary한 성격을 갖고 있기 때문이다.\n",
    "arbitrary하다는 것은 어떤 의미일까?\n",
    "\n",
    "cf) 가령, 강아지는 dog, 고양이는 cat이라고 부릅니다. 그런데 어떤 이유로 dog, cat이라고 표기하는 것일까?<br> &nbsp;\n",
    "강아지와 비슷한 동물로는 늑대가 있다. 그런데 이 두 동물의 비슷한 특성에도 불구하고, dog와 wolf는 스펠링이 완전히 딴판. 스펠링으로는 이 동물의 비슷한 특성을 전혀 나타내지 못 하고 있다. 즉, 스펠링만 가지고서는 우리는 이 동물이 비슷하다는 것을 알 수가 없다. 따라서 단어들은 굉장히 arbitrary한 assignment가 되어있습니다. <br>\n",
    "&nbsp; 이미지 같은 경우에는 pixel의 intensity 관점에서 보면 높고, 낮음이 명확합니다.\n",
    "오디오 같은 경우에는 magnitue가 크면 소리가 큰 것이고, magnitue가 낮으면 소리가 작은 것이라고 볼 수 있듯이 표현 방법이 보다 명확하다. <br> &nbsp;\n",
    "그렇다면 text도 dog가 cat보다는 wolf와 가깝다는 것을 판단할 수 있게하는 표현 방법은 무엇인지 보자.\n",
    "\n",
    "#### Procedures\n",
    "\n",
    "- A sentence is a variable-length sequence of tokens: $X = (x_{1},...,x_{T})$\n",
    "- Each token could be any one from a vocabulary: $ x_{t} \\in V $ <br>\n",
    " 토큰으로 구성된 Vocabulary는 어떻게 토큰을 정의하느냐에 따라서도 다르다. 각 토큰은 predefined된 딕셔너리의 term으로 볼 수도 있다.\n",
    "    - Ex):  space-separated, 형태소(Morphs) 단위, 모든 syllable(어절) 단위, 비트 숫자 등 원하는 데로 뽑을 수 있다.\n",
    "\n",
    "- 컴퓨터에게 단어를 숫자로 표현하기 위해서, 단어장(Vocabulary)을 만들고, 중복되지 않는 인덱스(index) 로 바꾸고 문장을 일련의 정수로 표현.(Encoding)\n",
    "    - Ex) One hot Encoding(One of K Encoding) <br>\n",
    "    Pros&Cons: 모든 토큰 간의 거리가 같다.(arbitrary함을 나타낼 수 있다.)\n",
    "    \n",
    "- BUT 의마가 없다. 개와 늑대의 거리가 가까우면 좋겠다.(의미를 Capture할 수 있도록)\n",
    "- $ \\to $ &nbsp; 각 토큰을  연속 벡터 공간(Continuous vector space)에 투영하하자! 임베딩(Embedding)! 방법 아래 (Table Look-up 실습 해보기)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Represent a sentence \n",
    "\n",
    "<img src=\"./image/img_tablelookup.PNG\" width=\"70%\">\n",
    "\n",
    "지금까지 Token representation에 대해서 보았다. Neural Net 입장에서는 DAG의 첫번째 노드가 지나고나면, sentence는 sequence of vector가 되는 것이다.\n",
    "\n",
    "그러면 각 vector들은 각 token이 무슨 의미를 갖고있는지를 갖고있는 것이다. 그렇다면, 이를 갖고 뭔가 계산을 해서, 벡터 하나가 마지막에 나오고 나면, 그 벡터의 사이즈는 카테고리의 갯수와 같아야 한다. 그래야만 softmax function에 나오면 distribution과 같이 나올 것이다.\n",
    "\n",
    "이 input sentence가 어떤 카테고리에 속할지의 probability를 ouput으로 보여주는 것이다. Arbitrary sub-graph가 계산을 하고나면, 마지막에 나온 output이 input의 representation이 되는 것이다.\n",
    "\n",
    "이는 사실 카테고리 분포(categorical distribution)으로 muticlass classification한 것과 같다.\n",
    "\n",
    "그런데 sentnece의 size는 고정되어 있지 않다. 문장은 긴 문장도 있으며, 짧은 문장도 있다. input이 fixed size가 아니라, size가 계속 바뀐다면 어떻게 fixed size representation을 찾을 수 있는지. \n",
    "\n",
    "토큰에 대한 의미를 갖고있는 vector를 찾는 것이 table lookup이었다면,\n",
    "문장에 대한 의미를 갖고있는 vector를 찾는 것이 이 장에서 다루어 보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: \n",
    "\n",
    "- https://www.edwith.org/deepnlp 조경현교수님, 딥러닝을 이용한 자연어 처리 강의 및 강의 자료 <br>\n",
    "- https://wikidocs.net/21758 (위 강의 강의록)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Look-up 실습\n",
    "\n",
    "그것을 하는 방법은 NN이 one-hot vector에 weight matrix를 곱해주는 것이다. vocabulary token 하나, 하나마다 one-hot vector가 된다. 그리고 그 vetor에 weight matrix를 곱해주는 것이다. 그런데 one-hot vector는 token에 해당하는 위치만 1이고, 나머지 모든 원소는 0이고 따라서 1에 해당하는 weight matrix의 값만 가져오므로, 일종의 table lookup 과정이라고 볼 수 있다.\n",
    "\n",
    "즉, 다시 말하면 NN이 어떤 토큰을 받으면, 그 토큰에 해당하는 index를 찾고, index에 해당하는 vector를 뽑는다. (continuous vector?) 우리는 이를 table lookup이라고 한다. 그리고 이는 DAG에서 하나의 노드로 구현이 된다. 이 operation 자체도 미분가능한 것이고(야코비안 vector product도 계산할 수 있으며) pytorch에서는 이를 embedding layer라고 한다.\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "- https://www.tensorflow.org/tutorials/representation/word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
