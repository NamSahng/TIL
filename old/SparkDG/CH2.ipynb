{"cells":[{"cell_type":"code","source":["# 세션확인\nspark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.session.SparkSession at 0x7f6d6195fc50&gt;\n</div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["### 2.5 SparkSession\n\n- spark 코드:\n  <pre><code> val myRange = spark.range(1000).toDF(\"number\")</code></pre>\n\n- 1 x 1000 데이터 프레임 각각 값 0~999 생성. \n- spark의 DF는 단일 컴퓨터에 저장하기에 데이터가 크거나 계산에 오랜 시간이 걸려 여러 컴퓨터에 분산.\n  - 스프레드시트와 R과 파이썬에서의 DF는 일반적으로 분산컴퓨터가 아닌 단일 컴퓨터에 존재.\n\n- spark는 Dataset, Dataframe, SQL 테이블, RDD라는 핵심 추상화 개념을 가짐.\n  - df가 가장 쉽고 효율적. 2부 마지막에 Dataset, 3부에서 RDD를 다룰 예정."],"metadata":{}},{"cell_type":"code","source":["myRange = spark.range(1000).toDF(\"number\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["### 2.7 트랜스포메이션\n\n- 스파크의 핵심 데이터 구조는 한번 생성하면 변경할 수 없는 불변성(immutable)을 가짐.\n  - 변경을 위해서 트랜스포메이션 명령\n  \n- 스칼라 코드\n<pre><code> val divisBy2 = myRange.where(\"number % 2 = 0\") </code></pre>\n\n- transformation의 2가지유형\n  - narrow dependency: 각 입력의 파티션이 하나의 출력 파티션에만 영향을 미침 (ex: where)\n    - pipelining을 자동으로 수행, 모든 작업이 메모리에서 일어남.\n  - wide dependency: 하나의 입력 파티션이 여러 출력 파티션에 영향 (ex: shuffle(클러스터에서 파티션을 교환))\n    - 셔플의 결과를 디스크에 저장. 중요한 주제임.\n    - 파티션: 클러스터의 물리적 머신에 존재하는 row의 집합"],"metadata":{}},{"cell_type":"code","source":["divisBy2 = myRange.where(\"number % 2 = 0\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["- 실제 연산을 수행을 위해 액션 명령. (트랜스포메이션은 논리적 실행 계획)\n  - 결과를 계산하도록 지시하는 명령.\n  - 액션 지정할 때 job이 시작됨.\n    - job은 필터(좁은 트렌스포메이션)을 수행한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션)한다.\n  \n### 2.8 액션  \n\n- 액션의 유형:\n  - 콘솔에서 데이터를 보는 액션\n  - 각 언어로 된 네이티브 객체에 데이터를 모으는 액션\n  - 출력 데이터소스에 저장하는 액션"],"metadata":{}},{"cell_type":"code","source":["divisBy2.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>500\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### 2.10 종합\n\n- inferSchema: DataFrame의 스키마 정보를 알아내는 스키마 추론 기능"],"metadata":{}},{"cell_type":"code","source":["flightData2015 = spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(\"/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# flightData2015.head(5)\nflightData2015.take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>\n[Row(DEST_COUNTRY_NAME=&apos;United States&apos;, ORIGIN_COUNTRY_NAME=&apos;Romania&apos;, count=15),\n Row(DEST_COUNTRY_NAME=&apos;United States&apos;, ORIGIN_COUNTRY_NAME=&apos;Croatia&apos;, count=1),\n Row(DEST_COUNTRY_NAME=&apos;United States&apos;, ORIGIN_COUNTRY_NAME=&apos;Ireland&apos;, count=344),\n Row(DEST_COUNTRY_NAME=&apos;Egypt&apos;, ORIGIN_COUNTRY_NAME=&apos;United States&apos;, count=15),\n Row(DEST_COUNTRY_NAME=&apos;United States&apos;, ORIGIN_COUNTRY_NAME=&apos;India&apos;, count=62)]\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["- read: 좁은 트랜스포메이션\n- sort: 넓은 트랜스포메이션\n  - df를 변경하지 않고 이전 df를 변환하여 새로운 df를 생성해 반환\n\n- explain: df의 계보(lineage)나 스파크 쿼리 실행 계획을 확인 가능.\n  - explain 결과의 sort exchange filescan을 주목."],"metadata":{}},{"cell_type":"code","source":["flightData2015.sort(\"count\").explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) Sort [count#272 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#272 ASC NULLS FIRST, 200)\n   +- *(1) FileScan csv [DEST_COUNTRY_NAME#270,ORIGIN_COUNTRY_NAME#271,count#272] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\n\nflightData2015.sort(\"count\").take(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>\n[Row(DEST_COUNTRY_NAME=&apos;United States&apos;, ORIGIN_COUNTRY_NAME=&apos;Singapore&apos;, count=1),\n Row(DEST_COUNTRY_NAME=&apos;Moldova&apos;, ORIGIN_COUNTRY_NAME=&apos;United States&apos;, count=1)]\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["- 스파크는 셔플 수행 시 기본적으로 200개의 셔플 파티션 생성.\n- 아래 코드를 통해 셔플 파티션 5개로 설정해 셔플 출력 파티션 줄이기 가능\n\n<pre><code>spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")</code></pre>\n\n\n- 스파크는 계보를 통해 입력 데이터에 수행한 연산을 전체 파티션에 어떻게 재연산하는지 알 수 있음.\n  - 이 기능이 스파크 프로그래밍 모델인 함수형 프로그래밍의 핵심.\n  - 함수형 프로그래밍은 데이터의 변환 규칙이 일정한 경우 같은 입력에 대해 항상 같은 출력 생성.\n  \n- 사용자는 물리적 데이터를 직접 다루지 않고, 셔플 파티션 파라미터와 같은 물리적 실행 특성을 제어.\n  - 스파크 UI를 통해 잡의 실행 상태와 잡의 물리적, 논리적 실행 특성 확인 가능."],"metadata":{}},{"cell_type":"code","source":["flightData2015.sort(\"count\").explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) Sort [count#272 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#272 ASC NULLS FIRST, 5)\n   +- *(1) FileScan csv [DEST_COUNTRY_NAME#270,ORIGIN_COUNTRY_NAME#271,count#272] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["### 2.10.1 DataFrame과 SQL\n\n- 사용자가 SQL이나 R, 자바, 파이썬, 스칼라의 데이터프레임으로 비즈니스로직을 표현하면 스파크에서 실제 코드를 실행하기 전에 로직을 기본 실행계획으로 컴파일 한다.\n  - explain으로 실행 계획 확인 가능\n  \n- 스파크 SQL로 DF를 테이블이나 뷰로 등록하면 SQL 사용가능.\n  - 성능차이는 없음"],"metadata":{}},{"cell_type":"code","source":["flightData2015.createOrReplaceTempView(\"flight_data_2015\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["- 두 실행 계획이 동일한 기본 실행 계획으로 컴파일 되는 것을 확인 가능."],"metadata":{}},{"cell_type":"code","source":["# 파이썬 코드\nsqlWay = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\")\n\ndataFrameWay = flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .count()\n\nsqlWay.explain()\ndataFrameWay.explain()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#270], functions=[finalmerge_count(merge count#305L) AS count(1)#293L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#270, 5)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#270], functions=[partial_count(1) AS count#305L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#270] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#270], functions=[finalmerge_count(merge count#310L) AS count(1)#300L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#270, 5)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#270], functions=[partial_count(1) AS count#310L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#270] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql.functions import max\n\nflightData2015.select(max(\"count\")).take(1)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["maxSql = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5\n\"\"\")\n\nmaxSql.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["maxSql.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nTakeOrderedAndProject(limit=5, orderBy=[aggOrder#316L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#270,destination_total#314L])\n+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#270], functions=[finalmerge_sum(merge sum#328L) AS sum(cast(count#272 as bigint))#315L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#270, 5)\n      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#270], functions=[partial_sum(cast(count#272 as bigint)) AS sum#328L])\n         +- *(1) FileScan csv [DEST_COUNTRY_NAME#270,count#272] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["- 실행계획이 총 7단계이다. \n  - csv파일을 read $\\to$ df를 group by $\\to$ df를 sum $\\to$ df 컬럼명 변경 $\\to$ df sort $\\to$ df limit $\\to$ df collect $\\to$ array(...)\n\n- explain에서 출력된 실행계획은 물리적 실행 시점에서 수행하는 최적화로 인해 다르지만, 모든 부분을 포함하고 있음.\n  - partial_sum 함수를 호출할 때 집계가 2 단계로 나누어짐.\n  - 2단계로 나누어지는 이유는 숫자 목록의 합을 구하는 연산이 가환성(commutative)을 갖고 있어 합계 연산시 파티션별 처리가 가능하기 때문."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nTakeOrderedAndProject(limit=5, orderBy=[destination_total#372L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#270,destination_total#372L])\n+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#270], functions=[finalmerge_sum(merge sum#378L) AS sum(cast(count#272 as bigint))#368L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#270, 5)\n      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#270], functions=[partial_sum(cast(count#272 as bigint)) AS sum#378L])\n         +- *(1) FileScan csv [DEST_COUNTRY_NAME#270,count#272] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":25}],"metadata":{"name":"CH2","notebookId":241295041590051},"nbformat":4,"nbformat_minor":0}
