{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"NDQVFVlchRx0","colab_type":"code","outputId":"85a7c694-e4be-4578-b83d-b12819b343e9","executionInfo":{"status":"ok","timestamp":1559017120459,"user_tz":-540,"elapsed":3809,"user":{"displayName":"랄랄라난나","photoUrl":"","userId":"04977961339279299961"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5dDiCNJFjECT","colab_type":"code","colab":{}},"source":["import os\n","path = 'gdrive/My Drive/Colab_Notebooks/cs224n-2019/a2'\n","os.chdir(path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYRP40qjjYtD","colab_type":"code","outputId":"152f1be6-7e89-48f2-d300-084f31f08482","executionInfo":{"status":"ok","timestamp":1559017127334,"user_tz":-540,"elapsed":1733,"user":{"displayName":"랄랄라난나","photoUrl":"","userId":"04977961339279299961"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!dir"],"execution_count":3,"outputs":[{"output_type":"stream","text":["collect_submission.sh  get_datasets.sh\tsgd.py\tw2v-Skipgram.ipynb\n","env.yml\t\t       run.ipynb\tutils\tword2vec.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xAlIoyZVkqN6","colab_type":"code","outputId":"fc3bf6bb-f238-4f9e-dac3-c99c5ab7024f","executionInfo":{"status":"ok","timestamp":1559017135916,"user_tz":-540,"elapsed":3993,"user":{"displayName":"랄랄라난나","photoUrl":"","userId":"04977961339279299961"}},"colab":{"base_uri":"https://localhost:8080/","height":618}},"source":["!sudo bash get_datasets.sh"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2019-05-28 04:18:52--  http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip [following]\n","--2019-05-28 04:18:52--  https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6372817 (6.1M) [application/zip]\n","Saving to: ‘stanfordSentimentTreebank.zip’\n","\n","stanfordSentimentTr 100%[===================>]   6.08M  3.41MB/s    in 1.8s    \n","\n","2019-05-28 04:18:54 (3.41 MB/s) - ‘stanfordSentimentTreebank.zip’ saved [6372817/6372817]\n","\n","Archive:  stanfordSentimentTreebank.zip\n","   creating: stanfordSentimentTreebank/\n","  inflating: stanfordSentimentTreebank/datasetSentences.txt  \n","   creating: __MACOSX/\n","   creating: __MACOSX/stanfordSentimentTreebank/\n","  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSentences.txt  \n","  inflating: stanfordSentimentTreebank/datasetSplit.txt  \n","  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSplit.txt  \n","  inflating: stanfordSentimentTreebank/dictionary.txt  \n","  inflating: __MACOSX/stanfordSentimentTreebank/._dictionary.txt  \n","  inflating: stanfordSentimentTreebank/original_rt_snippets.txt  \n","  inflating: __MACOSX/stanfordSentimentTreebank/._original_rt_snippets.txt  \n","  inflating: stanfordSentimentTreebank/README.txt  \n","  inflating: __MACOSX/stanfordSentimentTreebank/._README.txt  \n","  inflating: stanfordSentimentTreebank/sentiment_labels.txt  \n","  inflating: __MACOSX/stanfordSentimentTreebank/._sentiment_labels.txt  \n","  inflating: stanfordSentimentTreebank/SOStr.txt  \n","  inflating: stanfordSentimentTreebank/STree.txt  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wgkLEl0LhKPl","colab_type":"code","colab":{}},"source":["import random\n","import numpy as np\n","import matplotlib\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","import time\n","\n","from utils.treebank import StanfordSentiment\n","from word2vec import *\n","from sgd import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XzfUGxvWjrFa","colab_type":"code","colab":{}},"source":["# Check Python Version\n","import sys\n","assert sys.version_info[0] == 3\n","assert sys.version_info[1] >= 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUi3FjQWj-0v","colab_type":"code","colab":{}},"source":["# Reset the random seed to make sure that everyone gets the same results\n","random.seed(314)\n","dataset = StanfordSentiment()\n","tokens = dataset.tokens()\n","nWords = len(tokens)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rqij4avAhG5K","colab_type":"code","outputId":"7df76926-8641-416d-bcec-f7fbf490956e","colab":{"base_uri":"https://localhost:8080/","height":756},"executionInfo":{"status":"ok","timestamp":1559024735615,"user_tz":-540,"elapsed":7563123,"user":{"displayName":"랄랄라난나","photoUrl":"","userId":"04977961339279299961"}}},"source":["\n","# We are going to train 10-dimensional vectors for this assignment\n","dimVectors = 10\n","\n","# Context size\n","C = 5\n","\n","# Reset the random seed to make sure that everyone gets the same results\n","random.seed(31415)\n","np.random.seed(9265)\n","\n","startTime=time.time()\n","wordVectors = np.concatenate(\n","    ((np.random.rand(nWords, dimVectors) - 0.5) /\n","       dimVectors, np.zeros((nWords, dimVectors))),\n","    axis=0)\n","\n","wordVectors = sgd(\n","    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n","        negSamplingLossAndGradient),\n","    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=1000)\n","# Note that normalization is not called here. This is not a bug,\n","# normalizing during training loses the notion of length.\n","\n","print(\"sanity check: cost at convergence should be around or below 10\")\n","print(\"training took %d seconds\" % (time.time() - startTime))\n","\n","# concatenate the input and output word vectors\n","wordVectors = np.concatenate(\n","    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n","    axis=0)\n","\n","visualizeWords = [\n","    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n","    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n","    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n","    \"hail\", \"coffee\", \"tea\"]\n","\n","visualizeIdx = [tokens[word] for word in visualizeWords]\n","visualizeVecs = wordVectors[visualizeIdx, :]\n","temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n","covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n","U,S,V = np.linalg.svd(covariance)\n","coord = temp.dot(U[:,0:2])\n","\n","for i in range(len(visualizeWords)):\n","    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n","        bbox=dict(facecolor='green', alpha=0.1))\n","\n","plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n","plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n","\n","plt.savefig('word_vectors.png')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["iter 1000: 25.462040\n","iter 2000: 25.298383\n","iter 3000: 25.066112\n","iter 4000: 24.643775\n","iter 5000: 24.221150\n","iter 6000: 23.827730\n","iter 7000: 23.325909\n","iter 8000: 22.876001\n","iter 9000: 22.430623\n","iter 10000: 21.875617\n","iter 11000: 21.342645\n","iter 12000: 20.930986\n","iter 13000: 20.469180\n","iter 14000: 19.993677\n","iter 15000: 19.524647\n","iter 16000: 19.034326\n","iter 17000: 18.574380\n","iter 18000: 18.166311\n","iter 19000: 17.784676\n","iter 20000: 17.415276\n","iter 21000: 17.064078\n","iter 22000: 16.737958\n","iter 23000: 16.381199\n","iter 24000: 16.014223\n","iter 25000: 15.750859\n","iter 26000: 15.514099\n","iter 27000: 15.201966\n","iter 28000: 14.954310\n","iter 29000: 14.656397\n","iter 30000: 14.334909\n","iter 31000: 14.089950\n","iter 32000: 13.934838\n","iter 33000: 13.678010\n","iter 34000: 13.440420\n","iter 35000: 13.309861\n","iter 36000: 13.131846\n","iter 37000: 12.950496\n","iter 38000: 12.813099\n","iter 39000: 12.661149\n","iter 40000: 12.478976\n","sanity check: cost at convergence should be around or below 10\n","training took 7562 seconds\n"],"name":"stdout"}]}]}