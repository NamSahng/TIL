{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "- Sampling based Inference 중 Gibbs sampling의 Case Study 중 LDA를 알아보자.\n",
    "- Topic Model의 큰 붐과 impact를 주었었다.\n",
    "- Collapsed Gibbs sampling이 어떻게 되는지 보자.\n",
    "\n",
    "\n",
    "### Topic Modeling\n",
    "\n",
    "- <img  src=\"./image/W10-19.PNG\" width=\"90%\">\n",
    "- 토픽 모델링을 살펴보면 다음과같다\n",
    "    - 오바마한 기사들을 10가지 주제로 나누어져있는 것과 시간에 따라 그 비율이 어떻게 나타나는지 볼 수 있다.\n",
    "    - 알 수 없는 대용량의 문서들을 통해 토픽과 토픽들의 비율과 토픽을 설정하기 위한 단어 군들을 알 수 있다.\n",
    "\n",
    "### Latent Dirichlet Allocation\n",
    "\n",
    "- <img  src=\"./image/W10-20.PNG\" width=\"100%\">\n",
    "- LDA는 Soft clustering in Text Data 이며 Text corpus의 구조를 잘 나타내고 있으며(document, word, topic), prior를 사용하는 Bayesian Model이다.\n",
    "    - $ \\alpha, \\beta $는 Dirichlet Distribution prior로 , W는 evidence(word)로 구분되어 색칠이 되어있다.\n",
    "        - $ \\alpha $: 전체 corpus에서의 topic distribution을 가짐.\n",
    "        - $ \\beta $: 특정 topic에 어떤 단어가 얼마나 쓰이는지에 대한 prior knowledge.\n",
    "    - $ Z $: 단어들에 대한 cluster(topic) assignment. $ \\theta $: 문서에 대한 topic assignment\n",
    "    - $ N $: m이라는 문서의 단어의 개수, $ M $: 문서의 개수\n",
    "    - $ \\varphi $ : 토픽마다 등장할 단어의 확률\n",
    "    - $ K $: 토픽의 개수\n",
    "    - $ w $: topic마다 등장할 단어의 확률인 $ \\varphi $ 와 topic assignment인 $ Z $ 로 합쳐저 $ w $가 결정\n",
    "- <img  src=\"./image/W10-21.PNG\" width=\"100%\">\n",
    "- $ \\alpha, \\beta $는 prior로 주어진 것이므로(물론 Learning을 할 수 있지만) 우리가 배우는 LDA에서 Floating하는 $ Z, \\theta, \\varphi $에 대해 잘 Assign을 하면 될 것이고 그중에 가장 Evidence에 근접한 Z를 잘알면 Z, W를 통해 $  \\theta, \\varphi $를 알아 낼 수 있겠다.\n",
    "    - ** Finding the most likely allocation of Z is the key of inference on $ \\theta $ and $  \\varphi $ **\n",
    "\n",
    "\n",
    "\n",
    "#### LDA - Gibbs Sampling on Z\n",
    "\n",
    "- Z에 대한 Allocation (가장 잘 topic assignment를 하는 방법)하는 방법을 Gibbs Sampling으로 한다.\n",
    "\n",
    "- Bayesian Network를 Factorize하는 것 부터 시작해보자.\n",
    "    - $ P(W,Z,\\theta, \\varphi ; \\alpha, \\beta) = \\prod_{i=1}^{K} P(\\varphi_{i} ; \\beta ) \\prod_{j=1}^{M} P(\\theta_{j} ; \\alpha ) \\prod_{l=1}^{N} P(Z_{j,l}|\\theta_{j}) P(W_{j,l}|\\varphi_{z_{j,l}})  \\qquad \\cdots eq(0) $\n",
    "        - 세미콜론은 prior에 대한 정보에 대한 표현\n",
    "        - $ P(\\varphi_{i} ; \\beta ) == P(\\varphi_{i}|\\beta)P(\\beta) $\n",
    "    - eq(0)에서 $ \\theta $ and $ \\varphi $를 collapse해(없에서) $ W, Z, \\alpha , \\beta $만 남길 것이다.\n",
    "        - W는 데이터이고, Z는 깁스샘플링의 대상이며, $\\alpha , \\beta$는 given prior(지식)이기 때문에 \n",
    "    - 그리고 이렇게 줄여진 형태를 Collapsed Gibbs Sampling이라 한다.\n",
    "    \n",
    "\n",
    "- 억지로 없에기 위해 Marginalization on $ \\theta $ and $ \\varphi $\n",
    "    - $ P(W,Z ; \\alpha, \\beta) = \\int_{\\theta} \\int_{\\varphi} P (W,Z,\\theta, \\varphi ; \\alpha, \\beta) d\\theta d\\varphi $ \n",
    "        - 이를 $ \\varphi $와 $ \\theta $에 관한 term으로 나누어 보면\n",
    "    - $ =  \\int_{\\varphi} \\prod_{i=1}^{K} P(\\varphi_{i} ; \\beta ) \\prod_{j=1}^{M} \\prod_{l=1}^{N}  P(W_{j,l}|\\varphi_{z_{j,l}})  d\\varphi \\times \\int_{\\theta} \\prod_{j=1}^{M} P(\\theta_{j} ; \\alpha ) \\prod_{l=1}^{N} P(Z_{j,l}|\\theta_{j}) d\\theta $\n",
    "        - 통계학자들이 적분을 없에기 위해 사용하는 trick을 해보자. \n",
    "    - 위 식을 $ eq(1) \\times eq(2) $로 나누어 보자. \n",
    "\n",
    "\n",
    "- (1) Marginalization on $ \\varphi $\n",
    "    - <img  src=\"./image/W10-22.PNG\" width=\"100%\">\n",
    "    - $ \\varphi $는 Topic x 사전 단어의 matrix 형태이며, 특정 하나의 topic에서 모든 단어의 값의 합은 1이 되도록 만들어져있다.\n",
    "    - (a)의 식에서 개별 $ \\varphi $는 독립적으로 적용되기 때문에 한 topic별로 보아 식 (b)로 바꾸어 (b)처럼 적분 안의 곱을 뺄 수 있다.\n",
    "    - (b)에서는 더이상 내려갈 수 없어, PDF level까지 가야한다.\n",
    "    - 식 (b)에서 앞의 PDF는 디리클레, 뒤의 PDF는 다항분포이다.\n",
    "    - 디리클레 분포의 PDF\n",
    "        - $ x \\sim Dir(\\alpha)  $\n",
    "            - $ \\varphi_{i} $는 X, $ \\beta $는 알파에 연결\n",
    "        - $ P(X|\\alpha) = {{ \\Gamma (\\sum_{i=1}^{K} \\alpha_{i}) }\\over{ \\prod_{i=1}^{K} \\Gamma(\\alpha_{i})}} \\prod_{i=1}^{K}{x_{i}^{\\alpha_{i}-1}} $\n",
    "            - 여기 K는 디리클레분포의 차원, dimension K는 우리의 V(vocabulary)에 연결하여\n",
    "        - 맞춰주면 식 (c)가 나오고, 뒷쪽은 그대로 내려온다.\n",
    "    - 그리고 $ n_{j,r}^{i} $의 counting을 도입한다.\n",
    "        - i번째 topic, j번째 문서, r번째 단어장의 단어(unique word)\n",
    "    - <img  src=\"./image/W10-23.PNG\" width=\"60%\">\n",
    "        - 이때, $ n_{j,17}^{1} = 2 $이고, $ n_{j,17}^{2} = 1 $로 볼 수 있다.\n",
    "    - (c)의 뒷부분 (다항분포)의 부분($ \\prod_{j=1}^{M} \\prod_{l=1}^{N}  P(W_{j,l}|\\varphi_{z_{j,l}}) $)을 위의 counting을 활용해 표현하는 trick으로 표현가능하여 (d)처럼 바뀐다. $ \\prod_{v=1}^{V} \\varphi_{i,v}^{n_{(.),v}^{i}} $\n",
    "        -  $ (.) $는 상관하지 않음(모두)\n",
    "    - (d)에서 dimension이 같은 부분을 합쳐 표현하면 (e)가 된다.\n",
    "    - (e)에서 $ \\varphi $ 상관없는 부분(분수)를 밖으로 빼주고, $ \\varphi_{i,v}^{n_{(.),v}^{i}+\\beta_{v}-1} $과 디리클레 분포의 식을 보면 $ \\alpha_{i} $를 $ n_{(.),v}^{i}+\\beta_{v} $의 형태로 볼 수 있겠다 생각해 없어지게(나누어) 표현하면, 식 (f)가 된다.(\n",
    "    - 식 (f)는 새로운 디리클레 dist가 되고 이를 적분하면 1이 되므로 왼쪽 식만 나오는 식 (g)가 나온다.\n",
    "        - 이것이 통계학자들이 적분을 피하는 즐기는 방법이다.\n",
    "    \n",
    "    \n",
    "- (2) Marginalization on $ \\theta $\n",
    "    - <img  src=\"./image/W10-24.PNG\" width=\"100%\">\n",
    "    - $ \\varphi $와 유사하게 진행이 된다. (디리클레 x 다항)\n",
    "    - $ n_{j,r}^{i} $의 counting으로는 $\\theta_{j,k}$: 특정 doc의 특정 topic에 대한 분포에서의 지수쪽으로 들어감.\n",
    "    \n",
    "    \n",
    "\n",
    "- $ \\theta $ and $ \\varphi $는 같은 메커니즘으로 없앴다.\n",
    "    - 디리클레 PDF x 다항 PDF가 다시 디리클레 PDF가 되면서, 적분 = 1이 되었다.\n",
    "    - In LDA: i.e. $  \\int_{\\theta} \\prod_{j=1}^{M} P(\\theta_{j} ; \\alpha ) \\prod_{l=1}^{N} P(Z_{j,l}|\\theta_{j}) d\\theta $ 에서\n",
    "        - $ P(\\theta_{j} ; \\alpha ) $는 prior로 $ P(\\theta) $\n",
    "        - $ P(Z_{j,l}|\\theta_{j}) $는 likelihood로 $ P(Z_{j,l}|\\theta_{j}) $ 적용이됨.\n",
    "    - 이렇게 likelihood과 prior의 곱이 다시 prior distribution을 만들어 낼 때의 관계를 ** Conjugate Prior **관계라고 한다.\n",
    "    - 이렇게 Conjugate Prior의 관계가 만들어진다면 우리가 했던 Marginaliztion을 적용할 수 있으며, 이를 이용해 쉽게 Gibbs Sampling할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "#### Gibbs Sampling formula \n",
    "\n",
    "- 결국, 지금 까지 수행한 식은 다음과 같다.\n",
    "- $ P(W,Z ; \\alpha, \\beta) = \\prod_{i=1}^{K} { { \\prod_{v=1}^{V} \\Gamma(n_{(.),v}^{i} + \\beta_{v}) \\Gamma( \\sum_{v=1}^{V} \\beta_{v} )  } \\over{ \\prod_{v=1}^{V} \\Gamma( \\beta_{v} )  \\Gamma( \\sum_{v=1}^{V} n_{(.),v}^{i} + \\beta_{v}) }} \\prod_{j=1}^{M} { { \\prod_{i=1}^{k} \\Gamma(n_{(.),v}^{i} + \\alpha_{i}) \\Gamma( \\sum_{i=1}^{k} \\alpha_{i} )  } \\over{ \\prod_{i=1}^{k} \\Gamma( \\alpha_{i} )  \\Gamma( \\sum_{i=1}^{k} n_{(.),v}^{i} + \\alpha_{i} ) }} $\n",
    "    - 여기서, $ \\alpha, \\beta, W $는 prior 또는 Data point이고, Z가 샘플링 Target이 된다.\n",
    "        - Z는 행렬의 형태이며 m번째 문서 l번째 단어에 대한 topic assginment의 형태로 샘플링 가능하도록 수식을 변형하면\n",
    "    - $ P(Z_{(m,l)} = k | Z_{-(m,l)}, W ; \\alpha, \\beta ) = {{ P(Z_{(m,l)} = k , Z_{-(m,l)}, W ; \\alpha, \\beta )}\\over{P(Z_{-(m,l)}, W ; \\alpha, \\beta )}} \\propto P(Z_{(m,l)} = k , Z_{-(m,l)}, W ; \\alpha, \\beta ) $\n",
    "    - 결국, $ P(Z_{(m,l)} = k , Z_{-(m,l)}, W ; \\alpha, \\beta ) $ 이 식을 어떻게 계산할 수 있을까로 연결이 된다.\n",
    "    \n",
    "\n",
    "- <img  src=\"./image/W10-25.PNG\" width=\"100%\">\n",
    "    - 노랑 밑줄: constant, 초록: 변형\n",
    "    - 연결된 다음 식에서 밑줄친 부분은 topic assignment에 영향을 받지 않는 상수가 되어, 끄집어낸다. (샘플링 할때, 변화하지 않음)\n",
    "    - 이제, 문서를 m인 특정 문서로 고정하고, 단어 l을 특정 단어로 고정하자.\n",
    "        - 감마함수 안의 V는 바꾸기 힘들다.\n",
    "    - 마지막으로 분모의 constant를 없앤다.\n",
    "    \n",
    "    \n",
    "- <img  src=\"./image/W10-26.PNG\" width=\"100%\">\n",
    "    - 위에서 한 부분을 가져오고, $ n_{j,r}^{i,-(m,l)} $는 $  n_{j,r}^{i}: $j번째 문서의 r번째 사전의 단어가 i번째 topic에 assign되는 횟수에서 m번째 문서, l번째 단어는 제외한 notation 추가.\n",
    "        - inference할 내용이기 때문에\n",
    "    - 그리고 i는 k번째는 빼고 돌도록 만들고\n",
    "    - k번째 단어(m, l번째 단어에)에 하나 더 assign할 것이므로 count에 +1해 준다.\n",
    "    - 그리고 하나의 트릭을 더 사용한다. 파란 박스는 그대로 내려오고 초록박스는 다음과 같이  변형되어 내려온다.\n",
    "        - 감마의 정의에 의해 +1이 사라지면서, 노랑 원의 식은 파랑 박스로 들어갈 수 으며 찐파랑이 된다.(곱셈의 k neq 1부분이 없어짐)\n",
    "    - 그리고 찐파랑의 부분은 영향을 안받는 constant가 되게된다.\n",
    "    - 따라서 의미있는 간소화된 식 찐노랑 박스의 식에서의 정보를 갖고 깁스샘플링을 할 것이다.\n",
    "    \n",
    "\n",
    "- 프로그램을 위한 알고리즘 형태의 LDA는 다음과 같다.\n",
    "    - <img  src=\"./image/W10-27.PNG\" width=\"100%\">\n",
    "    - Z를 Assign하면 doc-topic prob.인 $ \\theta $ 그리고 topic-word prob인 $ \\varphi $에 대해 estimation이 가능하다.\n",
    "    - converge 할때마다 loop를 돌게 하려면 perplexity를 사용한다. perplexity의 계산은 좀 빡세다.\n",
    "        - [딥러닝을 이용한 자연어처리에서의 perplexity 설명](https://github.com/NamSahng/Summary/blob/master/NLP%26DL_CKH/5.%20Neural%20Language%20Models%20-%20part2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Reference:\n",
    "\n",
    "- 문일철 교수님, 인공지능 및 기계학습 개론 II, https://www.edwith.org/machinelearning2__17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
