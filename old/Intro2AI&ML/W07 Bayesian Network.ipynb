{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECAP ON PROBABILITY \n",
    "\n",
    "### 1) Probabilities\n",
    "\n",
    "- Frequentistic View: It is the relative frequency with which an outcome would be obtained if　the process were repeated a large number of times under similar　conditions.\n",
    "- Bayesian View: Probability is your degree of belief in an outcome.\n",
    "\n",
    "- 확률 자체로 사용보다 조건부 확률, 무엇이 Given 일 때의 확률을 더 많이 사용한다. 결합확률 또한 중요한데 이는 많은 정보를 주기 때문이다.\n",
    "\n",
    "### 2) Joint & Conditional Probability\n",
    "\n",
    "- Conditional Probability\n",
    "    - P(A = true | B = true) \n",
    "\n",
    "- Joint Probability\n",
    "<img  src=\"./image/W7-1.PNG\" width=\"60%\">\n",
    "\n",
    "\n",
    "### 3) Computing with Probabilities: Law of Total Probability & Chain Rule (Factorization)\n",
    "-  Ransdom variable(확률변수): 표본 공간에 일정한 확률을 가지고 발생하는 사건에 수치를 일대일 대응한 함수. '주사위에서는 주사위를 한번 던질 때 나오는 숫자’\n",
    "\n",
    "- Law of Total Probabilities\n",
    "    - a.k.a \"Summing out\" or marginalization\n",
    "    - $ P(a) = \\sum_{b}P(a, b) = \\sum_{b}P(a | b) P(b) $\n",
    "    - When B is any random variable\n",
    "    - Joint를 알면 Marginalization을 통해 개별에 대해서 알 수 있다.\n",
    "        - given a joint distribution (e.g., P(a,b,c,d))\n",
    "        - We can obtain any “marginal” probability (e.g., P(b)) by summing out the other variables.\n",
    "        - $ P(c|b) = \\sum_{a}\\sum_{d}P(a, c, d | b ) = {{\\sum_{a}\\sum_{d}P(a, c, d , b )} \\over {P(b)}} $\n",
    "        - 1/P(b) 는  just a normalization constant\n",
    "    - 즉, joint를 알면 개별 individual prob이나 conditional prob도 알 수 있지만 \n",
    "    - BUT 근데 지수적으로 알아야하는 값들이 늘어남 (나이브 베이즈의 나이브 이유)\n",
    "            \n",
    "            \n",
    "          \n",
    "- Chain Rule or Factorization\n",
    "    - P(a, b, c, … z) = P(a | b, c, …. z) P(b, c, … z)\n",
    "    - 따라서 계속 해보면,\n",
    "        - P(a, b, c, … z) = P(a | b, c, …. z) P(b | c,.. z) P(c| .. z)..P(z)\n",
    "\n",
    "\n",
    "### 4) Independence\n",
    "\n",
    "- Marginal Independence: 두 변수가 독립이면 다음과 같다.\n",
    "    - P(A|B) = P(A)\n",
    "    - P(A,B) = P(A)P(B)\n",
    "    - P(B|A) = P(B)\n",
    "        - 이를 이용해서 동전던지기의 확률과 같은 것을 계산할 때\n",
    "        - 앞 뒤 사건과 독립이라 쉽게 확률을 계산할 수 있었다. ( $ \\prod $를 이용해)\n",
    "\n",
    "- Conditinoal Independence\n",
    "\n",
    "<img  src=\"./image/W7-2.PNG\" width=\"60%\">        \n",
    "- 그리고 다음과 같으면, Conditinoal Independence라고 한다. (나이브베이즈에서 사용) \n",
    "    - P(OfficerA=Go|OfficerB=Go,Commander=Go) = P(OfficerA=Go|Commander=Go)\n",
    "- 위 상황에서\n",
    "\n",
    "- P(OfficerA=Go|OfficerB=Go) > P(OfficerA=Go) 이러면  This is not marginally independent!\n",
    "    - X and Y are independent if and only if P(X)=P(X|Y)\n",
    "    - Consequently, P(X,Y)=P(X)P(Y)\n",
    "    - 이래야 marginally independent!한 것.\n",
    "\n",
    "## Bayesian Network\n",
    "\n",
    "- 나이브 베이즈도 베이지안 넷웤이였고, 최신모델들도 BN이 많다.\n",
    "- BN은  Random Variables, 조건부 독립 등의 graphical Notation이며 전체 결합확률을 컴펙트하게 보여준다.\n",
    "\n",
    "- $ P(Y = y) \\prod_{1 \\le i \\le d} P(X_{i}=x_{i}|Y = y) $ 의 Graphical Representation\n",
    "<img  src=\"./image/W7-3.PNG\" width=\"40%\">      \n",
    "\n",
    "- A graphical notation of\n",
    "    - Random variables\n",
    "    - Conditional independence (조건부 독립이 이미 가정되어있다! Encoded)\n",
    "    - To obtain a compact representation of the full joint distributions\n",
    "- Syntax\n",
    "- A acyclic and directed graph\n",
    "- A set of nodes\n",
    "    - A random variable\n",
    "    - A conditional distribution given its parents\n",
    "    - $ P (X_{i}|Parents(X_{i})) $\n",
    "- A set of links:  Direct influence from the parent to the child\n",
    "\n",
    "\n",
    "- cf) 베이지안 네트워크는 Directed Ayclic Graph 이다. 나이브 베이즈, k means, gmm, hmm이 베이지안 네트워크의 예시들이다. 로지스틱 리그레션은 Undirected Graphical 모델이며 이는 CRF의 제한된 버전이다. 로지스틱회귀가 하나의 NN의 레이어의 퍼셉트론이다. 그리고 조경현 교수님은 NN도 DAG의 관점으로 보는 것이 유리하다고 말씀하심.\n",
    "    - from https://stats.stackexchange.com/questions/94511/difference-between-bayes-network-neural-network-decision-tree-and-petri-nets\n",
    "    - https://github.com/NamSahng/Summary/blob/master/NLP%26DL_CKH/Basics-HypothesisSet.ipynb\n",
    "\n",
    "\n",
    "### 1) Typical Local Structures\n",
    "\n",
    "- Common parent\n",
    "    - Fixing \"alarm\" decouples \"JohnCalls\" and \"MaryCalls\"\n",
    "    - $ J \\perp M|A $\n",
    "        - $ P(J,M|A) = P(J|A)P(M|A) $\n",
    "\n",
    "- Cascading\n",
    "    - Fixing \"alarm\" decouples \"Buglary\" and \"MaryCalls\" \n",
    "    - $ B \\perp M|A $\n",
    "        - $ P(M|M,A) = P(M|A) $\n",
    "        \n",
    "- V-Structure\n",
    "    - Fixing \"alarm\" couples \"Buglary\" and \"Earthquake\" \n",
    "    - $ ~ (B \\perp E | A) $\n",
    "        - $ P(B,E,A) = P(B)P(E)P(A|B,E) $\n",
    "\n",
    "<img  src=\"./image/W7-4.PNG\" width=\"40%\"> \n",
    "        \n",
    "- Common parent와 Cascading의 관계의 경우에는 A를 알면 독립이 되는데\n",
    "- V-Structure에서는 특정 관계가 생긴다!\n",
    "\n",
    "### 2) Bayes Ball Algorithm & Markov Blaket\n",
    "\n",
    "- Bayes Ball : 독립을 판정할 때 사용 (All none given에서는 위와 같이 V 구조만 안 굴러감.)\n",
    "- Markov Blanket: P(A|blanket, B)=P(A|blanket)  \n",
    "    - Blanket={parents, children, children’s other parents}\n",
    "    - Random variable A에 대하여 다른 12개의 랜덤 버라이어블에 대해 6개만 알면(given)이면 conditional independent가 만족된다. (D-Separation도 비슷한 내용이다.)\n",
    "    \n",
    "<img  src=\"./image/W7-5.PNG\" width=\"25%\">     \n",
    "\n",
    "### 3) Factorization of Bayesian Network\n",
    "\n",
    "- 조건부 독립이 인정? 확인 이되면 , 결합확률은 빡세니 팩토라이즈하면(그냥 정리에 의해 할 수 있는 거니까 이건)\n",
    "- P(a, b, c, … z) = P(a | b, c, …. z) P(b | c,.. z) P(c| .. z)..P(z) 여기서 중간 것들을 간단히 할 수 있다.\n",
    "    - ex) P(a | b, c, d) = P(a | b) -> 알아야하는 파라미터 개줄음 (파라미터 8개에서 2개 다 바이너리이면)\n",
    "    \n",
    "<img  src=\"./image/W7-6.PNG\" width=\"40%\">     \n",
    "\n",
    "- Plate Notation: 간단하게 표현하자.\n",
    "\n",
    "<img  src=\"./image/W7-7.PNG\" width=\"25%\">\n",
    "\n",
    "\n",
    "### 4) INFERENCE ON BAYESIAN NETWORKS\n",
    "\n",
    "<img  src=\"./image/W7-8.PNG\" width=\"45%\">\n",
    "\n",
    "- ** Inference Question 1 - Likelihood : P(B=true, MC=true)=? **\n",
    "    - Given a set of evidence, what is the likelihood of the evidence set?\n",
    "        - $ X =  \\left\\{ X_{1}...X_{N} \\right\\} $: all random variables\n",
    "        - $ X_{V} = \\left\\{ X_{k+1}...X_{N} \\right\\} $: evidence variables, ($ x_{V} $ : eveidence values)\n",
    "        - $ X_{H} = X - X_{V} =  \\left\\{ X_{1}...X_{k} \\right\\} $: hidden variables\n",
    "\n",
    "    - General Form:\n",
    "        - $ P (x_{V}) = \\sum_{X_{H}}P(X_{H}, X_{V}) = \\sum_{x_{1}}\\cdots\\sum_{x_{k}} P(x_{1}...x_{k}, x_{V}) $\n",
    "        - likelihood of $ (x_{V}) $ 를 구해라\n",
    "        \n",
    "     \n",
    "        \n",
    "- ** Inference Question 2 - Conditional Probability : P(A|B=true, MC=true)=? **\n",
    "    - Given a set of evidence, what is the conditional probability of interested hidden variables?\n",
    "        - $ X_{H} = \\left\\{ Y, Z \\right\\} $\n",
    "            - Y: interested hidden variables\n",
    "            - Z: uninterested hidden variables\n",
    "    - General Form: \n",
    "        - $ P(Y|x_{v}) = \\sum_{z}P(Y,Z=z | x_{V}) = \\sum_{z} {{P(Y,Z,x_{V})}\\over{P(x_{V})}}  =  \\sum_{z} {{P(Y,Z,x_{V})}\\over{\\sum_{y,z}P(Y=y,Z=z,x_{V})}} $\n",
    "    - Conditional probability of Y given $ x_{V} $ 를 구해라.\n",
    "    \n",
    " \n",
    "    \n",
    "- ** Inference Question 3 - Most Probable Assignment : argmaxaP(A|B=true, MC=true)=? **\n",
    "    - Given a set of evidence, what is the most probable assignment, or explanation, given the evidence?\n",
    "        - Some variables of interests \n",
    "        - Need to utilize the inference question 2 (Conditional probability)\n",
    "        - Maximum a posteriori configuration of Y\n",
    "    - Applications of a posteriori (사후분포의 기능!)\n",
    "        - 예측: Prediction $ \\to P( A | B , E ) $\n",
    "        - 진단: Diagnosis $ \\to P( B , E | A) $\n",
    "        \n",
    "        \n",
    "#### Marginalization and Elimination\n",
    "\n",
    "- $ P(a = True, b = True, mc = True) \\sum_{JC}\\sum_{E}P(a,b,E,JC,MC) = \\sum_{JC}\\sum_{E} P(JC|a)P(MC|a)P(a|b,E)P(E)p(b) $\n",
    "    - 이건 계산이 너무 빡세니\n",
    "- $ P(a, b , mc) =  \\sum_{JC}\\sum_{E}P(a,b,E,JC,MC) = p(b)P(MC|a) \\sum_{JC}P(JC|a) \\sum_{E} P(a|b,E)P(E)  $\n",
    "    - 이렇게 해야 계산이 훨 쉬워진다.\n",
    "- 이런 성질을 이용해,\n",
    "- ex) $ P(E|JC,MC) = \\alpha P(E,JC,MC) $ 을 이용하고 풀어보자\n",
    "- $ P(E,JC,MC,b,a) = \\alpha P(e) \\sum_{B} P(b) \\sum_{A} P(a|b,E)P(JC|a)P(MC|a)  $\n",
    "    - topological order로 라인업하고\n",
    "    - Consider a prob distribution as a function하자. $ f_{E =t} = 0.002 $\n",
    "- $ = \\alpha f_{E}(e) \\sum_{B} f_{B}(b) \\sum_{A} f_{A}(a,b,E)f_{JC}(a)f_{MC}(a) $\n",
    "    - 왜 function notation? \n",
    "    - 조건부와 같은 것들의 의미를 없애 계산을 쉽게하려고\n",
    "- 그리고 다음과 같이 계산한다.\n",
    "\n",
    "<img  src=\"./image/W7-9.PNG\" width=\"60%\">        \n",
    "\n",
    "\n",
    "\n",
    "### 5) Potential Function\n",
    "\n",
    "- Potential Function: a function which is not a probability function yet, but once normalized it can be a probability distribution function.\n",
    "    - probabilistic graphical models로 만들고 싶은데, 아직 그렇게 못한다. 잠재력은 있지만 아직 아닌 것\n",
    "        \n",
    "- P(A,B,C,D) = P(A|B)P(B|C)P(C|D)P(D)인 다음 그래프에서 다음과 같이 Clique와 Separator로 구분해 만들어 보자.\n",
    "<img  src=\"./image/W7-10.PNG\" width=\"40%\">        \n",
    "- 그러면 노드와 링크에 대해 다음과 같이 생각할 수 있다.\n",
    "    - $ \\psi(a,b) , \\psi(b,c) , \\psi(c,d)  $  and $ \\phi(b), \\phi(c) $\n",
    "- 그리고 다음과 같은 2가지 방법으로 function을 set up 할 수 있다.\n",
    "    - P(A|B)P(B|C)P(C|D)P(D)을 맨 오른 쪽 식과 같이 연결하려는 것.\n",
    "<img  src=\"./image/W7-11.PNG\" width=\"55%\"> \n",
    "- 그런데, 두번째 방법은 직관적이지만 노드가 결합확률로 잡혀져있어 문제가 있어 보인다.\n",
    "\n",
    "- Marginalization 도 가능:\n",
    "    - $ \\psi(w) = \\sum_{v-w} \\psi(v)  $\n",
    "    - Constructing a potential of a subset (w) of all variables (v)\n",
    "\n",
    "\n",
    "- Absorption in Clique Graph\n",
    "    - Absorption이라하는 클리크 그래프에서 operation을 알아보자.\n",
    "    - 이를 통해 확률을 inference\n",
    "    - Let’s assume $ P(B) = \\sum_{A} \\psi(A,B) = \\sum_{C} \\psi(B,C) = \\phi(B)  $\n",
    "        - 그리고 여기서 A에서 관측이 되면($ P(A,B) \\to P(A=1,B)  $), 프사이에 값들이 변화하면서\n",
    "        - observation이 propagate한다.\n",
    "        - ** Belief Propagation **\n",
    "    - How to propagate the belief? Absorption (update) rule!\n",
    "        - Assume $ \\psi^{*}(A,B),\\psi(B,C), \\phi(B) $ \n",
    "        - Define the update rule for separators: $  \\phi^{*}(B) = \\sum_{A} \\psi^{*}(A,B) $\n",
    "        - Define the update rule for cliques: $ \\psi^{*}(B,C) = \\psi(B,C) {{\\phi^{*}(B)}\\over{\\phi(B)}}  $\n",
    "    - 위에서 가정했듯이 했기 때문에 이러한 위와 같이 표현할 수 있고\n",
    "    - 이는 Local Consistency를 Guarantee한다. $ \\to $ Global consistency after iterations\n",
    "<img  src=\"./image/W7-12.PNG\" width=\"45%\">  \n",
    "\n",
    "- Simple Example of Belief Propagation\n",
    "<img  src=\"./image/W7-13.PNG\" width=\"80%\">  \n",
    "\n",
    "    - 이를 통해 Local Consistency를 만족함을 알 수 있고,\n",
    "    - 관측이 있는 것에서 알고싶은 확률에 대해  Most probable Assignment 계산도 가능하다. \n",
    "    - 이런식으로 큰 BN에 대해, 특정 Observation이 있으면\n",
    "    - Belief를 그냥 summation하는 것보다 효율적으로 전파하고, 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "<img  src=\"./image/W7-14.PNG\" width=\"60%\">  \n",
    "<img  src=\"./image/W7-15.PNG\" width=\"60%\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Reference:\n",
    "\n",
    "- 문일철 교수님, 인공지능 및 기계학습 개론 II, https://www.edwith.org/machinelearning2__17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
