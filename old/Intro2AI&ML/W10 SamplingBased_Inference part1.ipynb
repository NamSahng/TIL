{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W10 Sampling Based Inference part1\n",
    "\n",
    "- 지금까지 EM을 통해 parameter를 Inference 해보았다.\n",
    "- EM과 완전히 다른 방법인 Sampling Based Inference을 W10 에 배워보자.\n",
    "\n",
    "\n",
    "## Basic Sampling Methods\n",
    "\n",
    "### A. Forward Sampling\n",
    "\n",
    "- 가장 기본적인 Sampling 방법으로 Topological order(ex Bayesian Network)에 따라 Sample을 Generate한다.\n",
    "\n",
    "- ex) <img  src=\"./image/W7-8.PNG\" width=\"45%\">\n",
    "\n",
    "- 그리고 이렇게 Sample을 Topological order에 따라 많이많이(10만개 100만개 정도) 만든 다음에 Count하면 다음과 같은 것을 구할 수 있다.\n",
    "    - ex 1) $ P(E = T | MC = T ) $같은 것(Inferenc Question 2. Conditional Probability). 근데 바로 구하긴 힘드니, $ {{P(E = T , MC = T )}\\over{P(MC = T)}} $를 다 세서 원하는 확률을 구할 수 있다.\n",
    "    - W7에서는 variable Elimination을 사용해 구했지만, 지금 이 방법은 아주 쉬우면서 무식하고 컴퓨터가 따라하기 쉬운 방법이다.\n",
    "    - 문제점: Random하게 계속 샘플하면, 정확한 값을 찾을 수 있지만 지금 방법은 그렇지 않음\n",
    "    - 그러나 아주 많이 반복하면 오차가 무시할 수 있을 정도로 작아지겠지만, 시간이 오래걸린다. 그래서 현실에 쓰이긴 어렵다.\n",
    "    \n",
    "    - ex 2) Forward Sampling in GMM <img  src=\"./image/W10-1.PNG\" width=\"90%\">\n",
    "    - $ \\pi $를 통해 z를 샘플링하고, 평균과 분산 파라미터를 통해 x, mixture distribution을 샘플링 할 수 있다.\n",
    "    - 이렇게 샘플링하여 모드(?)나 분포의 파라미터를 찾을 수 있다.\n",
    "\n",
    "\n",
    "### B. Rejection Sampling\n",
    "\n",
    "- Forward Sampling에는 Topological order에 따라 Sample을 Generate 계속한다음에 count하는 것이다.\n",
    "    - ex) W7-8의 그림에서 P(E=T|MC=T,A=F)을 알고 싶을 때, \n",
    "    - Topology를 따라 중간에 우리의 조건에 안맞으면 reject하고 (ex) E=T 이후에 A = T이면 sampling멈추고 reject)\n",
    "    - Count(E=T,MC=T,A=F)/# of Samples을 답한다.\n",
    "\n",
    "\n",
    "- Rejection Sampling from Numerical View\n",
    "    - 수치적인 관점에서의 Rejection Sampling (위에는 discrete한 경우에서이다)\n",
    "    - q(x)는 Normal을 따르고, p(x)는 알아내고 싶은 mixture distribution이라고 해보자.\n",
    "    - 그리고 q(x)는 p(x)를 envelope할 수 있게(감쌀 수 있게) M을 곱하자.\n",
    "    - <img  src=\"./image/W10-2.PNG\" width=\"80%\">\n",
    "    - 그리고 xi라는 지점에서 sampling되었을 때, p(x)의 파라미터는 알 수 없지만 evaluation을 할 수 있어 p(xi)의 값(높이)은 알 수 있다면, \n",
    "    - q(xi)/Mq(xi)을 이용해, 샘플링된 xi값의 y값이 q(xi)/Mq(xi)보다 크면 reject하며 그 부분은 그림으 빗금친 부분일 것이다.\n",
    "    - Mq(x)가 p(x)를 envelope하지 않으면, rejection sampling이 안돌아가며 이를 위해 M이 너무 크면 Rejection Region이 커지는 문제로 시간상 문제가 생길 수 있다.\n",
    "\n",
    "\n",
    "- Rejection Sampling in GMM\n",
    "    - <img  src=\"./image/W10-3.PNG\" width=\"70%\">\n",
    "    - Q를 어떻게 envelop하게 하냐에 따라 샘플링 된것이 다르다. 그냥 노말 하나로 하면 3번째 봉우리가 잘 언더샘플링 된 것을 볼 수 있다.\n",
    "        - 그냥 노말인 것도 M(3인 부분)값을 좀더 높여서 하면 언더샘플링 이슈 어느정도 해결가능하지만, 그러면 Rejection이 너무 많아짐.\n",
    "        - 두번째 mixture로 한 Q는 평균을 어느 정도 맞춰 준것을 볼 수 있다.\n",
    "    - 여전히 현실에서 쓰기는 힘들어보인다.\n",
    "\n",
    "\n",
    "### Importance Sampling\n",
    "\n",
    "- Rejection없이 샘플링 하여 낭비를 줄이자.\n",
    "- Sampling의 목적은 generating PDF가 아니라, \n",
    "    - Calculating the expectation of PDF or Calculating a certain probability에 있다.\n",
    "    - 기댓값이나 특정 확률을 구하는\n",
    "    \n",
    "\n",
    "- 함수 f에 대한 기대값을 계산\n",
    "    - $ E(f) = \\int f(z)p(z) dz = \\int f(z){{p(z)}\\over{q(z)}}q(z) dz $ \n",
    "    - $ \\cong {{1}\\over{L}} \\sum_{l=1}^{L} {{p(z^{l})}\\over{q(z^{l})}} f(z^{l}) $\n",
    "        - 무한하게 샘플링할 수 없으니 q(z)dz를 $ {{1}\\over{L}} \\sum_{l=1}^{L}  $ normalizing constant로 사용해 summation으로 표현.\n",
    "        - z는 sample이고 L은 sample횟수 \n",
    "        - $ {{p(z^{l})}\\over{q(z^{l})}} f(z^{l}) $는 다 계산 가능하다\n",
    "        - p(z)는 evaluation할 수있다 가정(위처럼) , q(z)는 우리가 sampling하기 편한 distribution\n",
    "    - 이 때, $ {{1}\\over{L}} \\sum_{l=1}^{L} f(z^{l})$ 이거는 평균이므로,\n",
    "    - $  {{p(z^{l})}\\over{q(z^{l})}}  $의 역할은 가중치로 전체는 가중평균을 나타냄을 알 수 있으며 가중치를 $ r^{l} =  {{p(z^{l})}\\over{q(z^{l})}}  $ 로 표현한다.\n",
    "        - 만약, $ p(z)^{l} $와 $ q(z)^{l} $가 정규화가 안되있으면, 다음과 같이 정규화 할 수 있다.\n",
    "        - $ E(f) \\cong {{1}\\over{L}} \\sum_{l=1}^{L} {{p(z^{l})}\\over{q(z^{l})}} f(z^{l})  = {{1}\\over{L}} {{z_{q}}\\over{z_{p}}}  \\sum_{l=1}^{L} {{p(z^{l})}\\over{q(z^{l})}} f(z^{l})  $\n",
    "\n",
    "- example $ P(Z>1) $\n",
    "    -  $ P(Z>1) = \\int_{1}^{\\infty} 1_{z>1}p(z) dz = \\int_{1}^{\\infty} 1_{z>1}{{p(z)}\\over{q(z)}}q(z) dz   $\n",
    "        - Z가 1보다 큰 것만 적분할 때, z\n",
    "    - $ \\cong {{1}\\over{L}} \\sum_{l=1}^{L} {{p(z^{l})}\\over{q(z^{l})}} 1_{Z^{l}>1} $\n",
    "        - 1보다 큰 zl에 대해, zl에 대해 evaluation한 p(zl)값을 sampling distribution에서 나온 zl로 나누어 계산할 수 있다.\n",
    "    \n",
    "    \n",
    "- Discrete한 도메인에서의 예제\n",
    "    - <img  src=\"./image/W10-4.PNG\" width=\"100%\">\n",
    "    - 최종적으로 우리가 관심있었던 $ P(E = T| MC = T, A = F) $의 case의 importance와 전체 likely의 weight를 나누어 normalize하면 된다.\n",
    "        - $ {{1}\\over{L}} \\sum_{l=1}^{L} {{p(z^{l})}\\over{q(z^{l})}} f(z^{l}) $ 이  SumSW/ NormSW로 대응된다.\n",
    "        \n",
    "        \n",
    "## Sampling Based Inference\n",
    "        \n",
    "- Rejection Sampling과 Importance Sampling은 아직까지 연구되는 주제이다.\n",
    "- 하지만, ML에서 가장 많이 사용하는 Sampling기반의 inference는 Gibbs Sampling이다.\n",
    "    - 그리고 깁스샘플링은 Metropolis-Hastings Algorithm의 special한 case에 속한다.\n",
    "- Metropolis-Hastings Algorithm, Gibbs Sampling에 대해 알아보고 실제로 적용할 때는 어떻게 사용되는지 알아보자.\n",
    "\n",
    "- Detour: EM Algorithm\n",
    "\n",
    "- <img  src=\"./image/W8-7.PNG\" width=\"80%\">\n",
    "    -  $ \\sum_{Z}P(X,Z|\\theta) \\to ln(\\sum_{Z}P(X,Z|\\theta)) $에서 Z에 대해 Marginalize out 해야했기 때문에 계산하기 어려웠다.\n",
    "    - 이 Sampling Based Inference가 EM알고리즘에 활용되는 부분은 Expectation의\n",
    "        - ** Assign Z by $ P(Z|X,\\theta) $ ** 부분이며\n",
    "        - 이 부분을 이전에는 optimize하여 Z를 assign했었는데, 이를 sampling기반으로 해보자는 것이다.\n",
    "    - 사실 Sampling Based Inference이 EM과 같은 GMM에 잘 적용되기보다 여러 단계의 latent variable들이 순차적으로 연결되어 있을 때 잘 적용된다.\n",
    "        - ex) LDA(Latent Dirichlet Allocation): text분야의 soft clustering 에서\n",
    "        - Gibbs Sampler가 Collapse되어서 아주 잘 활용될 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "### Markov Chain\n",
    "\n",
    "- EM이 결국에는 $ \\theta $를 optimize하는 것이 목적인데, 그 과정에서 Z도 나오는 것이다. \n",
    "- 여기서는 Optimization이아니라 Z를 지속적으로 sampling하다 보니 어느 순간 Optimized된 $ \\theta $를 구하는 것이 다를 것이다.\n",
    "    - Forward, Rejection, Importance Sampling은 i.i.d였다. (이전 샘플과 현재한 샘플이 독립)\n",
    "    - 하지만, 이번에 배우는 Gibbs Sampling은 Markov Chain을 활용해 샘플링하여 이전에 버렸던 샘플도 활용해 샘플링한다.\n",
    "\n",
    "\n",
    "- Markov chain\n",
    "    - 노드: Each node has a ** probability distribution of states ** 확률 분포로 갖고 있다. \n",
    "        - i.e.) The probability that a state is the current state of a system\n",
    "        - Concrete observation of a system: [1 0 0] $ to $ the system is at the first state\n",
    "        - Stochastic observation of a system: [0.7 0.2 0.1] $ to $ the system is likely at the first state (3개의 discrete state에서 다루고 있다.)\n",
    "        - The node has a vector of state probability distribution\n",
    "    - 링크: Each link suggests a probabilistic state transition\n",
    "        - If a system is at the first state, the probability distribution of the next state is [0.3 0.4 0.3]\n",
    "        - The link has a matrix of state transition probability distribution. (Transition Matrix라고 함)\n",
    "\n",
    "    - ex) system에 3개의 state가 있을 때 <img  src=\"./image/W10-5.PNG\" width=\"80%\">\n",
    "\n",
    "\n",
    "- a. Accessible & Communicate\n",
    "    - $ i \\to j $: State j is ** Accessible ** from i if $ T_{i,j}^{k} > 0 \\space and \\space k \\geq 0 $ \n",
    "        - state가 천이(transition이)가 가능하다.(0이 아니다.)\n",
    "    - $ i \\leftrightarrow j $ : State  i and j ** Communicate ** if $ i \\to j $ and $ j \\to i $\n",
    "        - 양방향이 가능하다.\n",
    "\n",
    "- b. Reducibility & Irreducible\n",
    "    - a Markov Chain is ** Irreducible ** if $ i \\leftrightarrow j, \\forall i \\in S, \\forall j \\in S $ \n",
    "        - Irreducible하면 지속적으로 이동이 가능 할 것이다.\n",
    "        - [출처 Kim Hyungjun](https://medium.com/@kim_hjun/markov-chain-stationary-distribution-5198941234f6) <img  src=\"./image/W10-7.PNG\" width=\"80%\">\n",
    "        \n",
    "- c. Periodicity & aperiodic\n",
    "     - State i has period d if $ d = gcd \\left\\{ n:T_{i,j}^{n} \\right\\} $\n",
    "         - Greatest Common Divisor(최대공약수)\n",
    "         - ex) State i에 방문하는 peroid가 4, 8, 12이면 period를 4로 보겠다.\n",
    "     - If d=1, State i is ** aperiodic **\n",
    "         - ex) 1, 3, 4, 11로 방문하면 aperiodic\n",
    "     \n",
    "- d. Transience & Recurrent\n",
    "    - State j is ** Recurrent ** if $ P(inf(t>1 : X_{t} = j) < \\infty | X_{0} = j) = 1$\n",
    "        - state j에 특정 타임에 방문을 했는데, 나중에 방문할 확률이 분명히 등장한다면 Recurrent라 한다.\n",
    "    - 그렇지 않으면 else ** transient **\n",
    "         \n",
    "- e. Ergodicity\n",
    "    - a state is ** Ergodic ** if the state is (positive) ** recurrent ** and ** aperiodic **\n",
    "        - periodic하게 언젠간 방문한다 하면 Ergodic하다.\n",
    "    - Markov chain is ergodic if all states are ergodic\n",
    "        - 모든 state가 Ergodic하면 Ergodic한 Markkov Chain이다 한다.\n",
    "        \n",
    "\n",
    "### Stationary Distribution\n",
    "\n",
    "- [Linear Algebra에서 Markov Matirx와 Steady State](https://github.com/NamSahng/Summary/blob/master/LinearAlgebra%26ItsApplications_LSH/Chapter5.ipynb)\n",
    "\n",
    "- Stationary Distribution이 Metropolis-Hastings Algorithm을 작동하게하는 핵심이다.\n",
    "\n",
    "\n",
    "- Return time 먼저 정의:\n",
    "    - $ RT_{i} = min \\left\\{ n > 0: X_{n} = i | X_{0} = i \\right\\} $\n",
    "    - 특정 타임에서 state i를 방문했는데, 바로 다음에 방문하는 time은?\n",
    "    \n",
    "\n",
    "- Limit Theorem of Markov Chain: OR이나 산공쪽의 개념으로 완벽하게 커버는 못하지만, 필요한 것을 보자.\n",
    "    - Markov Chain이 Ergodic하고 irreducible하면, 다음이 성립한다.\n",
    "    - $ \\pi_{i} = lim_{n \\to \\infty} T_{i,j}^{(n)} = {{1}\\over{E\\left[ RT_{i} \\right]}} $\n",
    "        - $ \\pi_{i} $는 모든 state마다 정의 되는 그런 값인데, 특정 state에 system이 있을 확률.\n",
    "        - 현재, Stationary Distribution에서 다음 transition을 해도 Stationary Distribution이 바뀌지 않음.(Steady State와 비슷해 보임)\n",
    "        - $ \\pi T = \\pi $\n",
    "    - 그리고 이 $ \\pi $가 Expected Return Time의 역수와 같아진다.\n",
    "    - $ \\pi_{i} $는 Uniquely determined 되며 다음과 같이 정할 수 있다.\n",
    "        - $ \\pi_{j} = \\sum_{i\\in S} \\pi_{i}T_{i,j} \\qquad .... eq(1) $     (위에 말한 것)\n",
    "        - $ \\pi_{i} \\geq 0 , \\sum_{i\\in S} \\pi_{i} = 1 \\qquad .... eq(2)$ (확률 분포니까 당연)\n",
    "\n",
    "\n",
    "- Given $ T $에서 $ \\pi $ 계산방법:\n",
    "    - $ \\pi(I_{|S|,|S|} -T + 1_{|S|,|S|}) = 1_{1,|S|}  $\n",
    "        - 위 식이 행렬의 형태여서 이를 조금 풀고 파이를 풀면.$ \\pi - \\pi T + \\pi = 1 $\n",
    "        - 그리고 이를 나누어서 보면 위의  Limit Theorem of Markov Chain의 식들에서 온것을 알 수있다.\n",
    "        - $ \\pi - \\pi T $ 이 부분은 eq(1)에서\n",
    "        - $ \\pi = 1 $ 의 부분은 eq(2)의 두 번째 것에서\n",
    "    - 그래서, $ \\pi = 1_{1,|S|}(I_{|S|,|S|} -T + 1_{|S|,|S|})^{-1} $ 역행렬로서 구할 수 있다.\n",
    "\n",
    "\n",
    "- 2개의 Transition Matrix에 대해 pi를 구해보는 실험.\n",
    "\n",
    "- <img  src=\"./image/W10-6.PNG\" width=\"80%\">\n",
    "    - pi * T = T를 보임으로써 stationary distribution을 잘 구했음을 확인 할 수 있다.\n",
    "    - 그리고 위에서 구한 2가지 pi들에 대해 Reversible한지 보자.\n",
    "    - Reversible markov chain은 다음과 같은 Balance Equation을 만족해야 한다.\n",
    "        - $ \\pi_{i} T_{i,j} = \\pi_{i} T_{j,i} $\n",
    "        - 즉 여기서 state i에서 j로갈 확률과 j에서 i로 갈 확률이 동일한지 보면\n",
    "- <img  src=\"./image/W10-8.PNG\" width=\"80%\">\n",
    "    - state 1에서 2로 그다음 state 2에서 1로 가는 확률 계산\n",
    "    - 결과는 오른쪽 것만 Reversible한 것을 알 수 있고, Sationary이면 꼭 reversible은 아니지만,\n",
    "        - Reversible이면 Stationary인 것은 증명이 되어있다. (Stationary가 강한 conditon)\n",
    "\n",
    "- 그리고 이 Balance Equation (or Detailed Balace)로 Stationary Distribution을 구하는 것을 계속 해볼 것이다.\n",
    "\n",
    "\n",
    "### Markov Chain for Sampling\n",
    "\n",
    "- Forward, Rejection, Importance Sampling의 문제점은 sample들을 i.i.d하게 뽑아 이전 sample을 사용하지 않은 것에 있다. (Importance도 결국은 Sampling Weight을 주는데만 사용했지만 i.i.d는 마찬가지)\n",
    "\n",
    "\n",
    "- $ P(E|MC=T, A=F) $ 나 E가 latent일 경우와 같은 $ P(Z|MC=T, A=F) $ 와 같은 것을 Markov Chain을 이용한 sampling으로 어떻게 inference 하는 것인가. \n",
    "    - 그리고 어떻게 E나 Z에 Assign할 것인가.\n",
    "\n",
    "- Sequence of Random Variable에 대한 Assign을 Process 형태로.\n",
    "    - <img  src=\"./image/W10-9.PNG\" width=\"80%\">\n",
    "    - z1에서 Assign된 정보를 활용해 다음 z+1에 Assign을 한다는 것이다. 이렇게 쭈루룩\n",
    "    \n",
    "\n",
    "- 기존의 Operation Research분야에서는 Markov Chain 또는 Markov Chain Analysis는 \n",
    "    - Stationary Distritbution, Return time, 다음 상품은 언제오는지, 큐는 각각 어느 size일까?를 찾는데 관심이 있었다.\n",
    "    - given Transition Matrix(rule), $ p(z^{t+1}| z^{t}) $ interested in finding $ \\pi(z) $ Stationary Distritbution.\n",
    "  \n",
    "  \n",
    "- ** Markov Chain Monte Carlo **\n",
    "    - 우리는 어떻게 보면 반대로 target Stationary Distritbution를 알고 있다하고,\n",
    "    - 그리고 이 Stationary Distritbution을 만드는 Transition Matrix(rule)는 무엇인지와 \n",
    "    - 그리고 어떻게 하면 만든 Transition Matrix(rule)을 잘 설명할 수 있는지에 관심이 있다.\n",
    "        - Transition Matrix(rule)을 잘 만드는 방법은 우리가 원하는 Stationary Distritbution을 표현할 수 있도록 Transition Matrix(rule)를 잘 만들어서\n",
    "        - 그리고 이를 활용해 sampling을 매 term마다 해보자.\n",
    "    - 이렇게 처음에는 맘대로 정한 assignment가 되겠지만 이걸 지속적으로 현재 sampling된 assignment를 참조해 다음 sampling을 해보면\n",
    "    - 나중에는 결국에는 우리가 원했던 Stationary Distritbution과 유사해 질 것 같다는 것이  Markov Chain Monte Carlo 의 핵심 아이디어이다.\n",
    "    - ex)<img  src=\"./image/W10-10.PNG\" width=\"100%\">\n",
    "    - 빨강은 False, 파랑은 True\n",
    "    - 노란색은 Evidence 노드들이고 초록색은 Latent Variable이라고 하면,\n",
    "    - 계속 이전의 sample들을 감안(확률적으로)하여 sample할 것이다.\n",
    "\n",
    "\n",
    "### Metropolis-Hastings Algorithm\n",
    "\n",
    "- MCMC 에서 주어진 $ \\pi $ (Stationary Distribution)에서 Transition Matrix(rule)을 만들어 내는 알고리즘 (General Alogorithm of MCMC)\n",
    "    - 현재 time의 assignment를 $ z^{t} $라 하면,\n",
    "    - 그리고 다음 time에 assign될 candidate $z^{*}$를 porposal distribution $ q_{t} $으로 제안한다.\n",
    "        - $ z^{*} \\sim q(z^{*}| z^{t}) $\n",
    "        - 여기 까지는 importance나 rejection sampling과 유사하다.\n",
    "        - 그러나 given $ z^{t} $인 markov property를 활용했다는 것이 다르다.\n",
    "    - 그리고 Acceptance probability $ \\alpha $ 에 따라 (coin toss를 해보아서) \n",
    "        - Accept면 $ \\to z^{t+1} = z^{*} $ \n",
    "        - Reject면 $ \\to z^{t+1} = z^{t} $ \n",
    "        \n",
    "\n",
    "- 수식으로는 다음과 같다.\n",
    "    - Consider a ratio, $ r(z^{*}| z^{t}) = {{q( z^{t}|z^{*})P(z^{*})}\\over{z^{*}|z^{t}P(z^{t}) }} $\n",
    "        - q는 우리가 정해주는 것(rejection에서 normal을 정해주었듯), P()는 evaluation\n",
    "        - 이를 ration로 정하며, 1이 되고자 한다.\n",
    "        - 왜냐하면 P(z)가 Stationary distribution $ \\pi(z) $가 되길 원하기 때문이다\n",
    "        - 이 때, 위에서 우리가 배운 Stationary distribution보다 제약이 센 Reversible MC의 조건 (detailed balance: $ \\pi_{i} T_{i,j} = \\pi_{i} T_{j,i}  $)을 사용해 Accept probability를 정한다.\n",
    "        - detailed balance를 만족하려고 Acceptance probability $ \\alpha $를 만들었다.\n",
    "        - 또한 q는 우리가 정했기 때문에 Reversible MC이 되기위해 만들어 지지 않았다.\n",
    "    - 만약 $ r(z^{*}| z^{t}) <1 $ 이라면\n",
    "        - $ {{q( z^{t}|z^{*})P(z^{*})}\\over{z^{*}|z^{t}P(z^{t}) }} $을 보면\n",
    "        - $ z^{t}$에서 $ z^{*} $로 갈 확률이 반대로 갈 확률보다 더 큰 것이다.\n",
    "        - 그러면 detailed balance 조건 만족하지 않음\n",
    "    -  $ r(z^{*}| z^{t}) = {{q( z^{t}|z^{*})P(z^{*})}\\over{z^{*}|z^{t}P(z^{t}) }} $ 이 식에서 분모를 양변에 곱하고, Ratio의 값 또한 확률이므로 1이 제일 큰값이므로 다음과 같이 설정 해주면 되며 Acceptance prob은 다음과 같이 설정하면 된다.\n",
    "    - <img  src=\"./image/W10-11.PNG\" width=\"70%\">\n",
    "    \n",
    "    \n",
    "    \n",
    "### Random Walk M-H Algorithm\n",
    "\n",
    "- MCMC는 결국에 Transition matrix를 만들기 위한 것이니, Transition Matrix의 관점에서 보면\n",
    "    - 어떤 특정 state t에서 다음 번 candidate state $ * $로 갈 Transition Matrix\n",
    "    - $ T_{t,*}^{MH} = q(z^{*}|z^{t}) \\alpha(z^{*}|z^{t}) $\n",
    "        - 이렇게 하면 결국 balance equation을 만족할 것이다.\n",
    "- $ \\alpha(z^{*}|z^{t}) = min \\left\\{ 1,  {{q( z^{t}|z^{*})P(z^{*})}\\over{z^{*}|z^{t}P(z^{t}) }} \\right\\} $\n",
    "\n",
    "\n",
    "- Random Walk M-H Algorithm. (특정 instance에 실제로 해보자)\n",
    "    - $ T_{t,*}^{MH} = q(z^{*}|z^{t}) \\alpha(z^{*}|z^{t}) $\n",
    "    - $ z^{*} \\sim N(z^{t}, \\sigma^{2}) $ (이부분(Normal에서 뽑아져 나와서) 때문에 Random walk가 나오는 것이다.)\n",
    "        - 이를 풀어서 쓰면 아래처럼\n",
    "    - $ q(z^{*}|z^{t}) = {{1}\\over{\\sigma \\sqrt{2\\pi}}}exp({{(z^{*}-z^{t})^{2}}\\over{2\\sigma^{2}}}) $\n",
    "    - 다음의 그림 위의 평균이 $ z^{t} $이고 $ \\sigma^{2} $의 분산에서 $ z^{*} $를 뽑고\n",
    "    - <img  src=\"./image/W10-12.PNG\" width=\"30%\">\n",
    "    - 아래처럼 accept되면 $ z^{t} $로\n",
    "    \n",
    "- Random Walk M-H Algorithm을 통해 sampling하여 Mixture distribution을 알아본 결과\n",
    "    - random walk할 때 sigma의 크기에 따라 어떻게 Mixture distribution을 샘플링하는지\n",
    "    - Latent Mode Selection Sampling은 BN의 Z부분이, Observed Variable Sampling은 X부분이 어떻게 select됬는지 보여주는 그래프 (내려가면서 시간이 흘러가는 것)\n",
    "- <img  src=\"./image/W10-13.PNG\" width=\"90%\">\n",
    "     - 분산이 1일때는 3번째께 undersampling, 40일 땐 정밀함이 부족\n",
    "     - 그래서 분산을 처음에는 넓게 샘플링 하고, 나중에는 분산을 줄여나가면서 정밀하게 할 수 있다.\n",
    "     \n",
    "     \n",
    "\n",
    "### Gibbs Sampling\n",
    "\n",
    "- 지금까지  M-H Algorithm을 잘이해했다면, 깁스샘플링이의 special한 case로 잘 이해할 수 있다.\n",
    "    - M-H Algorithm에서 porposal distribution $ q_{t} $를 왜 새로운 것을 잡는지, 이미 잘 정의된 다른 것이 있는데? 그리고 그것을 재사용하는게 어떠니 에서 시작한다.\n",
    "\n",
    "- $ z^{t} = (z_{k}^{t},z_{-k}^{t}) \\to z^{*} = (z_{k}^{*},z_{-k}^{t}) $\n",
    "    - 이번에는 다 update 치는 것이 아니라 k라는 한 latent에서만 update치자\n",
    "    - $ T_{t,*}^{MH} = q(z^{*}|z^{t}) \\alpha(z^{*}|z^{t}) $\n",
    "        - M-H에서 q부분을 기존에 모델이 제안하는 probablistic distribution을 그대로 활용하되, 여러 latent중 k만 update하자\n",
    "    - $ q(z^{*}|z^{t}) = P(z_{k}^{*},z_{-k}^{t}|z_{-k}^{t}) = P(z_{k}^{*},z_{-k}^{t}) $\n",
    "\n",
    "    - 이때 Balance equation은 다음과 같이 된다.:\n",
    "        - $ P(z^{t}) q(z^{*}|z^{t}) = P(z^{*})  q(z^{t}|z^{*}) $ 이것을 만족하기위해 왼쪽부터 시작하면\n",
    "        - $ P(z^{t}) q(z^{*}|z^{t}) =  P(z_{k}^{t},z_{-k}^{t})P(z_{k}^{*},z_{-k}^{t})$\n",
    "            - 위위에있는 공식 적용\n",
    "        - $ = P(z_{k}^{t}|z_{-k}^{t})P(z_{-k}^{t})P(z_{k}^{*},z_{-k}^{t})  $\n",
    "            - 앞의 joint를 factorize\n",
    "        - $ = P(z_{k}^{t}|z_{-k}^{t}) P(z_{k}^{*},z_{-k}^{t}) =  P(z^{*})  q(z^{t}|z^{*}) $\n",
    "            - 뒤를 joint로 하면 만족되며\n",
    "    - 즉, $ q(z^{*}|z^{t})$ 를 잘알려진 $ P(z_{k}^{*},z_{-k}^{t}) $로 정의 하니까\n",
    "    - ** Balance equation이 자동으로 만족되어, Acceptance probability가 없어져 버린 다. **\n",
    "\n",
    "    - Gibbs sample의 example\n",
    "    - ex)<img  src=\"./image/W10-14.PNG\" width=\"60%\">\n",
    "    - 위와 같은 Bayesina Net에서 $ P(E,JC,B|A=F,MC=T) $를 알려고 하면\n",
    "        - 여기서 A, MC는 Evidence이고 나머지 latent에 대해 coin toss를 하여 Assign하는 것이다.\n",
    "    - 예를들어 B에 확률 Assign한다하면($ P(B|E,A,JC,MC) $), Markov Blanket에 의해 $ P(B|E,A,JC,MC) = P(B|E,A) $ (A는 coin toss에 의해 나온 값으로 evidence로 받아들이고) 이때의 B의 확률은 어떻게 되는지 update.\n",
    "        - 그리고 만약 다음타임에 B가 바뀌었다, 했으면  M-H Algorithm에서는 Acceptance probability를 계산했을 것이지만,\n",
    "        - 이제는 그대로 받아들이겠다하는 것이 특징이다.\n",
    "    \n",
    "    \n",
    "- Concept of Gibbs Sampling\n",
    "    - Each step involves replace the value of one of the variables by a value drawn form the distriution of that variable conditioned on the values of the remaining variables.\n",
    "    - <img  src=\"./image/W10-15.PNG\" width=\"80%\">\n",
    "    - 이는 깁스샘플러가 likelihood가 개별 variable-wise로 높은 곳으로 움직이는 것을 의미한다.\n",
    "    \n",
    "\n",
    "- 알고리즘 형태로 보면 다음과 같다.\n",
    "    - initialize할 때는 적당한 information으로 한다. \n",
    "    - <img  src=\"./image/W10-16.PNG\" width=\"60%\">\n",
    "    \n",
    "    \n",
    "- Gibbs Sampling based GMM\n",
    "    - GMM에서는 잘 안되서 안하지만, 그래도 깁스샘플링을 적용할 수 있다.\n",
    "    - optimize해서 assign하는 것이 아니라 샘플링을 개별 point마다 $P(Z|-)$를 assign 하여 보이는 것은 hard assignment처럼 보인다.\n",
    "    - <img  src=\"./image/W10-17.PNG\" width=\"70%\">\n",
    "    - EM과 비교\n",
    "        - 그리고 sample을 통해서 optimize하기 때문에 converge 속도가 EM에 비해 느리긴하다.\n",
    "        - EM처럼 수식을 풀어서 optimize하는 것이 아니라 sample을 통해서 \n",
    "        - 그러나 이 그래프에서는 안보이지만 MCMC기반이나 sampling기반이 saddle point에 대해 덜 취약하다. BUT 긴시간이 필요.\n",
    "    - <img  src=\"./image/W10-18.PNG\" width=\"70%\">\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- ** EM을 더 발전하면 Variational Inference(기계학습 심화에서 나옴)라는 것이 PGM(probabilistic graphical models)의 inference의 한축으로 쓰이고, **\n",
    "- ** Gibbs Sampler 계열의 sampling based inference가 또 다른 축으로 큰 기둥 2개가 있다고 생각할 수 있다. **\n",
    "\n",
    "- 이제 배울 LDA를 Gibbs Sampler 기반으로 한번 배울 것인데 Conjugate prior로 collapse가 잘되고 좋다, LDA는 사실 variational inference로 먼저 original paper에 쓰였는데 최근에는 Gibbs Sampler의 기반으로 많이 사용하고 있다.\n",
    "    - LDA는 text에 대한 soft cluster를 하는 방법을 배워보자\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Reference:\n",
    "\n",
    "- 문일철 교수님, 인공지능 및 기계학습 개론 II, https://www.edwith.org/machinelearning2__17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
