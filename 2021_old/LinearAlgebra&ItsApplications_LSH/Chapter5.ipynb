{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 5 Eigenvalues and Eigenvectors\n",
    "\n",
    "벡터 공간을 간단하게 표현하는 데 초점. <br>\n",
    "$ Ax = \\lambda x  $<br> Ax의 결과를 람다라는 스칼라(Eigenvalue)와 벡터(Eigenvector)로 표현하자! ( A는 n x n ) <br>\n",
    "$ \\to (A-\\lambda I )x = 0 \\ \\ $ for non-zero x, $ det (A-\\lambda I ) = 0 $ 을 만족해야 (singular 여야) <br>\n",
    "이 때 람다에 대해 n차 식이 나오고 이 람다의 근을 구하는 것이, Eigenvalue를 찾는 것이다. 그리고 Eigenvector는 Nullspace of $  (A-\\lambda I) $이다. (Redueced row echelon form!) 행렬의 크기에 상관없이 eigenvalue 하나에 대해, nullspace를 이루는 free variable은 1개씩 밖에 나오지 않아 다행이다. 또한 eigenvalue에 따라 나오는 eigenvector들 끼리 independent해서 벡터공간을 해석하게 할 수 있는 또 하나의 방법을 제시한다.\n",
    "\n",
    "- some properties<br>\n",
    "1) For Upper Triangular Matrix, Eigen values are diagonal elements of the matrix. (pivots).<br> \n",
    "$ det A = \\prod_{i=1}^{n} pivot_{i} =  \\prod_{i=1}^{n} d_{i} =  \\prod_{i=1}^{n} \\lambda_{i}  $ <br>\n",
    "2) Trace of matrix A $ = \\sum_{i=1}^{n} a_{ii} = \\sum_{i=1}^{n} \\lambda_{i}  $\n",
    "\n",
    "#### 5.2 Diagonalization of a Matrix\n",
    "\n",
    "$ A = S \\Lambda S^{-1} $ <br>\n",
    "S는 각 Eigenvalue에 대한 Eigenvector를 열로 갖는 행렬, 람다 행렬은 Eigenvalue들을 diagonal elements로 갖는 행렬 <br>\n",
    "$ S = \\begin{bmatrix} e_{1} & \\cdots & e_{n} \\end{bmatrix} \\quad \\Lambda = \\begin{bmatrix}\n",
    "\\lambda_{1} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_{2} & 0 & \\vdots \\\\\n",
    "\\vdots  & 0 & \\ddots & \\vdots \\\\\n",
    "0  & \\cdots & \\cdots & \\lambda_{n}\n",
    "\\end{bmatrix} $ <br>\n",
    "pf ) $ AS = S \\Lambda \\quad Ae_{i} = \\lambda_{i}e_{i} $ <br>\n",
    "det(S) = 0이면(inverse 가 없으면), S는 존재하지 않음(분할이 안됨).\n",
    "<img  src=\"./image/image_5.1.PNG\" width=\"50%\">\n",
    "<img  src=\"./image/image_5.2.PNG\" width=\"50%\">\n",
    "\n",
    "Diagonalization은 Difference, Differential Equation 을 풀 때 유용하다. <br>\n",
    "Eigen Value는 complex number 일 수도 있으며, A가 Symmetric(대칭행렬)이면 eigenvector들이 orthogonal하다. <br>\n",
    "Diagonalization이 data compression, dimension reduction등 의 핵심적인 개념이 된다.\n",
    "\n",
    "Remark 1. If eigen values ($ \\lambda_{1}, ... ,\\lambda_{n})  $ are distinct, then eigenvectors (e1, ..., en) are linearly independent. <br>\n",
    "\n",
    "Remark 2. Matrix S is not unique since $ke$ can still be eigen vector.\n",
    "\n",
    "Remark 3. The order of eigenvalues and eigenvectors in S and $\\Lambda$ must be same.\n",
    "\n",
    "Remark 4. Not all matrices have n linearly independent eigenvectors, so not all matrices are diagonaligable. <br>\n",
    "cf) determinant가 0이 되더라도 independent한 eigenvalue, eigenvectors를 구할 수 있다. eigenvalue가 중근이 아니여야한다. eigenvalue가 n개 나와야 Diagonalization이 가능하다.\n",
    "\n",
    "Remark 5. power of A<br>\n",
    "$ A $의 eigenvalue: $\\lambda_{i}$ eigenvector: $ S $ 일 때, $ A^{k} $의 eigenvalue: $\\lambda_{i}^{k}$ eigenvector: $ S $ <br>\n",
    "이 점이 Difference, Differential Equation 을 풀 때 유용하다. \n",
    "\n",
    "#### 5.3 Difference Equation and Powers $A^{k} $\n",
    "\n",
    "Difference: discrete time &nbsp;&nbsp;  Differential: Continious\n",
    "\n",
    "Difference Equation: 수열 <br>\n",
    "$ pa_{n+2} + qa_{n+1} + ra_{n} = 0 $ <br>\n",
    "p + q + r = 0이면, $ p(a_{n+2}-a_{n+1}) = r(a_{n+1}-a_{n}) $ 로 등비수열의 꼴로 풀 수 있다. <br>\n",
    "$ p + q + r \\neq 0 $ 이면n $ a_{n} = c_{1}(\\alpha_{1})^{n} + c_{2}(\\alpha_{2})^{n} $의 꼴로 나오며, 알파값들이 함수 공간에서의 basis function이다.\n",
    "\n",
    "ex) Fibonazzi Sequence<br>\n",
    "$ F_{k+2} = F_{k+1} + F_{k} $ 인 수열에서, $  F_{1000} $의 값은 ? <br>\n",
    "$ u_{k} = \\begin{bmatrix} F_{k+1} \\\\ F_{k} \\end{bmatrix} $, $\\quad u_{k+1} = A u_{k} $ <br> $ F_{k+2} = F_{k+1} + F_{k} $, $ F_{k+1} = F_{k+1} $을 활용.\n",
    "$ A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} $ <br>\n",
    "$ u_{k+1} = A u_{k} $ then $ u_{k} = A^{k}u_{0} $ <br>\n",
    "$ A^{k} $ 를 잘 표현하자.<br>\n",
    "$ \\to A^{k} = S \\Lambda^{k} S^{-1} = \\begin{bmatrix} e_{1} & e_{2}  \\end{bmatrix}  \\begin{bmatrix} \\lambda_{1}^{k} & 0 \\\\ 0 & \\lambda_{2}^{k}  \\end{bmatrix} S^{-1} $\n",
    "<br> 이 때, $ u_{k} = \\begin{bmatrix} e_{1} & e_{2}  \\end{bmatrix}  \\begin{bmatrix} \\lambda_{1}^{k} & 0 \\\\ 0 & \\lambda_{2}^{k}  \\end{bmatrix} S^{-1}u_{0} $ 이다. <br>\n",
    "$ S^{-1}u_{0} =  \\begin{bmatrix} c_{1} \\\\ c_{2}  \\end{bmatrix}  $로 놓으면, <br>\n",
    "$ u_{k} = \\begin{bmatrix} e_{1} & e_{2}  \\end{bmatrix}  \\begin{bmatrix} c_{1}\\lambda_{1}^{k} & 0 \\\\ 0 & c_{2}\\lambda_{2}^{k}  \\end{bmatrix} = c_{1}\\lambda_{1}^{k} e_{1} + c_{2}\\lambda_{2}^{k} e_{2}  $ 이다. <br>\n",
    "이제 eigenvalues를 구하면, <img  src=\"./image/image_5.3.PNG\" width=\"50%\">\n",
    "이제 eigenvector를 구하면, $ \\begin{bmatrix} 1-\\lambda & 1 \\\\ 1 & -\\lambda  \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2}  \\end{bmatrix} =   \\begin{bmatrix} 0 \\\\ 0  \\end{bmatrix}    $ <br> 여기서 x1, x2가 서로 적절한 관계만 가지면 되니까, 아래행을 이용해 $ \\to x_{1} - \\lambda x_{2} = 0 $ , $ \\to x_{1} = \\lambda x_{2}$, x2가 1일 때 x1은 람다로 <br>\n",
    "$ S = \\begin{bmatrix} \\lambda_{1} & \\lambda_{2} \\\\ 1 & 1  \\end{bmatrix} $ 이 나옴. 이제 $ c_{1}\\lambda_{1}^{k} e_{1} + c_{2}\\lambda_{2}^{k} e_{2} $ 을 적용.\n",
    "<img  src=\"./image/image_5.4.PNG\" width=\"50%\">\n",
    "<img  src=\"./image/image_5.5.PNG\" width=\"50%\">\n",
    " $  F_{1000} $의 integer 값은 음수부분(-뒷항)은 너무작아져 왼쪽 양수부분에 종속될 것이다.\n",
    " \n",
    " $ F_{k+2} - F_{k+1} - F_{k}= 0 \\to \\lambda^{2}-\\lambda -1 = 0 $ 으로 놓는 것을 Characteristic Equation이라 한다.\n",
    " \n",
    "- If there are som multiple roots in the Characteristic equation. <br>\n",
    "$ \\to c_{1}\\lambda^{n}+ c_{2}n\\lambda^{n} $ (doublee root) <br>\n",
    "$ \\to c_{1}\\lambda^{n}+ c_{2}n\\lambda^{n} + c_{3}n^{2}\\lambda^{n}  $ (triple root)\n",
    "\n",
    "- For non-homogeneous Difference Equation <br>\n",
    "$ pa_{n+2} + qa_{n+1} + ra_{n} \\neq 0 = b_{n} $ <br>\n",
    "$ b_{n} $에 달려있다. ex) $b_{n} = 2({{1}\\over{2}})^{n}$ 이면, 1/2가 basis가 되어, <br>\n",
    "particualrt solution: $ \\to c ({{1}\\over{2}})^{n} $ 이 되며 c를 식에 대입하면 <br>\n",
    "$c( p({{1}\\over{2}})^{n+2} + q({{1}\\over{2}})^{n+1} + r({{1}\\over{2}})^{n} ) =   2({{1}\\over{2}})^{n} $ <br>\n",
    "$ c (({{p}\\over{4}}) + ({{q}\\over{2}}) + r) =  2 $ , p,q,r은 주어졌으니 풀 수있다. <br> 이것도, 평행이동과 같이 구하는 것이다.\n",
    "\n",
    "\n",
    "#### Least Square solution of  overconstrained homogeneous equation\n",
    " \n",
    "Chapter 3에서 배운 Ax = b인 overconstrained problem에서 b = 0이면 $ \\hat{X} = (A^{T}A)^{-1}A^{T}b $를 적용할 수 없다. (non-homogeneous equation 일 때만 가능) <br>\n",
    "Ax = 0 인 overconstrained homogeneous equation에서는 여전히 목적은 $ min \\ || Ax ||^{2} $ 이지만 <br> 나올수 있는 solution X가 너무 많으니까 X에 constraint를 준다. 예를 들어 1, $ || X ||^{2} =1 $ 크기가 1이 되도록 제한.(1이 아니어도됨) <br>\n",
    "$ || Ax ||^{2} = (Ax)^{T}(Ax) = x^{T}A^{T}Ax $ 이 때,  $ A^{T}A $ 은 nxn 작은 정사각 행렬이되니까, $ A^{T}Ax = \\lambda x $로 놓고 , $ x^{T}A^{T}Ax = x^{T} \\lambda x $ 이며 람다는 상수값이니까, $ \\lambda || x ||^{2} $이고 , 새로운 제약으로 $ || X ||^{2} =1 $로 $ min \\ || Ax ||^{2}= min \\ \\lambda $ 문제가 된다. <br>\n",
    "즉, $  (A^{T}A) $의 minimum eigenvalue에 대응하는 eigenvector  <br> cf) $  (A^{T}A) $에 대해서는 항상 양수의 eigenvalue를 갖는 구나. positive eigenvalue를 통해 SVD를 한다. <br>\n",
    "또한 이러한 constraint는 합쳐서 ($ min \\ || Ax ||^{2} $, $ || X ||^{2} =1 $ ) $ f(x,\\lambda) = || Ax ||^{2} + \\lambda(1-|| X ||^{2}) $ (이 때 람다는 Lagrange Multiplier) 표현하고,<br>\n",
    "$ {{\\partial f}\\over{\\partial x}} \\to 0 \\quad {{\\partial f}\\over{\\partial \\lambda}} \\to 0 $ 이렇게 편미분해 극점을 활용해도 $ A^{T}Ax = \\lambda x $가 유도된다. \n",
    "\n",
    "#### Markov Matrix\n",
    "<img  src=\"./image/image_5.7.PNG\" width=\"70%\">\n",
    "<img  src=\"./image/image_5.6.PNG\" width=\"30%\">\n",
    "Markov Matrix $ \\to $ state transition in probability <br>\n",
    "Markov Matrix의 column vector의 합은 1 각각의 원소들은 $ 0 \\le a_{ij} <1 $ <br>\n",
    "또한 eigenvalues 중 하나는 꼭 1이나오고 나머지는 절대값이 1보다 작다. (이 것 때문에 stable 에 1보다 작은 것은 영향을 안줌) <br>\n",
    "\n",
    "$ u_{k} = \\begin{bmatrix} y_{k} \\\\ z_{k}  \\end{bmatrix}  $인데, $\\lim_{k \\to \\infty}u_{k}$는 수렴한다. (Stable State, Steady State)\n",
    "<img  src=\"./image/image_5.8.PNG\" width=\"60%\">\n",
    "<img  src=\"./image/image_5.9.PNG\" width=\"60%\">\n",
    "\n",
    "#### 5.4 Differential Equation and $ e^{At} $\n",
    "\n",
    "미분방정식, $ {{dy(t)}\\over{dt}} = a y(t) $를 만족하는 함수 <br>\n",
    "$ y(t) = e^{at}  $ 가 미분방정식의 시작 <br>\n",
    "$ a_{n+1} - r a_{n} \\quad \\to \\quad  y'(t) - a y(t) $ <br>\n",
    "$ y'(t) - a y(t) = 0 $을 homogeneous ordinary  1st-order  differential equation 이라한다. \n",
    "\n",
    "$ y'(t) - a y(t) = 0 $ 일 때 solution은  $ y' = ay \\to dy/dt = ay \\to (1/y)dy = adt  \\to \\int (1/y)dy = \\int  adt  \\to ln y = at + \\tilde{c} $  <br>\n",
    "$ \\to  y(t) = c e^{at}   $ 이므로, $ y'(t) - a y(t) = 0 $의 해를  $ y(t) = e^{\\lambda t} $로 놓고 대입하면,  $ \\lambda e^{\\lambda t} + a e^{\\lambda t} = 0 $이며 $ \\lambda = -a  $ . <br> $ y(t) = e   e^{-at} $ 이고 임의의 상수값을 곱해도 조건을 만족해 c를 놓는다. $  e^{-at} $를 basis function 으로 볼 수 있다. <br>\n",
    "\n",
    "$y'' + a y' + by = 0 $ ( homogeneous ordinary  2nd-order differential equation) 의 해도 difference equation 과 같이 $ c_{1}e^{\\lambda_{1} t}+c_{2}e^{\\lambda_{2} t} $ 나올 것이다.\n",
    "$y'' + a y' + by = 0 $ 또한 charactistic equation으로 $ \\lambda^{2}+ a\\lambda + b = 0 $ 나타 낼 수 있다. 그리고 이것이 중근을 가지지 않으면 $ c_{1}e^{\\lambda_{1}t} + c_{2}e^{\\lambda_{2}t} $, 중근일 때는 $ c_{1}e^{\\lambda_{1}t} + c_{1}te^{\\lambda_{1}t} $가 나온다. <br>\n",
    "difference equation 에서 $ u_{k+1} = A u{k} $를 적용했다면, Differential에서는 $ u' = Au $ 를 적용. <br>\n",
    "$ u_{k} = A^{k}u_{0} = S \\Lambda S^{-1}u_{0} $에서 $ S = \\begin{bmatrix}\n",
    "\\lambda_{1} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_{2} & 0 & \\vdots \\\\\n",
    "\\vdots  & 0 & \\ddots & \\vdots \\\\\n",
    "0  & \\cdots & \\cdots & \\lambda_{n} \\end{bmatrix} $  <br>\n",
    "여기서는 $ u_{t} = S e^{\\Lambda t} S^{-1}u_{0} $ 이고, $ e^{\\Lambda t} = \\begin{bmatrix}\n",
    "e^{\\lambda_{1} t} & 0 & \\cdots & 0 \\\\\n",
    "0 & e^{\\lambda_{2} t} & 0 & \\vdots \\\\\n",
    "\\vdots  & 0 & \\ddots & \\vdots \\\\\n",
    "0  & \\cdots & \\cdots & e^{\\lambda_{n} t} \\end{bmatrix} $\n",
    "\n",
    "$ u' = \\begin{bmatrix} x'(t) \\\\ y'(t) \\end{bmatrix} =  \\begin{bmatrix} ax(t) + by(t) \\\\ cx(t) + dy(t) \\end{bmatrix} =  \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} u $<br>\n",
    "아래행을 잘 보면 이런식이 나옴. $ x(t) = (1/c)y'(t) - (d/c)y(t) \\to $  한번미분해서 $ x'(t) $에 넣음 <br>\n",
    "$  (1/c)y''(t) - (d/c)y'(t) = a((1/c)y'(t) - (d/c)y(t)) + by(t)  $이 식이 나오고 이를 정리하면 <br>\n",
    "$ y''(t) - (a+d)y'(t) + (ad-bc)y(t) = 0 $ y에 대한 2차 미분방정식이나온다.\n",
    "\n",
    "$ | A - \\lambda I | = \\begin{vmatrix} a-\\lambda & b \\\\ c & d-\\lambda \\end{vmatrix} = 0 \\to  \\lambda^{2} - (a+d)\\lambda + (ad-bc) = 0 $ <br>\n",
    "Characteristic Equation로 푸는 거나, 행렬 A에 대한 eigenvalue로 푸는거나 똑같다. <br> Taylor series 와 비슷하게 전개되므로 인해 행렬이 지수로 들어가면 이렇게 되는 것이다.\n",
    "<img  src=\"./image/image_5.10.PNG\" width=\"60%\">\n",
    "<img  src=\"./image/image_5.11.PNG\" width=\"60%\">\n",
    "\n",
    "cf) Matrix A,B에 대해서 $ e^{tA} \\cdot  e^{tB} = e^{t(A+B)}$, $  \\qquad e^{tA} \\cdot  e^{-tA} = I$\n",
    "\n",
    "Eigenvalue가 다 다르지 않다면? = $ S^{-1} $이 없으면? <br>\n",
    "이와 비슷하게 어떤 행렬이든 Decomposition 할 수 있는 General한 방법이 있다.$ \\to $ SVD\n",
    "\n",
    "\n",
    "#### 5.5 Complex Matrix\n",
    "\n",
    "<img  src=\"./image/image_5.12.PNG\" width=\"60%\">\n",
    "z = a+jb <br>\n",
    "argument = $ \\theta = arg\\left\\{ z \\right\\} = tan^{-1}(b/a) $ <br>\n",
    "$ z = r(cos\\theta + j sin \\theta) = r e ^{i\\theta} $ (Euler formula을 통한 polar from)\n",
    "\n",
    "- complex vector <br>\n",
    "<img  src=\"./image/image_5.13.PNG\" width=\"60%\"> <br>\n",
    "복소수 행렬 A 의 성분의 Conjugate이고 이를 Transpose 한 것을 A 에르미트라고 한다.(A Hermitian , A is Hermitian은 다른 의미이다.)\n",
    "$$ A^{H} = \\bar{A}^{T} $$\n",
    "\n",
    "if $ A^{H} = A $, then \"A is Hermitian\". (A는 에르미트 행렬이다). symmetric $ \\subset $ Hermitian\n",
    "\n",
    "Property 1. If $ A^{H} = A$,then $ x^{H}Ax $ is real. x is complex vectors. <br>\n",
    "$ x^{H}Ax $를 quadratic form이며 Real number를 갖. 이를 이용해 타원과 같은 것을 모델링한다. <br>\n",
    "$ x^{H}Ax = (x^{H}Ax)^{H} $이므로.\n",
    "\n",
    "cf) $ R = A^{H}A $의 quadritic form $ x^{H}A^{H}Ax = (Ax)^{H}(Ax) = || Ax ||^{2} > 0 $ zero vector가 아니면 항상 positive 구나. R 을 Correlation matrix 라 한다. Correlation matrix로 Eigenvalue를 나중에 구한다. SVD의 원리도 된다.\n",
    "\n",
    "Property 2. If $ A^{H} = A$,then every eigenvalue is real. <br>\n",
    "$\\to  Ax = \\lambda  x$ for complex vector x, quadritic form으로 만든다. <br>\n",
    "$ x^{H}Ax = \\lambda x^{H} x =   \\lambda ||x||^{2} $ $\\qquad \\to \\lambda = {{ x^{H}Ax }\\over{||x||^{2}}}   $ <br>\n",
    "이 때 A가 correlation matrix라면 0보다 큰 eigenvalue를 가지며, 이 positive한 eigenvalue의 root값이 SVD의 singlur value를 구성한다.<br>\n",
    "G.E.를 중요하다고 배웠지만 실제로 많이 쓰진 않는다. Least Square랑 SVD를 많이 사용한다.\n",
    "\n",
    "Property 3. two eigenvectors of Hermitian matrix, if they come from different eigen values, are orthogoanal to one another. (Symmetric도 수직이 됨)\n",
    "\n",
    "In cas A is real and symmetrix, $ A = S\\Lambda S^{-1} $에서 <br>\n",
    "$  A = Q\\Lambda Q^{-1} =  Q\\Lambda Q^{T}   $라고 할 수 있다.\n",
    "\n",
    "$ Q\\Lambda Q^{T} = \\begin{bmatrix} x_{1} & \\cdots & x_{n} \\end{bmatrix}  \\begin{bmatrix}\n",
    "\\lambda_{1} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_{2} & 0 & \\vdots \\\\\n",
    "\\vdots  & 0 & \\ddots & \\vdots \\\\\n",
    "0  & \\cdots & \\cdots & \\lambda_{n}\n",
    "\\end{bmatrix}  \\begin{bmatrix} x_{1}^{T} \\\\ \\vdots \\\\ x_{n}^{T} \\end{bmatrix}  = \\lambda_{1} x_{1}x_{1}^{T} + \\cdots + \\lambda_{n} x_{n}x_{n}^{T} = A $ <br>\n",
    "$ \\lambda_{1} x_{1}x_{1}^{T} + \\cdots + \\lambda_{n} x_{n}x_{n}^{T}  $의 의미는 $ x_{i}$ 크기가 모두 1이니까 projection ($ {{x_{1}x_{1}^{T}}\\over{x_{1}^{T}x_{1}}}$은 x1방향으로하는) 이며 이런 것을 Spectral Therom이라 한다. <br>\n",
    "예를 들어 a라는 벡터가 q1과 q2의 성분으로 주어진다 하면, $ a = (q_{1}^{T}a)q_{1} + (q_{2}^{T}a)q_{2} $로 나타냄을 보면, $ (q_{2}^{T}a) $의 역할을 하는 것이 위의 $ \\lambda_{i} $임을 알 수 있다. <br>\n",
    "만약 람다를 내림차순으로 정리해서 $ \\lambda_{1} > \\lambda_{2} > \\lambda_{3} .... $이라고 하면, A라는 행렬을 3개로 표현하겠다. 하면 $ \\lambda_{1} > \\lambda_{2} > \\lambda_{3} $와 그 eigenvector를 고르는 것을, Data Compression, Principle Component Analysis(PCA) 라고 한다.\n",
    "\n",
    "- Unitary matrix <br>\n",
    "for real orthogonal, $ Q^{T}Q = I $ Q와 비슷하게, <br>\n",
    "for complex orthogonal, $ U^{H}U = I $ <br>\n",
    "Unitary도 rotation transformation으로 볼수 있다. <br>\n",
    "property 1.  Unitary도 길이, 각도가 보존된다.<br>\n",
    "property 2.  Every eigenvalue of U has absolute value $ \\lambda = 1 $<br> complex plane에서 r=1인 원에 있을 것이다.+ U가 모두 real이면, complex conjugate로 갖겠다.) \n",
    "property 3. Each eigenvector from different eigenvalue is orthogonal. <br>\n",
    "(이것도 SVD에 이어진다.)<br>\n",
    "\n",
    "<img  src=\"./image/image_5.20.PNG\" width=\"70%\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
