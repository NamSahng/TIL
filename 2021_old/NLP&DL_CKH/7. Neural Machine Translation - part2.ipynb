{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Neural Machine Translation - [Bahdanau et al., 2015]\n",
    "\n",
    "- 조경현 교수님이 바다나우라는 친구랑 RNN을 사용\n",
    "\n",
    "<img  src=\"./image/img_6-5.PNG\" width=\"60%\">   \n",
    "\n",
    "#### 1) Source sentence representation\n",
    "- Number of Source Token == Num of token representation\n",
    "- Bidirectional RNN을 사용해 Context에 dependent하게\n",
    "\n",
    "<img  src=\"./image/img_7-1.PNG\" width=\"40%\">   \n",
    "\n",
    "\n",
    "#### 2) Target prefix representation\n",
    "- Causal Structure를 따라서 generate하기 위한 Unidirectional RNN\n",
    "    - 교수님은 당시 RNN을 좋아했음, 요즘 많은 사람들이 떠나가지만\n",
    "<img  src=\"./image/img_7-2.PNG\" width=\"35%\">  \n",
    "\n",
    "- $ z_{t} = RNN_{decoder}(z_{t-1},y_{t-1}) $\n",
    "- 그러면 마지막에 지금까지 translate한 문장이 무엇인가 summarize하는 벡터가 하나 남는다.\n",
    "    - 이 벡터를 사용할 것이다.\n",
    "    \n",
    "#### 3) Attention mechanism\n",
    "- source와 target을 어떻게 합쳐야하는지\n",
    "    - Attention mechanism(당시에는 soft alignment라고 부름)\n",
    "<img  src=\"./image/img_7-3.PNG\" width=\"45%\">  \n",
    "\n",
    "- zt가 위에서 말한 지금까지 무엇을 translate했는지를 summarize하니까, \n",
    "    - 지금까지 translate한 zt를 보고, 원래 번역을 하려고 했던 source의 문장을 봤을 때\n",
    "    - 어느 부분을 봐야 다음 token을 predict하는 데 필요한지(단어를 선택하는지) 결정을 하는 것이다.\n",
    "    - 그래서 RN과 weighting factor도 계산해 사용할 것이다.\n",
    "    - 그리고 이를 통해 골라진 애만 사용해 ct로 나타낸다.\n",
    "\n",
    "#### 4) Fuse the source context vector and target prefix vector\n",
    "\n",
    "<img  src=\"./image/img_7-4.PNG\" width=\"60%\">  \n",
    "\n",
    "- 그리고 이 ct와 zt 벡터가 복잡한 NN(An Arbitrary sub-graph)에 통과해서\n",
    "    - 궁극적으로 softmax에가게 되고 여기서 나온 결과를 distribution으로 바꾸어 줄 것이다.\n",
    "        - loss는 NLL로 이것의 gradient는 back propa를 써서, stochastic gradient descent와 adaptive learning rate을 사용해 파라미터 업데이트 하고, val set으로 Early stop을 할지 말지.... \n",
    "        \n",
    "- 무얼보고 무얼하는 이런 긴 스토리가 결국에는 처음에 배운 것과 같은 computational graph 하나가 나온 것.\n",
    "    - ML ,NN도 어떤 기능을 하는 DAG를 만들면, 최대한 자동으로 동작한다.\n",
    "\n",
    "\n",
    "\n",
    "#### - Conceptual process\n",
    "- a. Encode: read the entire source sentence to know what to translate\n",
    "- b. Attention: at each step, decide which source token(s) to translate next\n",
    "- c. Decode: based on what has been translated and what need to be translated, predict the next target token.\n",
    "- d. Repeat 2-3 until the [end-of-sentence] special token is generated\n",
    "\n",
    "#### Results\n",
    "\n",
    "<img  src=\"./image/img_7-5.PNG\" width=\"45%\">  \n",
    "\n",
    "- 바다나우 1저자는 인턴으로 거의 다하긴 했는데, 너무 말이 되서 빨리 실험을 했던 결과이다.\n",
    "- 위 모델의 Attention mechanism이 검색과 비슷해 RNNSearch로 불렀다.\n",
    "- 문장이 길어도 성능이 떨어지지 않음을 보여주고, 많은 사람들이 Neural MT를 연구하기 시작했다.\n",
    "- Ex from Last Time:\n",
    "    - Source: \n",
    "        - An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre ** to carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital. **\n",
    "    - When collapsed: \n",
    "        - Un privilège d’admission est le droit d’un médecin de reconnaître un patient à l’hôpital ou un centre médical ** d’un diagnostic ou de prendre un diagnostic en fonction de son état de santé. **\n",
    "    -  RNNSearch:\n",
    "        - Un privilège d’admission est le droit d’un médecin d’admettre un patient à un hôpital ou un centre médical ** pour effectuer un diagnostic ou une procédure, selon son statut de travailleur des soins de santé à l’hôpital. **\n",
    "\n",
    "####  Attention  Mechanism\n",
    "\n",
    "- Attention 메커니즘이 무얼 하는 것인가. 지금까지 Self Attention, (Language모델에서 미래는 보지 않는) Causal Attenntion, MT에서는 Source Side에다가만 하는 Attention을 배웠다. \n",
    "    - 사실 text classification에서의 Attention은 역할이 조금 애매할 수 있지만,\n",
    "    - Translation에서는 명확하다. Target Language의 token을 하나 만들기 위해서는 source side의 어떤 단어들을 봤냐를 의미한다.\n",
    "        - 그래서 MT에서 Attention이 먼저 나오고, self attention로 확장되어 나왔다.\n",
    "\n",
    "- MT에서 Alignment를 얼마나 잘하는지 다음과 같이 볼 수 있다.\n",
    "   \n",
    "<img  src=\"./image/img_7-6.PNG\" width=\"45%\">  \n",
    "\n",
    "\n",
    "<img  src=\"./image/img_7-7.PNG\" width=\"80%\">  \n",
    "\n",
    "- 그리고 Large target vocab에 대한 알고리즘(파라미터가 늘어나 어려워짐) 하나를 만들고, 영독어의 번역에서 1년 사이에 기존의 SOTA 알고리즘 보다 더 좋아졌고, \n",
    "    - 2017년에는 대부분의 언어에서 Finnish를 제외하고 NN을 사용한 것이 가장 효과적인 알고리즘이 되었다.\n",
    "    - 그리고 google, ms, amazon, facebook, systran(번역회사중 가장 오래된 회사), 유럽 특허청, Booking.com 등에서 사용.\n",
    "\n",
    "#### In Practice\n",
    "\n",
    "- Many excellent open-source packages exist:\n",
    "    - Nematus https://github.com/EdinburghNLP/nematus\n",
    "        - Used to build state-of-the-art translation systems for WMT’16 and WMT’17.\n",
    "    - Supported by U. Edinburgh (Rico Sennrich’s group)\n",
    "        - OpenNMT-py https://github.com/OpenNMT/OpenNMT-py\n",
    "        - Implements latest architectures and algorithms\n",
    "    - Supported by Harvard NLP (Sasha Rush’s group)\n",
    "        - FairSeq https://github.com/facebookresearch/fairseq\n",
    "        - Focuses on the convolutional seq2seq\n",
    "    - Supported by Facebook AI Research\n",
    "        - Sockeye https://github.com/awslabs/sockeye\n",
    "        - Supported by Amazon \n",
    "\n",
    "- Facebook: Convolutional sequence-to-sequence models [Gehring et al., 2017]\n",
    "    - Encoder: CNN-based sentence representation\n",
    "    - Decoder: CNN-based conditional language model\n",
    "        - Dilated CNN, GLU 그리고 Attention만\n",
    "\n",
    "- Google: Transforms [Vaswani et al., 2017]\n",
    "    - Encoder: Self-attention based sentence representation\n",
    "    - Decoder: Self-attention based conditional language model\n",
    "        - self attention만을 써서 해보자.\n",
    "        \n",
    "- 이 2개가 거의 성능이 비슷한 SOTA라고 한다.\n",
    "- 이것이, 처음에는 MT를 위해서 만들었었지만 보니까 QA, Image Caption Generation 등에서 다양하게 쓸 수 있었다. \n",
    "\n",
    "\n",
    "#### QA(machine reading) from SQuAD\n",
    "\n",
    "<img  src=\"./image/img_7-8.PNG\" width=\"80%\">  \n",
    "\n",
    "\n",
    "- 1) Encode the paragraph into a set of continuous vectors : $ \\left\\{ p_{1}, p_{2}, ..., p_{T'} \\right\\} $\n",
    "    - e.g., use a bidirectional recurrent network., self attention, convolution ...\n",
    "- 2) Encode the question into a single, fixed-size vector : $ q $\n",
    "    - e.g., use a self-attention network followed by averaging. , RNN\n",
    "- 3) Compare each token in the paragraph against the question using the RN module: $ s_{i} = \\sigma (RN(p_{i}, q)) $\n",
    "    - 이게 Attention Mechanism\n",
    "- 4) Turn the scores into a Categorical distribution using softmax.\n",
    "- 5) Train it by maximizing the log-probability of the correct answer.\n",
    "\n",
    "\n",
    "- 딥마인드에서 먼저 해보고 쭉 해본것.\n",
    "    - Nothing really Changes other than the network Architecture (DAG) \n",
    "\n",
    "\n",
    "#### In this lecture, we’ve learned\n",
    "\n",
    "- What machine translation is:\n",
    "    - It maps a sentence in a source language into its translation in a target language.\n",
    "- What neural machine translation is:\n",
    "    - A single neural network is used to approximate the entire translation process.\n",
    "- How to build an RNN neural machine translation system:\n",
    "    - Encoder: a bidirectional RNN\n",
    "    - Decoder: a unidirectional RNN coupled with the attention mechanism\n",
    "\n",
    "\n",
    "- RNN에 포커스를 맞춰서 봤지만,\n",
    "    - 했던 것들에서 조금만 바꾸면, 유명한 transformer, convolutional seq2seq 등의 모델을 만들 수 있다.\n",
    "    - 더 잘하고 싶으면, sentence represention을 하는 애들의 조합을 잘주면, 더 잘할 수 있다.\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Q. MT의 Evaluation은?\n",
    "    - A. BLEU score: 문장 하나에 대한 사람이 번역한 reference translation 문장 몇 개가 주어진다. (하나 이상 5~10)\n",
    "    - reference 문장에 대하 n-gram 약 4개 정도로 보고, MT가 번역한 문장의 n-gram을 봐서\n",
    "    - Precision을 계산한다. 실제 사람이 썼을 법한 n-gram이 몇 개인지 개수를 계산\n",
    "        - 근데 Precision은 아무 얘기 안하면 100%니까, reference와 length가 얼마나 비슷하냐에 대한 패널티도 준다.\n",
    "    - 이 evaluation 방법이 2001년 논문으로 10년 넘은 paper중 가장 임펙트 있는 페이퍼가 됫다.\n",
    "        - 실제로 이게 높은 것이 잘되고, 이것보다 Human Judging과 상관이 깊은 measure가 없다.\n",
    "        - 그래도 Human Eval이 더 좋긴하다.\n",
    "\n",
    "\n",
    "- Q. RNN을 이제 버리고 다른 곳으로 간다는데 어디로 가는것인가.\n",
    "    - A. RNN이 Parallel implementation이 아니다보니, training이 너무 오래걸려(test는 거의 차이가 없지만)\n",
    "    - 그래서 가는 것이 위의 구글 페북의 MT모델들로 가고 있다.\n",
    "    - 근데 궁극적으로 문제에 따라 서로 다른 알고리즘들이 섞여 해결할 것 같다. 아닐 수도 있지만.\n",
    "    \n",
    "\n",
    "- Q. Training Set이 너무나 커지니까 효과적으로 줄이는 필터링과 같은 방법은?\n",
    "    - A. 이거 자체가 하나의 research topic이다.\n",
    "    - 예를들어 Language Model이 sentence scoring하는 것이 었으니까, 이를 완벽하게 풀수 있다 하면,\n",
    "    - training set을 보고 Noisy한 애를 filtering할 수 있을 것이다.\n",
    "        - 근데 LM을 제대로 training하기 위해서 또 training set에 노이즈가 없어야 한다.\n",
    "    - 그래서 이게 2개가 섞여있는 문제라 다양한 휴리스틱이 있다.\n",
    "        - 통계적으로 걸러내는 방식이 있는데 이것이 이것은 굉장히 problem specific하다.\n",
    "        - 이미지나, 번역이나 등등\n",
    "        \n",
    "\n",
    "- Q. Vocab Size는 어떻게 설정해야하나.\n",
    "    - A. Vocab Size굉장히 중요하다. Vocab Size이 크면 클수록, Softmax에서 비용이 커진다.\n",
    "    - 영어와 불어 번역은 큰 문제가 안되는게, 영어나 불어는 단어가 100만도 안나온다.\n",
    "    - 근데 한국어, 독어, 핀란드어와 같이 morphology(형태)가 다양한 언어는 \n",
    "        - 그냥 빈칸으로 vocab을 만들면 특히 독어는 몇 백만이 나온다.\n",
    "    - 이를 처음에는 Importance Sampling과 같은 알고리즘으로 해결하려고 했다.\n",
    "    - 근데 궁극적으로 Rico Sennrich라는 사람이 subword로 segmentation해보자라고 해서\n",
    "        - 이를 위해 허프만 코딩, 형태소 분석으로 해결하려고 했는데\n",
    "    - 70년데의 Byte Pair Encoding을 하니까 독어를 20K로 커버가능하게 하여 이걸 많이 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: \n",
    "\n",
    "- https://www.edwith.org/deepnlp 조경현교수님, 딥러닝을 이용한 자연어 처리 강의 및 강의 자료 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers:\n",
    "\n",
    "- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation: \n",
    "    - https://arxiv.org/abs/1406.1078\n",
    "\n",
    "- Neural Machine Translation by Jointly Learning to Align and Translate:\n",
    "    - https://arxiv.org/abs/1409.0473\n",
    "\n",
    "- Convolutional Sequence to Sequence Learning: \n",
    "    - https://arxiv.org/abs/1705.03122\n",
    "    \n",
    "- Attention Is All You Need:\n",
    "    - https://arxiv.org/abs/1706.03762\n",
    "    \n",
    "- Neural Machine Translation of Rare Words with Subword Units: (BPE관련)\n",
    "    - https://arxiv.org/abs/1508.07909"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
