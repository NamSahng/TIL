{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## case 1. Learning to Describe Multimedia\n",
    "\n",
    "- 지금까지 Sentence Representation에서부터 Machine Translation 까지의 딥러닝을 이용한 자연어처리를 활용한 것을 보았다.\n",
    "\n",
    "\n",
    "- Machine Translation을 보면,\n",
    "    - Input: a sentence written in a source language\n",
    "    - Output: a corresponding sentence in a target language\n",
    "    - 의 형태에서 Input이 굳이 natural language sentence가 아닌 다른 멀티미디어 데이터여도 이를 설명하는 문장을 output이 나올 것 같다는 질문에 case1은 시작된다.\n",
    "\n",
    "<img  src=\"./image/img_c1-1.PNG\" width=\"70%\"> \n",
    "\n",
    "- 어차피 Machine Translation의 모델이 보는 input은 Continuous vector of sentece니까(sentence가 아닌)\n",
    "    - 따라서, input으로 주어진 것이 language가 아닌 image, speech, video일 때도 이를 continuous vector로 바꾸면, 뒤의 NN 구조를 그대로 사용할 수 있다.\n",
    "    \n",
    "### a. Image Caption Generation [Xu et al., 2015]\n",
    "\n",
    "<img  src=\"./image/img_c1-2.PNG\" width=\"70%\"> \n",
    "\n",
    "- Input: an image\n",
    "- Output: an image caption\n",
    "- Network Architecture:\n",
    "    - Encoder: deep convolution network \n",
    "        - Encoding part의 Attention에서 image representation을 찾는 애로 바꾸어 주면된다.\n",
    "        - 그리고 image recognition을 찾는데 이미 많은 CNN 모델이 잘되있는 모델이 너무 많다. (당시 VGG, Alexnet)\n",
    "    - Decoder: recurrent language model with the attention mechanism.\n",
    "- Data: image-caption pairs\n",
    "    - 이 데이터도 Flicker, MS의 COCO 등 많다\n",
    "    \n",
    "- 그리고 Attention의 Weight을 보면 visualize할 수 있고 이를 하면 다음과 같이 나왔다.\n",
    "    - Attention에서 이미지의 높은 weight을 받은 것이, 밑줄 친 문장\n",
    "<img  src=\"./image/img_c1-3.PNG\" width=\"90%\"> \n",
    "\n",
    "- 잘은 모르겠지만 약간 이것도 Cherry Picking인 느낌으로 말하긴 하시지만, Network이 우리가 원하는 것을 함을 알 수 있다.\n",
    "\n",
    "### b. Video Description Generation [Li Yao et al., 2015]\n",
    "\n",
    "- 그리고 위의 기반과 때 마침 Youtube에서 나온 Video Clip과 Description의 데이터로 Video Description Generation을 시도해보았다.\n",
    "\n",
    "\n",
    "- Input: a short video clip - a sequence of video frames.\n",
    "- Output: a corresponding description\n",
    "- Network Architecture\n",
    "    - Encoder: a deep 2+3D convolutional network (조금 복잡)\n",
    "        - 1. A 2-D convolutional network for each frame (각 프레임은 2D로)\n",
    "        - 2. A 3-D convolutional network for the entire clip (Video 전체는 3D로)\n",
    "    - Decoder: recurrent language modelwith the attention mechanism\n",
    "\n",
    "- 그러나 역시 Video clip Description은 잘 안됐다.\n",
    "    - Video Clip의 몇개 frame과 Description의 문장에서의 Attention weight의 변화\n",
    "<img  src=\"./image/img_c1-4.PNG\" width=\"90%\"> \n",
    "\n",
    "### c. Speech Recognition [Chorowski et al., 2015]\n",
    "\n",
    "- Input: Speech (스펙트럼)\n",
    "- Output: transcription\n",
    "- Network Architecture\n",
    "    - Encoder: convolution+recurrent acoustic network\n",
    "    - Decoder: conditional recurrent language model + attention mechanism\n",
    "\n",
    "- 그러나 speech의 길이가 워낙 긴 문제를 여러가지 방식을 시도해서 당시에는 적당히(?) 잘 된 결과를 보여줌.\n",
    "\n",
    "<img  src=\"./image/img_c1-5.PNG\" width=\"80%\"> \n",
    "\n",
    "- Target쪽에서는 각 phoneme(음소) sequence를 뽑아내게 중간에 각 phoneme이 스펙트럼의 어디에 alignment 없이 training 시켜 visualize해본것 \n",
    "    - \"Michael Colored the bedroom wall with crayons.\" 이라는 val set에서\n",
    "\n",
    "<img  src=\"./image/img_c1-6.PNG\" width=\"80%\"> \n",
    "\n",
    "- 그리고 Attention weight을 계속 check하면 자연스럽게 움직이는 것을 볼 수 있다.\n",
    "- 당시에는 그냥 concept을 체크하는 부분이긴한데\n",
    "- 일종의 Speech Recognition SOTA 시스템은 이와 유사한 시스템을 사용하고 있다.\n",
    "    - 물론 훨씬 크긴하지만.\n",
    "\n",
    "- 이를 통해, Attention, CNN, RNN을 사용해 복잡하고 다양한 문제에서 풀 수 있겠다 한다.\n",
    "\n",
    "\n",
    "### Since 2015…\n",
    "\n",
    "- The attention (alignment) mechanism has become a work horse behind various AI models/applications including\n",
    "    - self attention의 메커니즘을 generate하는데만 쓰는 것이 아니라, 다른 문제를 푸는 NN을 만들 때, RNN , CNN등을 합쳐 복잡한 연산을 할 수 있다는 것을 알고\n",
    "        - Neural Turing machines (differentiable neural computer) [Graves et al., 2015&2016]\n",
    "        - Memory networks [Weston et al., 2015; Sukhbaatar et al., 2016]\n",
    "        - Dynamic neural Turing machines (key-value memory net)  [Gulcehre et al., 2017; Miller et al., 2017], …\n",
    "    - 이렇게 attention의 메커니즘을 활용한 것이 하나의 중요한 컴포넌트가 되어, 강화학습의 agent 또는 복잡한 reasoning을 활용하는데 사용한다. 강화학습의 history를 select할 때,\n",
    "        - Reinforcement learning: attentive history selection [Tian et al., 2016]\n",
    "    - 강화학습은 learning하고, data collection을 동시에 하는 것인데, 이 때 data collection을 좀더 효율적으로하게 하기 위해서\n",
    "        - Neural episodic control [Pritzel et al., 2017]\n",
    "    - Generative model하면 요즘 GAN만 생각하기 쉬운데, language model도 Generative model이고, 분포를 배운 후에 그 분포에서 likely한 sample을 뽑을 수 있는 것이 Generative model이다. 이렇게 image, speech 등을 generate할 때도 어텐션 메커니즘이 많이 쓰인다. 일부만 generate된 이미지에서도 빈부분을 채우는 당시 획기적이었던\n",
    "        - Generative models: DRAW [Gregor et al., 2016]\n",
    "    - self attention을 이용해 이미지를 읽고 이미지를 generate하는, 즉 machine translation을 하되 sentence가 2d인 느낌\n",
    "        - Image Transformer [Parmar et al., 2018], …\n",
    "    \n",
    "    \n",
    "    \n",
    "- 이렇게 Attention 메커니즘들이 다양하게 많이 쓰이는 것을 보았고, 여기서 가장 중요한 포인트는 Continuous Vector space에 Encoding (projection) 하는 것.\n",
    "    - Discrete Space나 주어진 surface form안에서 해야한다 했으면, 이런 발전이 안이루어 졌을 것이다.\n",
    "\n",
    "- 그러나 ** BUT ** 그래도 이제 시작이다. 박테리아보다 못하다. 이제 눈좀뜨고 살짝 듣고 그런 수준이다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: \n",
    "\n",
    "- https://www.edwith.org/deepnlp 조경현교수님, 딥러닝을 이용한 자연어 처리 강의 및 강의 자료 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers:\n",
    "\n",
    "- Image: \n",
    "    - Kelvin Xu, et al. \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\", 2015\n",
    "        - https://arxiv.org/abs/1502.03044\n",
    "- video: \n",
    "    - Li Yao, et al. \"Describing Videos by Exploiting Temporal Structure\", 2015\n",
    "        - https://arxiv.org/abs/1502.08029\n",
    "- speech: \n",
    "    - Jan Chorowski, et al. \"Attention-Based Models for Speech Recognition\", 2015\n",
    "        - https://arxiv.org/abs/1506.07503\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "- Other applications of Attention Mecahnism:\n",
    "    - Alex Graves, et al. \"Neural Turing Machines\", 2014\n",
    "        - https://arxiv.org/abs/1410.5401\n",
    "    - Jason Weston, et al. \"Memory Networks\", 2014\n",
    "        - https://arxiv.org/abs/1410.3916\n",
    "    - Sainbayar Sukhbaatar, et al. \"End-To-End Memory Networks\", 2015\n",
    "        - https://arxiv.org/abs/1503.08895\n",
    "    - Caglar Gulcehre, et al. \"Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes\", 2016\n",
    "        - https://arxiv.org/abs/1607.00036\n",
    "    - Alexander Pritzel, et al. \"Neural Episodic Control\", 2017\n",
    "        - https://arxiv.org/abs/1703.01988\n",
    "    - Karol Gregor, et al. \"DRAW: A Recurrent Neural Network For Image Generation\"\n",
    "        - https://arxiv.org/abs/1502.04623\n",
    "    - Niki Parmar, et al. \"Image Transformer\", 2018\n",
    "        - https://arxiv.org/abs/1802.05751\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
