{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 결정이론(Decision Theory)\n",
    "\n",
    "1.2절에서 우리는 불확실성을 정량화하고 조작하는 수학적 토대로 확률론을 살펴보았다.<br>\n",
    "패턴 인식 문제를 풀 때는 불확실성이 존재하는 상황에서 의사 결정을 내려야 하는 경우가 많다. 이런 상황에서 결정이론과 확률론을 함께 사용하면 최적의 의사결정 내릴 수 있다.\n",
    "\n",
    "추론(Inference)문제의 대표적 예시는 p(x,t)를 찾는 것이다.(regression/classification: input vector(x), target vector(t: continous / labeled))<br>\n",
    "이는 매우 어려운 문제로, 이 문제에 대한 해결책이 이 책의 많은 부분을 차지한다. t가 어떤 값을 가질 것 같은지를 바탕으로 특정 행동을 취해야 할 수도 있으며 이를 위한 이론적 토대가 바로 **결정이론** 이다.\n",
    "\n",
    "#### Q. 암환자 분류 문제\n",
    "\n",
    "$\\mathbf{x}$: 입력벡터 / $C_{1}: 암존재 \\ \\  C_{2}: 건강함$\n",
    "<br>\n",
    "일반적인 추론문제는 결합확률분포 $p(\\mathbf{x},C_{k})$를 결정하는 과정을 포함하고 있다. 해당 상황에 대해서 가장 완전하고 확률적인 설명을 알려줄 수 있는 것이 결합확률 분포!<br>\n",
    "결합확률분포는 매우 유용한 값이긴 하지만 최족적으로 우리가 하고 싶은 것은 치료할지 말지 결정하는 것! 해당 결정이 최적이기를 바라는데 이것이 바로 **결정**단계이다. 결정이론이 하려는 것은 적절한 확률들이 주어진 상태에서 어떻게 하면 최적의 결정을 내릴 수 있는가를 설명하는 것이며 추론 문제를 풀면 간단하다. (자세한 결정이론 내용은 Beiger(1985) Bather(2000)참고)\n",
    "\n",
    "의사결정에서 확률의 역할\n",
    "$$ p(C_{k} | \\mathbf{x} ) = {{p(\\mathbf{x} | C_{k}) p(C_{k})} \\over {p({\\mathbf{x}) }}} $$\n",
    "\n",
    "위의 모든 값은 결합확률분포 $p(\\mathbf{x},C_{k})$를 알면 Margininalization과 Conditioning을 활용해 구할 수 있다. <br>\n",
    "만약 우리의 목표가 $\\mathbf{x}$를 최소화하는 것이면, 직관적으로 우리는 더 높은 사후 확률을 가진 클래스를 고를 것이다. 이후에는 이 직관이 맞음을 설명하고, 일반적인 의사결정의 기준에 대해 살펴보자\n",
    "\n",
    "#### 1.5.1 오분류 비율의 최소화\n",
    "\n",
    "Decision Region(결정구역): 오분류를 줄이는 것이 목적일 때, 각 x를 가능한 클래스 중 포함하는 규칙의 입력공간의 구역 $\\mathcal{R}_{k}$<br>\n",
    "Decision Boundary(Surface): 결정 구역 사이의 경계. 결정구역은 인접할 필요 없으며 두 구역위에 있을 수도 있음.\n",
    "\n",
    "Error가 발생할 확률을 줄이는 방법(곱의 법칙을 활용해 최소화)하는 방법은 수식적으로도 올바르게 분류할 확률을 높이는 방법(극대화)과 같으며 이것이 더 쉽다.\n",
    "\n",
    "$$  p(correct) = {\\sum^{K}_{k=1} p(\\mathbf{x} \\in \\mathcal{R}_{k}, \\mathcal{C}_{k})} = {\\sum^{K}_{k=1} \\int_{\\mathcal{R}_{k}} p(\\mathbf{x}, \\mathcal{C}_{k}) d \\mathbf{x}}$$\n",
    "\n",
    "각 x가  $ p(\\mathbf{x}, \\mathcal{C}_{k})$가 최대인 클래스로 분류 되도록 $\\mathcal{R}_{k}$를 선택할 경우 위 식이 최대가 된다. 곱의 법칙을 적용하면 $ p(\\mathbf{x}, \\mathcal{C}_{k}) = p(\\mathcal{C}_{k} | \\mathbf{x})p(\\mathbf{x})$ 이며, 각각의 $\\mathbf{x}$는 가장 큰 사후 확률 $p(\\mathcal{C}_{k} | \\mathbf{x})$를 갖는 클래스로 분류되어야 함을 확인 할 수 있다.\n",
    "\n",
    "#### 1.5.2 기대 손실의 최소화\n",
    "\n",
    "실생활에서는 단순히 오분류를 줄이는 것보다 더 복잡.<br>\n",
    "예를 들어 암환자를 건강하다고 판단하는 오류가 더 심각함.<br>\n",
    "Loss Function(Cost)를 도입해 문제 공식화 가능. (효용함수(Utility)는 최대화하는 것으로 반대개념)\n",
    "\n",
    "손실행렬 $L_{kj}$을 가정하면 $\\mathbb{E}[L] = \\sum_{k} \\sum_{j} \\int_{\\mathcal{R}_{j}} L_{kj} p(\\mathbf{x},\\mathcal{C}_{k}) d\\mathbf{x} $ 로 표현가능하고, 이는 각 $\\mathbf{x}$에 대해 $\\sum_{k} p(\\mathbf{x},\\mathcal{C}_{k})$를 최소화 해야한다는 것을 의미하고, 앞과 같이 곱의 법칙을 활용하면, $p(\\mathbf{x},\\mathcal{C}_{k}) = p(\\mathcal{C}_{k}|\\mathbf{x}) p(\\mathbf{x}) $임으로 공통인자 $p(\\mathbf{x})$를 제거 할 수 있다. 따라서 기대 손실을 최소화 하는 결정 법칙은 각 $\\mathbf{x}$를 $\\sum_{k} L_{kj} p(\\mathcal{C}_{k}|\\mathbf{x}) $를 최소화하는 클래스 j에 할당하는 것이다. 각 클래스에 대한 $p(\\mathcal{C}_{k}|\\mathbf{x}) $를 알면 이 방법을 쉽게 실행할 수 있다.\n",
    "\n",
    "\n",
    "#### 1.5.3 거부 옵션\n",
    "\n",
    "결정을 내리기 힘든 지역에 대해 결정을 피할 수 있음. 손실행렬이 주어진 경우에는 기대 손실값을 최소화 하도록 거부 옵션을 확장 가능.\n",
    "\n",
    "#### 1.5.4 추론과 결정\n",
    "\n",
    "지금까지 분류문제를 **추론 단계(Inference Stage)**와 **결정 단계(Decision Stage)**로 나누었다. <br>\n",
    "추론단계: training set을 활용해 $p(\\mathcal{C}_{k}|\\mathbf{x}) $에 대한 모델을 학습시키는 단계 <br>\n",
    "결정 단계: 학습된 사후 확률들을 이용해 최적의 클래스 할다을 시행. <br>\n",
    "두가지 문제를 한 번에 풀어내는 방법도 있다. $\\mathbf{x}$에서 결정값을 돌려주는 함수(**판별함수 discriminatn function**)를 직접 학습시키는 것.\n",
    "\n",
    "사실 결정문제를 푸는 방법은 3가지다.<br>\n",
    "  \n",
    "(a) 베이지안 정리 활용: 각 클래스 $C_{k}$에 대해 조건부 확률 밀도 $p(\\mathbf{x}|\\mathcal{C}_{k})$를 구하는 추론문제를 풀고, 클래스별 사전확률 $p(C_{k})$를 구해 사후확률 $p(C_{k}|\\mathbf{x})$를 구한다. <br> 또는 이와 동일하게 결합분포 $p(\\mathbf{x},\\mathcal{C}_{k})$를 직접 모델링한 후 정규화를 통해 사후 확률을 구할 수 있다. 사후확률을 구한 후 결정 이론을 적용해 각각의 새 입력 변수 $\\mathbf{x}$에 대한 클래스를 구한다. 직간접적으로 입력값과 출력값의 분포를 모델링하는 이런 방식을 **생성 모델(Generative Model)**이라 한다. 왜냐하면 이렇게 만들어진 분포로 부터 표본을 추출함으로써 입력 공간에 합성 데이터 포인트들을 생성해 넣는 것이 가능하기 때문이다.\n",
    "\n",
    "단점: 결합분포를 찾아야 하므로, 가장 손이 많이 간다. 많은 응용 사례에서 $\\mathbf{x}$는 고차원이며, 따라서 각각의 클래스에 대해 일정 수준 이상의 조건부 밀도를 구하기 위해 큰 Training Set이 필요할 수 있다.<br>\n",
    "장점: $p(\\mathbf{x}) = \\sum_{k} p(\\mathbf{x}|\\mathcal{C}_{k}) p(\\mathcal{C}_{k}) $를 이용해 p(x)의 주변 밀도도 구할 수 있다. 이를 바탕으로 주어진 모델하에 발생 확률이 낮은 새 데이터 포인트를 미리 발견해 검출(**이상점 검출(Outlier Detectiion)**)가능.\n",
    "\n",
    "(b) **판별모델(Discriminative Model)**: 사후확률 $p(C_{k}|\\mathbf{x})$를 계산하는 추론 문제를 풀어낸 후 결정이론을 적용해 각 입력변수 x에 대한 클래스를 구함. 사후확률을 직접모델링하는 방법.\n",
    "\n",
    "장점: 분류알고리즘을 통해 결정을 내리면 a에서 결합 분포를 전부 계산하는 계산 낭비 및 데이터 요구량 감소로 더 효율적. ML에서 생성과 판별모델을 각각 사용하는 것의 장단점 논의는 많이 있어왔고 합치려는 시도도 많았다.\n",
    "\n",
    "(c) 각각의 입력값 $\\mathbf{x}$를 클래스에 사상하는 판별함수 $f(x)$를 찾는 것. 예를 들어 클래스가 2개면  $f=0, 1$로 나뉘게. 여기서는 확률론이 사용되지 않음. 추론과 결정단계를 합친 방식. \n",
    "\n",
    "단점: 사후확률 $p(C_{k}|\\mathbf{x})$을 알 수 없음.\n",
    "\n",
    "- 사후확률 $p(C_{k}|\\mathbf{x})$을 알 수 없을 때의 단점\n",
    "\n",
    "a) 위험의 최소화<br>\n",
    "손실 행렬의 값들이 때때로 변하는 금융관련 문제의 경우는 사후확률을 알 때, $\\sum_{k} L_{kj} p(\\mathcal{C}_{k}|\\mathbf{x}) $을 수정함으로 최소 위험 결정 기준을 구할 수 있음. 판별함수만 알면, 손실행렬이 변할 때마다 분류문제를 새로 풀어야 함.\n",
    "\n",
    "b) 거부옵션\n",
    "\n",
    "사후확률을 알면 주어진 거부 데이터 포인트 비율에 대해 오분류 비율(기대 손실값)을 최소화하는 거부 기준을 쉽게 구할 수 있음.\n",
    "\n",
    "c) 클래스 사전 확률에 대한 보상\n",
    "\n",
    "예를 들어, 1000개의 이미지 중 하나만 암 환자의 이미지만 있을 때 생기는 Class Imbalance 문제를 해결하기위해 training set을 balanced되게 하면 보상을 적용해야한다. 사후확률을 활용하는 것은 사전확률을 곱하고 정규화하여 보상해줄 수 있지만, 직접 판별함수를 구하는 방식은 수정이 불가능하다\n",
    "\n",
    "d) 모델들의 결합\n",
    "\n",
    "여러 다른 종류의 정보를 하나의 입력 공간으로 합치는 것보다 엑스레이 이미지를 해석하는 시스템과 혈액검사 결과 시스템을 따로 만드는 것이 효율적일 수 있다. 두 모델이 각 클래스에대한 사후확률을 제공할 때 확률의 법칙을 적용해 시스템적으로 서로 다른 출력값을 합하는 것이 가능하다. **조건부 독립(Conditional Independence)**의 성질을 이용한 가정(이미지의 분포와 혈액분포가 독립임을)하는 것이 가장 쉬운 방법이다. \n",
    "$ p(\\mathbf{x_{I}},\\mathbf{x_{B}}|\\mathcal{C}_{k})= p(\\mathbf{x_{I}}|\\mathcal{C}_{k})p(\\mathbf{x_{B}}|\\mathcal{C}_{k}) $ <br>\n",
    "$ \\begin{matrix} {p(\\mathcal{C}_{k}|\\mathbf{x_{I}},\\mathbf{x_{B}})} &\\propto& {p(\\mathbf{x_{I}},\\mathbf{x_{B}}|\\mathcal{C}_{k}) p(\\mathcal{C}_{k})} \\\\\n",
    "&\\propto& {p(\\mathbf{x_{I}}|\\mathcal{C}_{k})p(\\mathbf{x_{B}}|\\mathcal{C}_{k}) p(\\mathcal{C}_{k})} \\\\\n",
    "&\\propto& {{p(\\mathcal{C}_{k}|\\mathbf{x_{I}})p(\\mathcal{C}_{k}|\\mathbf{x_{B}})}\\over{p(\\mathcal{C}_{k})}}\n",
    "\\end{matrix}  $<br>\n",
    "사전확률 $p(\\mathcal{C}_{k})$는 클래스별 비율로 유추할 수 있다. 특정 조건부 독립 가정은 **나이브 베이즈 모델**의 예시이다. 결합 확률 분포 $p(\\mathbf{x_{I}},\\mathbf{x_{B}})$는 보통 나이브 베이즈 모델하에서 인수분해가 되지 않는다. 이후에 조건부 독립의 가정 없이도 데이터를 결합시키는 방법에 대해 살펴보자.\n",
    "\n",
    "#### 1.5.5 회귀에서의 손실 함수\n",
    "\n",
    "회귀 문제의 결정 단계에서는 각각의 x에 대해 t의 추정값 y(x)를 선택해야한다. 여기서 손실 L(t,y(x))가 발생할 때 평균(기대)손실은 $\\mathbb{E}\\left[ L \\right] = \\int \\int L(t,y(\\mathbf{x}))p(\\mathbf{x},t)d\\mathbf{x}dt$ 회귀에서 주로 사용하는  제곱손실 $L(t,y(\\mathbf{x})) = \\{ y(x)-t  \\}^{2}$ 를 대입하면 $\\mathbb{E}\\left[ L \\right] = \\int \\int \\{ y(x)-t  \\}^{2}p(\\mathbf{x},t)d\\mathbf{x}dt$ <br>\n",
    "우리의 목표는 $\\mathbb{E}\\left[ L \\right]$ 를 최소화하는 y(x)를 찾는 것. 만약 완벽하게 유연하게 함수 y(x)를 결정할 수 있다고 가정하면 [변분법](https://github.com/NamSahng/Summary/blob/master/Pattern_Recognition%26Machine_Learning/D.%20Supplement%20Calc_of_Variation.ipynb)을 적용해서 다음과 같이 적을 수 있다.\n",
    "$$ {{ \\delta \\mathbb{E}\\left[ L \\right]} \\over {\\delta y(x)} } = 2 \\int \\{ y(x)-t \\}p(\\mathbf{x},t)dt $$ \n",
    "식 y(x)에 대해서 해를 구해 확률의 합과 곱의 법칙을 적용하면, 다음을 얻는다.\n",
    "$$ y(x) = {{\\int tp(x,t) dt} \\over {p(x)} } = \\int tp(t|x)dt = \\mathbb{E}_{t}\\left[ t|x \\right] $$\n",
    "위식은 x가 주어졌을 때의 t의 조건부 평균으로써의 **회귀함수**라고 한다. 52pg 아래에 결과가 시각화 되어있으며 타겟 변수가 벡터 t로 표현되는 다차원 변수일 경우에 대해서도 쉽게 확장 가능하다.<br>\n",
    "이 경우 최적의 해는 조건부 평균 $y(x) = \\mathbb{E}_{t}\\left[ t|x \\right] $이다.\n",
    "\n",
    "다른 인사이트를 위해 $ {\\{ y(x) - t \\}^{2}} = { \\{ y(x) - \\mathbb{E}\\left[ t|x \\right] + \\mathbb{E}\\left[ t|x \\right] - t \\} ^{2}} = { {\\{ y(x) - \\mathbb{E}\\left[ t|x \\right] \\} ^{2} } + 2{\\{ y(x) - \\mathbb{E}\\left[ t|x \\right] \\}} {\\{ \\mathbb{E}\\left[ t|x \\right] -t \\}} + {\\{ \\mathbb{E}\\left[ t|x \\right] - t \\}}^{2} } $<br> \n",
    "(여기서 $ \\mathbb{E}_{t}\\left[ t|x \\right]$를 $ \\mathbb{E}\\left[ t|x \\right]$로 표기)로 전개하고 이 결과를 손실함수에 대입하고 t에 대해 적분하면 교차항이 사라지며 손실을 알 수있다. \n",
    "\n",
    "$$ \\mathbb{E} \\left[ L \\right] = \\int { \\{ y(x) - \\mathbb{E}\\left[ t|x \\right] \\} ^{2} p(x) dx } + \\int var \\left[ t|x \\right] p(x) dx  $$ \n",
    "여기서 $var \\left[ t|x \\right] p(x)$ 가 손실함수가 되며, 최적의 최소 제곱 예측은 조건부 평균으로 주어진다는 것을 보여준다. 두 번째 항은 t에 대한 분포의 분산을 계산하고, 이를 x에 대해 평균을 낸 것이다. 이 항은 표적 데이터가 가진 내재적 변동성을 표현하므로 노이즈로 해석 가능하다. 이 항은 y(x)에 대해 독립이며 더 이상 줄일 수 없는 손실함수의 최솟값에 해당한다.\n",
    "\n",
    "분류와 같이 회귀 문제에서 결정을 내리는 방식은 다음과 같다. \n",
    "\n",
    "(a) 결합 밀도 $ p(x,t )$ 를 구하는 추론문제를 푼다. 다음에 정규화하여 조건부 밀도 $ p(t|x) $를 구해 최종적으로 위와 같은 조건부 평균을 구한다.<br>\n",
    "(b) 조건부 밀도 $ p(x,t) $ 를 구하는 추론문제를 풀고 조건부 평균을 구한다.\n",
    "(c) 훈련 데이터로부터 회귀함수 $ y(x) $ 를 직접 구한다.\n",
    "\n",
    "조건부 분포 $ p(t|x) $ 가 다봉 분포인 상황에서, 제곱손실이 상당히 좋지 않은 결과를 가져오기 때분에 좀 더 복잡한 제곱손실을 일반화한 **민코프스키 손실(Minkowski Loss)**을 사용할 수 있다. 민코프스키 손실의 기댓값은 다음과 같다.\n",
    "$$ \\mathbb{E} \\left[ L_{q} \\right] = \\int \\int { \\left\\vert y(x) - t \\right\\vert ^{q} p(x,t) } dx dt $$\n",
    "\n",
    " $ q = 2 $ 일 경우 제곱 손실에 해당하며 54pg에서 다양한 q값에 대한 함수 $ \\left\\vert y(x) - t \\right\\vert ^{q} $ 와 $ y-t $ 값의 그래프를 확인 할 수 있다. $ \\mathbb{E} \\left[ L_{q} \\right] $ 의 최소값은 $ q = 2 $ 일 경우에 조건부 평균으로  $ q=1 $ 일 경우에는 조건부 중간값으로, $ q \\to 0 $ 일 경우에는 조건부 최빈값으로 주어지게 된다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
