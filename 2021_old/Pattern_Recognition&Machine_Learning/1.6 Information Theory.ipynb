{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 정보이론(Information Theory)\n",
    "\n",
    "지금까지 뒤부분의 토대가될 확률론과 결정이론의 다양한 부분을 살펴보았다. PR과 ML 테크닉을 이해하는데 또 하나의 중요한 이론인 정보이론을 보자. 지금과 마찬가지로 중요한 개념 위주로 볼 것이다. 자세한 내용은 (Vitrbi and Omura, 1979)를 보자.\n",
    "\n",
    "이산확률변수 x를 고려해보자.<br>\n",
    "이 변수가 특정 값을 갖고 있을 확인했을 때 전해지는 정보량은 x의 값을 학습하는 데 있어 '놀라움의 정도'(Degree of Surprise)로 생각할 수 있다. 매우 가능성이 높은 사건이 일어났다는 사실 보다 가능성이 희박한 사건이 발생했다는 것이 더 많은 정보를 받은 것으로 생각할 수 있다.\n",
    "\n",
    "따라서 우리가 사용하게 될 정보량의 측정 단위는 확률분포 p(x)에 종속이며 이에 대해 단조 함수인 정보량을 표현하는 함수 h(x)를 살펴보자.<br>\n",
    "서로 독립인 두 사건 x, y에서 x와 y가 함게 일어 났을 때의 정보량은 h(x,y) = h(x) + h(y)이다. 여기서 독립인 두 사건이 일어날 확률은 p(x,y) = p(x)p(y) 이므로 h(x)는 p(x)의 로그에 해당하는 것을 알 수 있다.\n",
    "$${ h(x) = {- log_{2}p(x) }}$$\n",
    "여기서 음의 부호는 정보량이 양수를 갖게 하기 위한 것이며, 로그의 밑은 임의로 정할 수 있다.(여기서 밑 2를 사용한 것은 정보 이론학계의 관습을 따른 것, 밑을 2를 사용할 경우 h(x)의 단위는 비트가 됨(이진비트)) 그리고 사건의 확률이 낮을 수록 얻는 정보량이 큰 것을 확인 가능.\n",
    "\n",
    "송신자가 어떤 확률변수의 값을 수신자에게 전송하는 상황에서 전송에 필요한 정보량의 기댓값을 **엔트로피라(Entorpy)**라고하며 다음과 같다.\n",
    "$${ H \\left[ x \\right] = - \\sum_{x} p(x) log_{2} p(x) }$$\n",
    "($lim_{p \\to 0} p ln p = 0$이기 때문에  p(x) = 0 에서, 걱정하지 않아도 됨.)\n",
    "\n",
    "- 이산 비균등분포와 이산 균등분포의 엔트로피 비교<br>\n",
    "{a,b,c,d,e,f,g,h}가 일어날 확률이 모두 1/8일 때, 엔트로피 $ H \\left[ x \\right] $ = 3bits <br>\n",
    "일어날 확률이 (1/2,1/4,1/8,1/16,1/64,1/64,1/64,1/64)일 때, $ H \\left[ x \\right] $ = 2bits로 이산 비균등 분포의 엔트로피가 이산 균등 분포의 엔트로피 보다 낮다.<br>\n",
    "엔트로피는 무질서의 척도로 해석하는 것과도 연관이 있다. \n",
    "\n",
    "수신자에게 변수의 상태를 전달하는 것에 초점을 맞추면, 전과 같이 3비트의 숫자를 사용하여 전달이 가능하다. 하지만 이산 비균등 분포라는 점을 활용하면 더 짧은 숫자를 사용하는 것도 가능하다. 확률이 높은 사건에 대해 짧은 코드를 부여하고 확률이 낮은 코드에 긴코드를 부여하면 ex) (0, 10, 110, 1110, 111100, 111101, 111110, 111111)를 사용할 때 평균 코드 길이는 = (1/2 * 1)  + (1/4 * 2) + ... + (4 * 1/64  * 6) = 2bits 임을 알 수 있다. (긴 코드 하나가 짧은 코드 2개가 붙여진 것과 같은 형태가 되면 해석이 불문명해지는 혼돈이 와 더 짧은 코드를 사용할 수 없다. ex) 11001110은 c, a, d로만 해석 가능)\n",
    "\n",
    "엔트로피와 가장 짧은 코드 길이 사이의 관계는 일반 적이며, **노이즈 없는 코딩이론(Noiseless Coding Theorem)**에 따르면 엔트로피는 확률 변수의 상태를 전송하기 위해 필요한 비트 숫자의 하한선이다.\n",
    "\n",
    "** 내트(Nats) **: 엔트로피를 정의하는데 자연로그를 사용하는 것, 비트와 ln2만큼 차이남.<br>\n",
    "엔트로피를 다른 관점에서 이해해보자. N개의 동일한 물체가 몇개의 통안에 담겨 있다고 가정. i번째 통안에 ni개의 물체가 담기도록. N개의 물체를 통에 나누어 담는 방법은 총 N!개의 방법이 있으며, i번째 통에는 물체를 정렬하기 위한 ni!가지 방법이 있어 물체를 통에 넣는 가짓 수를 다중도라고 하며 다음과 같이 정의한다.\n",
    "$${ W = {{N!} \\over {\\prod_{i} n_{i}!} }} $$\n",
    "엔트로피는 다중도의 로그를 취해 적절한 함수로 나눈 것이다.\n",
    "$$  H = {{1} \\over {N} } ln {W} = {{{1} \\over {N} } ln {N!} - {{1} \\over {N} } \\sum ln ({n_{i}!} )   }$$\n",
    "여기서 스털링의 근사식 $ lnN! \\simeq N ln N - N $을 적용하면\n",
    "$$ H = {- \\lim_{N \\to \\infty} \\sum_{i}  \\left( {n_{i}} \\over {N}  \\right) ln \\left( {n_{i}} \\over {N}  \\right) = - \\sum_{i} p_{i}ln (p_{i}) } $$\n",
    "$sum_{i} n_{i} = N$임을 이용했으며, $p_{i} = \\lim_{N \\to \\infty}(n_{i}/N) $은 물체가 i번째 통에 속하게 될 확률. 물리학 용어로<br>\n",
    "**미시상태(Microstate)**: 통안의 물체들의 순서. <br>\n",
    "**거시상태(Macrostate)**: ni/N으로 표현되는 통 각각이 가지고 있는 물체의 숫자 비율 <br>\n",
    "그리고 다중도 W를 거시상태의 가중치라고 하기도 한다.<br>\n",
    "통을 확률변수 X의 상태 xi로 해석하면 p(X = xi) = pi 이므로 확률 변수 X의 엔트로피는 \n",
    "$${ H \\left[ p \\right] = - \\sum_{i} p(x_{i}) lnp(x_{i}) }$$\n",
    "\n",
    "그림 1.30을 보면 확률분포가 뾰족하면 낮은 엔트로피, 퍼져있으면 높은 엔트로피를 갖는 것을 볼 수 있으며, 엔트로피가 최대가 되는 경우는 [라그랑주 승수법](https://github.com/NamSahng/Summary)을 활용해 H의 최댓값을 찾아냄으로 알 수 있다. H에 확률의 정규화 제약조건을 포함하면,  \n",
    "$$ \\tilde{H} = - \\sum_{i} p(x_{i}) lnp(x_{i}) + \\lambda \\left(  \\sum_{i} p(x_{i}) -1 \\right) $$\n",
    "위 식이 최대화되는 경우는 균등분포에서 이며, xi의 상태의 가지수가 M이면 엔트로피 값 H = lnM이다.<br>\n",
    "이 결과는 ** 옌센의 부등식(Jensen's Inequality) **으로 유도 가능하며 엔트로피의 이차 미분을 통해 이 임계접이 실제로 최대치라는 것을 확인할 수 있다. $I_{ij}$는 항등 행렬 원소.\n",
    "$$ {{\\partial^{2} \\tilde{H}} \\over {\\partial p(x_{i}) \\partial p(x_{j}) }}  = {- I_{ij} {{1} \\over {p_{i}}}} $$\n",
    "\n",
    "연속변수 x에 대한 분포 p(x)를 포함하는 과정을 보자. 먼저 x를 너비 $\\Delta$의 여러 구간으로 나누고 p(x)가 연속적이라고 가정하고 **평균값의 정리** 에 따라 다음을 만족하는 xi값이 존재한다. $\\int_{i\\Delta}^{(i+1)\\Delta} p(x) dx = p(x_{i})\\Delta $ 이를 모든 x값에 대해 해당 값이 i번째 칸에 속할 경우 값 xi를 할당하여 연속적인 변수 x를 정량화 할 수 있다. xi를 관측하게 될 확률은 $p(x_{i})\\Delta$이며, 이 때 해당 안트로피는 다음과 같이 나타낼 수 있다. ($\\sum_{i} p(x_{i})\\Delta = 1$)\n",
    "$ H \\Delta = - \\sum_{i} p(x_{i})\\Delta ln (p(x_{i}) \\Delta ) = - \\sum_{i} p(x_{i})\\Delta ln (p(x_{i})) - ln \\Delta $\n",
    "여기서 우변 두 번째항 $- ln \\Delta$를 제외하고 $ \\Delta \\to 0 $을 고려하면 우변의 첫번째 항 p(x)는 lnp(x)의 적분 값에 가까워진다.\n",
    "$$ \\lim_{\\Delta \\to 0} \\left\\{ - \\sum_{i} p(x_{i})\\Delta ln (p(x_{i})) \\right\\} = - \\int p(x)  ln p(x) dx $$\n",
    "위 식의 우변을 **미분 엔트로피(Differential Entropy)**라고 하며 이산 엔트로피와 미분 엔트로피는 $ln \\Delta$만큼 차이 남을 확인할 수 있다. $\\Delta \\to 0$를 취하면 $ln \\Delta$값은 발산한다. 따라서 연속변수의 엔트로피를 정확하게 지정하기 위해서는 아주 많은 수 의 비트가 필요함을 알 수 있다. 여러 변수들에 대해 정의된 밀도의 경우(함께 모아 벡터 $\\mathbf{x}$로 표현) 미분엔트로피는 다음과 같다.\n",
    "$$ H \\left[ \\mathbf{x} \\right] = - \\int p(\\mathbf{x}) ln (p(\\mathbf{x})) d\\mathbf{x} $$\n",
    "이산분포의 경우에는 확률 분포가 이산균등분포일 때 엔트로피 값이 최대가 되는 것을 확인 했다. 연속 변수일 때는 어떤지 알아보자. 최댓값을 잘 정의하기 위해 p(x)의 1차, 2차 모멘트와 정규화 상수에 제약조건을 두는 것이 필요하다. 우리의 제약조건은 다음과 같다.(확률분포의 총 적분값은 1이며 평균과 분산에 관한 식.)\n",
    "$$ \\int_{-\\infty}^{\\infty} p(x)dx = 1  $$\n",
    "$$ \\int_{-\\infty}^{\\infty} xp(x)dx = \\mu  $$\n",
    "$$ \\int_{-\\infty}^{\\infty} (x-\\mu)^{2} p(x)dx = \\sigma^{2}  $$\n",
    "라그랑주 승수법과 위 제약조건을 활용해 최댓값을 구할 수 있으며, 다음 범함수의 최댓값을 p(x)에 대해 구하여야한다.\n",
    "$$ {- \\int_{-\\infty}^{\\infty} p(x) ln (p(x)) dx} + { \\lambda_{1} \\left( \\int_{-\\infty}^{\\infty} p(x)dx - 1  \\right) + \\lambda_{2} \\left( \\int_{-\\infty}^{\\infty} xp(x)dx - \\mu \\right) + \\lambda_{3} \\left( \\int_{-\\infty}^{\\infty} (x-\\mu)^{2} p(x)dx - \\sigma^{2} \\right) } $$\n",
    "변분법을 활용해 범함수를 미분해 그 값이 0과 같다고 하면 다음 식이 나온다.\n",
    "$$ p(x) = exp \\left\\{ -1 + \\lambda_{1} + \\lambda_{2}x + \\lambda_{3}(x-\\mu)^2 \\right\\} $$\n",
    "라그랑주 승수는 이 결과를 3개 제약 조건식에 다시 대입함으로 구할 수 있다. 이의 최종 결과는\n",
    "$$ p(x) = {{1} \\over ({2\\pi\\sigma^{2})^{1/2}}} {exp \\left\\{ - {{(x-\\mu)}^{2} \\over {2\\sigma^{2}}} \\right\\} }$$\n",
    "즉 미분 엔트로피의 값을 최대화하는 분포는 가우시안 분포임을 알 수 있다. 엔트로피의 최댓값을 구할 때 분포가 음수값이 아니어야하는 제약조건은 두지 않았다. 하지만 결국 결과로 얻게 된 분포가 실제로 음수 값이 아니므로 필요가 없음을 알 수 있다. 가우시안의 엔트로피는 다음과같다\n",
    "$$ H \\left[ x \\right] = {{1}\\over{2}} \\left\\{ 1 + ln(2\\pi\\sigma^{2}) \\right\\}  $$\n",
    "분포가 넓게 퍼져있을 수록(분산, 표준편차가 클수록) 엔트로피는 증가하며 미분엔트로피는 이산엔트로피와 달리 음의 값을 가질 수 있다.(위 식에서 $\\sigma^{2} < 1/(2\\pi e)$ 이면 $H(x) < 0$ 이므로) \n",
    "\n",
    "x값과 y값을 함께 고려하는 결합분포 p(x,y)에 대해 고려할 때, 만약 x의 값이 알려져 있다면, y값을 알기위해 필요한 정보는 -ln(p(y|x))로 주어지다. 따라서 y를 위한 필요한 정보의 평균값은 다음과 같다.\n",
    "$$ H \\left[ \\mathbf{y} | \\mathbf{x} \\right] = - \\int \\int {p( \\mathbf{y}, \\mathbf{x}) ln(p( \\mathbf{y} | \\mathbf{x}))} $$\n",
    "이를 x에 대한 y의 **조건부 엔트로피(Conditional Entropy)**라 한다. 곱의 법칙을 적용하면 쉽게 도출할 수 있다.\n",
    "$$  H \\left[ \\mathbf{x} , \\mathbf{y} \\right] =  H \\left[ \\mathbf{y} | \\mathbf{x} \\right] +  H \\left[ \\mathbf{x} \\right] $$\n",
    "$H \\left[ \\mathbf{x} , \\mathbf{y} \\right] $는 p(x,y)의 미분엔트로피, $ H \\left[ \\mathbf{x} \\right]$는 주변 분포 p(x)에 대한 미분 엔트로피이다. 따라서 x와 y를 특정짓기 위해 필요한 정보의 양은 x만 따로 특정짓기 위해 필요한 양과, x가 주어질 때 y를 특정짓기 위해 필요한 양의 합과 같다.\n",
    "\n",
    "#### 1.6.1 상대적 엔트로피와 상호 정보량\n",
    "\n",
    "지금까지 엔트로피를 포함한 정보이론의 중요한 개념을 살펴보았고 이제 이 개념이 패턴인식에 어떻게 적용할 수 있는지 보자. 알려지지 않은 분포 p(x)를 고려할 때, 이를 피팅하기 위해 모델을 만들고 그결과 분포 q(x)를 구할 수 있엇다. 만약 q(x)를 이용해 x의 값을 수신자에게 전달하기 위해 코드를 만든다고 하면 p(x)가 아닌 q(x)를 사용했으므로 추가정보가 필요하다. 이때 추가로 필요한 정보량은 다음과 같이 주어진다.(우리가 가장 효율적인 방법을 취했다고 가정했을 때)\n",
    "$$ \\begin{matrix} KL(p || q) &=& - \\int p(\\mathbf{x})ln(q(\\mathbf{x})) d\\mathbf{x} - \\left( - \\int p(\\mathbf{x})ln(p(\\mathbf{x})) d\\mathbf{x}  \\right)  \\\\ &=& - \\int{ p(\\mathbf{x})ln \\left\\{ {{q(\\mathbf{x})}\\over{ p(\\mathbf{x}) }} \\right\\}   d\\mathbf{x}}   \\end{matrix}  $$\n",
    "이 때 정보량의 단위는 내트이며 위 식을 p(x)와 q(x)간의 **상대 엔트로피(Relative Entropy)** 또는 **쿨백 라이블러 발산(KL Divergence)**라고 부른다. 이 식은 대칭적이지 않아 $ KL(p||q) \\neq KL(q||p) $이다.\n",
    "\n",
    "이제 $ KL(p||q) \\geqslant 0 $에서 $ KL(p||q) = 0 $ 일 때가 p(x) = q(x)인 것과 동치임을 보이자. 이를 위해 **볼록함수(Convex Function)**의 개념을 살펴보면, 그림 1.31에서 보이는 것 처럼 모든 현이 함수 상에 혹은 그 위에 존재할 경우, f(x)가 볼록하다고 말한다. x = a 에서 x = b 사이의 구간의 x 값은 $0 \\leqslant \\lambda \\leqslant 1 $인 경우에 $ \\lambda a + (1 - \\lambda)b $라고 적을 수 있다. 이 구간에서 현은 $ \\lambda f(a) + (1 - \\lambda f(b) $가 되며 해당 구간의 함수값은 $ f(\\lambda a + (1 - \\lambda) b) $이므로 함수의 볼록성은 다음과 같이 나타낼 수 있다.\n",
    "$$  f(\\lambda a + (1 - \\lambda) b) \\leqslant  \\lambda f(a) + (1 - \\lambda f(b)  $$\n",
    "이는 함수의 이차 미분값이 모든 구간에서 양의 값을 가진다는 것과 동일하다. xlnx(x>0인 경우)와 x^2이 볼록 함수의 예시이다. 람다는 0과 1에 대해서만 앞 식을 만족할 경우에는 함수를 ** 순볼록(Strictly Convex) **라고 한다. 함수가 정반대의 성질을 가질 때는 오목, 순오목함수라고 할 수 있다. <br>\n",
    "수학적 귀납법을 사용하면 위 식으로 부터 볼록함수 f(x)가 다음을 만족함을 알 수 있다.\n",
    "$$ f \\left( \\sum_{i=1}^{M} \\lambda_{i}x_{i} \\right) \\leqslant \\sum_{i=1}^{M} \\lambda_{i} f(x_{i})  $$\n",
    "위 식이 바로 **옌센의 부등식(Jensen's Inequality)** 이며, 여기서 모든 포인트들의 집합 $ \\left\\{ x_{i} \\right\\} $에 대해 $\\lambda_{i} \\geqslant 0  $ 이며, $ \\sum_{i} \\lambda_{i} = 1 $이다. $\\lambda_{i}$ 를 $ \\left\\{ x_{i} \\right\\} $를 값으로 가지는 이산변수 x상의 확률분포로 해석하면 $ f( \\mathbb{E} \\left[ x \\right] ) \\leqslant  \\mathbb{E} \\left[ f(x) \\right] $ 이며 연속 변수에 대한 옌센의 부등식은 $ f \\left( \\int \\mathbf{x} p(\\mathbf{x}) d \\mathbf{x}  \\right) \\leqslant \\int f(\\mathbf{x})p(\\mathbf{x}) d\\mathbf{x} $ 이다. 이 형태의 옌센의 부등식에 KL Divergence에 적용하면 다음을 얻게 된다.\n",
    "$$ KL(p||q) = - \\int{ p(\\mathbf{x})ln \\left\\{ {{q(\\mathbf{x})}\\over{ p(\\mathbf{x}) }} \\right\\}   d\\mathbf{x}} \\geqslant - ln \\int q(\\mathbf{x}) d \\mathbf{x} = 0 $$\n",
    "여기서 -lnx가 볼록함수라는 것과 q(x)의 적분 값은 1 이라는 사실을 적용하였으며 -lnx가 순볼록함수고 등식이 성립하는 것은 모든 x에 대해 q(x) = p(x)인 것이 필요충분조건이 된다. 따라서 두 분포 p(x)와 q(x)가 얼마나 다른지 척도로 KL Divergence를 사용할 수 있다.\n",
    "\n",
    "밀도를 추정하는 것(ex) 알지 못한느 확률분폴르 모델링하는 문제)과 전송하는데이터를 압축하는 데는 밀접환 연관이 있음을 알 수 있다. 실제 분포를 알고 있을 때 가장 효율적인 압축이 가능하며, 평균적으로 (최소한) 두 분포 사이의 KL Divergence가 추가적으로 전송되야 한다.\n",
    "\n",
    "우리가 모델링하고자 하는 Unkown 분포 p(x)로 부터 데이터가 만들어지는 상황을 가정하자.\n",
    "변경가능한 매개변수 $\\theta$에 대해 종속적인 매개변수 분포 $q(\\mathbf{x}|\\theta)$ (ex) 다변량 가우시안분포 등)을 이용해 분포 p(x)를 추정하고자 시도할 때, $\\theta$를 구하는 구하는 방법 중 하나는 p(x)와 $q(\\mathbf{x}|\\theta)$ 사이의 KL Divergence를 최소화 하도록하는 세타를 찾는 것이다. p(x)에 대해 정확히 알고 있지 않음으로 직접 계산하는 것은 불가능 하지만 예를 들어 p(x)에 유한한 숫자의 훈련집합 포인트 $\\mathbf{x}_{n}(n=1,2,...,N)$을 추출 했을 때 식 $ \\mathbb{E}[f]  \\simeq {1\\over N} \\sum_{n=1}^{N}f(x_{n}) $에 따라 p(x)의 기댓값을 근사할 수 있다.\n",
    "$$ KL(p||q) \\simeq {{1} \\over {N}} \\sum_{n=1}^{N} \\left\\{ -ln(q(\\mathbf{x}_{n}| \\theta)) + ln(p(\\mathbf{x}_{n})) \\right\\} $$\n",
    "위 식에서 우변의 두 번째 항은 $\\theta$로 부터 독립이며, 첫 번째항은 분포 $q(\\mathbf{x}|\\theta)$하에서 $\\theta$의 음의 log-likelihood 함수를 훈련집합을 이용해 계산하는 것에 해당한다. 따라서 KL Divergence를 최소화한다는 것이 likelihood 함수를 최대화하는 것과 동일하다는 것을 알 수 있다. <br>\n",
    "이제 두변수 x와 y의 결합분포 p(x,y)로 살펴보면 두 변수 집합이 서로 독립적일 때 결합 분포의 결합확률 p(x,y) = p(x)p(y)와 연관있을 것이며, 독립이 아닐 때는 결합분포와 주변 분포간의 곱사이의 KL Divergence를 이용해 변수들이 '얼마나' 독립적인지 알아볼 수 있다.\n",
    "$$ \\mathbf{I} \\left[ \\mathbf{x}, \\mathbf{y} \\right] \\equiv KL(p(\\mathbf{x,y}) ||p(\\mathbf{x})p(\\mathbf{y}) ) = - \\int \\int p(\\mathbf{x,y}) ln \\left( {{p(\\mathbf{x})p(\\mathbf{y})} \\over {p(\\mathbf{x,y})}} \\right) d\\mathbf{x}d\\mathbf{y}  $$\n",
    "위 식을 변수 x와 y사이의 **상호정보량(Mutual Information)**이라고 한다. KL Divergence의 성질에 따라 $ \\mathbf{I} \\left[ \\mathbf{x}, \\mathbf{y} \\right] \\geqslant 0  $이며, x,y가 서로 독립일 때만 $ \\mathbf{I} \\left[ \\mathbf{x}, \\mathbf{y} \\right] = 0  $이다. 확률의 합과 곱의 법칙을 적용하면 상호정보량은 조건부 엔트로피와 다음의 관계를 갖는다.\n",
    "$$ \\mathbf{I} \\left[ \\mathbf{x}, \\mathbf{y} \\right] = \\mathbf{H} \\left[ \\mathbf{x} \\right] - \\mathbf{H} \\left[ \\mathbf{x|y} \\right] = \\mathbf{H} \\left[ \\mathbf{y} \\right] - \\mathbf{H} \\left[ \\mathbf{y|x} \\right] $$\n",
    "y에 대해 알고 있을 때 x값에 대한 불확실성(또는 x에 대해 알 고 있을 때 y에 대한 불확실성)을 표현한 것이 상호정보량이라고 생각할 수 있으며, 베이지안 관점에서는 p(x)를 x에 대한 사전분포로, p(x|y)를 새로운 데이터 y를 관찰한 후의 사후분포로 볼 수 있다. 따라서 베이지안 관점에서 상호정보량은 새 관찰값 y의 결과로 줄어드는 x에 대한 불확실성을 표현한 것이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
