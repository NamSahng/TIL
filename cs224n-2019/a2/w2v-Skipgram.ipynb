{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3618,
     "status": "ok",
     "timestamp": 1558963117957,
     "user": {
      "displayName": "랄랄라난나",
      "photoUrl": "",
      "userId": "04977961339279299961"
     },
     "user_tz": -540
    },
    "id": "rj2SyJDCIWYn",
    "outputId": "e3f6f439-965d-456a-f313-bb5622df22ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3MxCeLhEJG8D"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'gdrive/My Drive/Colab_Notebooks/cs224n-2019/a2'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2568,
     "status": "ok",
     "timestamp": 1558963122502,
     "user": {
      "displayName": "랄랄라난나",
      "photoUrl": "",
      "userId": "04977961339279299961"
     },
     "user_tz": -540
    },
    "id": "bQgyB5A-GppK",
    "outputId": "064fc058-e78c-4195-f10b-83b70671c416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect_submission.sh  __pycache__  Untitled0.ipynb  w2v-Skipgram.ipynb\n",
      "env.yml\t\t       run.py\t    Untitled1.ipynb  word2vec.py\n",
      "get_datasets.sh        sgd.py\t    utils\t     word_vectors.png\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpmpY51iGFWf"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# from utils.gradcheck import gradcheck_naive\n",
    "from utils.utils import normalizeRows, softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RxSa_ufTJcoZ"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = 1. / (1. + np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnCHsEpH22yL"
   },
   "source": [
    "- Steps for Naive Softmax Loss\n",
    "  - Generate one-hot input vector $ x \\in \\mathbb{R}^{|V|}  $ of the center word.\n",
    "  - Get our embedded word vector for center word $ v_{c} = Vx \\in \\mathbb{R}^{n}  $\n",
    "  - Generate a score vector $ z = Uv_{c} $\n",
    "  - Turn the score vector into probabilities, $ \\hat{y} = softmax(z) $\n",
    "  \n",
    "  - $ P(O=0|C=c) = \\hat{y} = {{\\exp(u_{o}^{T}v_{c} )}\\over{ \\sum_{w \\in Vocab} \\exp(u_{w}^{T}v_{c} )}} $\n",
    "  \n",
    "- Naive Softmax Loss (Conditional Independence Assumption: for a single pair of words c and o)\n",
    "  - $ \\begin{matrix} J &=& - \\log (P(O=0|C=c) ) \\\\\n",
    "&=& - \\sum_{w \\in Vocab} y_{w}\\log(\\hat{y_{w}}) \\\\\n",
    "&=& -\\log(\\hat{y_{o}}) \\end{matrix} $\n",
    "  \n",
    "  \n",
    "\n",
    "- Gradients\n",
    "  - $ \\begin{matrix} {{\\partial{J}}\\over{\\partial{v_{c}}}} &=& U^{T}(\\hat{y}-y) \\end{matrix} $\n",
    "  - $\\begin{matrix} {{\\partial{J}}\\over{\\partial{u_{w}}}} &=&  \\begin{cases}\n",
    "(\\hat{y}-1)^{T}v_{c}, &  w=0 \\\\\n",
    "(\\hat{y})^{T}v_{c}, & w \\neq 0 \\end{cases}  \\\\ &=&  (\\hat{y}-y)^{T}v_{c}\n",
    "\\end{matrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKfBkscPJfgf"
   },
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "    Implement the naive softmax loss and gradients between a center word's\n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models.\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors (rows of matrix) for all words in vocab\n",
    "                      (U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### Please use the provided softmax function (imported earlier in this file)\n",
    "    ### This numerically stable implementation helps you avoid issues pertaining\n",
    "    ### to integer overflow.\n",
    "    gradCenterVec = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    z = np.dot(outsideVectors, centerWordVec)\n",
    "    y_hat = softmax(z)\n",
    "    loss = -np.log(y_hat[outsideWordIdx])\n",
    "    \n",
    "    y = np.zeros((outsideVectors.shape[0]))\n",
    "    y[outsideWordIdx] = 1\n",
    "    gradCenterVec = np.dot(outsideVectors.T,y_hat - y)\n",
    "    #gradOutsideVecs = np.outer(y_hat - y, centerWordVec)\n",
    "    gradOutsideVecs = np.dot((y_hat - y).reshape((outsideVectors.shape[0],1)), centerWordVec.reshape((1,outsideVectors.shape[1])))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpsrV1pdRFol"
   },
   "outputs": [],
   "source": [
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MER9Us4Fku99"
   },
   "source": [
    "- Negative Sampling loss for a center word c, an outside word o, and K negative samples(10~15개가 적당하다고 함) from Vocab.\n",
    "  - $J_{neg-sample} = -\\log(\\sigma(u_{o}^{T}v_{c})) - \\sum_{k=1}^{K} \\log(\\sigma(-u_{k}^{T}v_{c})) $\n",
    "\n",
    "  \n",
    "- Gradients (using derivative of sigmoid)\n",
    "  - $ \\begin{matrix} {{\\partial{J}}\\over{\\partial{v_{c}}}} &=& (\\sigma(u_{o}^{T}v_{c})-1)u_{0} - \\sum_{k=1}^{K}(\\sigma(-u_{k}^{T}v_{c})-1)u_{k} \\end{matrix}   $\n",
    "  - $ \\begin{matrix} {{\\partial{J}}\\over{\\partial{u_{w}}}} &=&  \\begin{cases}\n",
    "(\\sigma(u_{o}^{T}v_{c})-1)v_{c}, &  w = o \\\\\n",
    "(1-\\sigma(u_{k}^{T}v_{c}))v_{c}, & w = k & \\mbox{here} \\ k = 1 ,...,10 \\end{cases}\n",
    "\\end{matrix}   $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WcG4LiQRJ05"
   },
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors (rows of matrix) for all words in vocab\n",
    "                      (U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    (dJ / dU)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### Please use your implementation of sigmoid in here.\n",
    "    \n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "    loss = 0\n",
    "\n",
    "    z = sigmoid(np.dot(outsideVectors[outsideWordIdx], centerWordVec))\n",
    "\n",
    "    loss += -np.log(z)\n",
    "    gradOutsideVecs[outsideWordIdx] = np.dot(z-1.0, centerWordVec)\n",
    "    gradCenterVec = np.dot(z - 1.0, outsideVectors[outsideWordIdx])\n",
    "\n",
    "    for i in range(K):\n",
    "        w_k = indices[i+1]\n",
    "        z1 = sigmoid(-np.dot(outsideVectors[w_k], centerWordVec))\n",
    "        loss += -np.log(z1)\n",
    "        gradOutsideVecs[w_k] += np.dot(1.0 - z1, centerWordVec)\n",
    "        gradCenterVec -= np.dot(z1 - 1.0, outsideVectors[w_k])\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLdQyO4DQ1_o"
   },
   "source": [
    " - $\\begin{matrix} J \n",
    "&=& -\\log P(w_{c-m},\\dots,w_{c-1},w_{c+1},\\dots,w_{c+m}|w_{c}) \\\\ \n",
    "&=& -\\log \\prod_{j=0,j \\neq m}^{2m} P(w_{c-m+j}|w_{c}) \\\\\n",
    "&=& -\\log \\prod_{j=0,j \\neq m}^{2m} P(u_{c-m+j}|v_{c}) \\\\\n",
    "&=& - \\sum_{j=0,j \\neq m}^{2m} \\log P(u_{c-m+j}|v_{c}) \\\\\n",
    "\\end{matrix} $\n",
    "\n",
    "\n",
    "- $  \\partial J_{skipgram}(v_{c},w_{t-m},\\dots,w_{t+m}, U)/\\partial U  = \\sum_{j=0,j \\neq m} {{\\partial J(v_{o},w_{w+j},U)}\\over{\\partial U}} $\n",
    "\n",
    "\n",
    "- $ \\partial J_{skipgram}(v_{c},w_{t-m},\\dots,w_{t+m}, U)/\\partial v_{w}  = \\begin{cases} \\sum_{j=0,j \\neq m} {{\\partial J(v_{o},w_{w+j},U)}\\over{\\partial v_{c}}} , & \\mbox{when} \\ w = c \\\\ 0 , & \\mbox{when} \\ w \\neq c\n",
    "\\end{cases}  $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ag_TX2-YR0IX"
   },
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "    Implement the skip-gram model in this function.\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) for all words in vocab\n",
    "                        (V in pdf handout)\n",
    "    outsideVectors -- outside word vectors (as rows) for all words in vocab\n",
    "                    (U in pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVecs -- the gradient with respect to the center word vectors\n",
    "            (dJ / dV in the pdf handout)\n",
    "    gradOutsideVectors -- the gradient with respect to the outside word vectors\n",
    "                        (dJ / dU in the pdf handout)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    center_word_indx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[center_word_indx]\n",
    "\n",
    "    for w in outsideWords:\n",
    "        _loss,_gradCenterVec, _gradOutsideVectors  = word2vecLossAndGradient(centerWordVec, word2Ind[w], outsideVectors, dataset)\n",
    "        loss += _loss\n",
    "        gradCenterVecs[center_word_indx] += _gradCenterVec\n",
    "        gradOutsideVectors += _gradOutsideVectors\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0_pWprNR702"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:] # 10 x 3행렬의 윗 부분 5 x 3을 centerWV로 지정\n",
    "    outsideVectors = wordVectors[int(N/2):,:] # 나머지 아래부분을 outsideVector로 지정\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize) # 1~5 randint\n",
    "        # window size가 x면 context는 2배의 random한 a~e까지의 seq. centerword도 a~e 중 random\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1) \n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXka1v4TR-QH"
   },
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3)) # 10 x 3 행렬, 정규화(행렬의 행값들 끼리 제곱해 더하여 루트 씌운 것으로 나눔)\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper( # lambda의 vec은 dummy_vectors와 같은 값\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print (\"Skip-Gram with naiveSoftmaxLossAndGradient\")\n",
    "\n",
    "    print (\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "            *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print (\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 11.16610900153398\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-1.26947339 -1.36873189  2.45158957]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    "Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.41045956  0.18834851  1.43272264]\n",
    " [ 0.38202831 -0.17530219 -1.33348241]\n",
    " [ 0.07009355 -0.03216399 -0.24466386]\n",
    " [ 0.09472154 -0.04346509 -0.33062865]\n",
    " [-0.13638384  0.06258276  0.47605228]]\n",
    "    \"\"\")\n",
    "\n",
    "    print (\"Skip-Gram with negSamplingLossAndGradient\")   \n",
    "    print (\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:],\n",
    "            dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
    "        )\n",
    "    )\n",
    "    print (\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 16.15119285363322\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-4.54650789 -1.85942252  0.76397441]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    " Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.69148188  0.31730185  2.41364029]\n",
    " [-0.22716495  0.10423969  0.79292674]\n",
    " [-0.45528438  0.20891737  1.58918512]\n",
    " [-0.31602611  0.14501561  1.10309954]\n",
    " [-0.80620296  0.36994417  2.81407799]]\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvM8M15KSlx8"
   },
   "outputs": [],
   "source": [
    "# 원래 utils.gradcheck에 있던 것.\n",
    "\n",
    "def gradcheck_naive(f, x, gradientText):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         loss and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    gradientText -- a string detailing some context about the gradient computation\n",
    "    \"\"\"\n",
    "    print(f)\n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point / word2vec_sgd_wrapper method로 이동\n",
    "    h = 1e-4        # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes ix in x to check the gradient.\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        x[ix] += h # increment by h\n",
    "        random.setstate(rndstate)\n",
    "        fxh, _ = f(x) # evalute f(x + h)\n",
    "        x[ix] -= 2 * h # restore to previous value (very important!)\n",
    "        random.setstate(rndstate)\n",
    "        fxnh, _ = f(x)\n",
    "        x[ix] += h\n",
    "        numgrad = (fxh - fxnh) / 2 / h\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed for %s.\" % gradientText)\n",
    "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print(\"Gradient check passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1249
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3903,
     "status": "ok",
     "timestamp": 1558963145834,
     "user": {
      "displayName": "랄랄라난나",
      "photoUrl": "",
      "userId": "04977961339279299961"
     },
     "user_tz": -540
    },
    "id": "-xCml6FtF3Hw",
    "outputId": "8b36980f-6afb-4251-a83a-e7708df0b5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "<function test_word2vec.<locals>.<lambda> at 0x7f0c93937598>\n",
      "Gradient check passed!\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "<function test_word2vec.<locals>.<lambda> at 0x7f0c92e9c840>\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "Skip-Gram with naiveSoftmaxLossAndGradient\n",
      "Your Result:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "    \n",
      "Skip-Gram with negSamplingLossAndGradient\n",
      "Your Result:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "test_word2vec()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kIjkRq07zERo"
   },
   "source": [
    "## Reference \n",
    "\n",
    "- http://web.stanford.edu/class/cs224n/\n",
    "- https://github.com/alongstar518/CS224NHomeworks/tree/master/homework2\n",
    "- 224n 2017 assignment: https://dalpo0814.tistory.com/10\n",
    "- Matrix calculus:  https://darkpgmr.tistory.com/141"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "w2v-Skipgram.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
